#!/usr/bin/env python3
"""
üîó –¢–µ—Å—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Å–≤—è–∑–∫–∏ —Ç—Ä–µ–π—Å–æ–≤ —Å –ø—Ä–æ–º–ø—Ç–∞–º–∏ –≤ Langfuse
"""

import os
import sys
import json
import time
from pathlib import Path
from datetime import datetime

# –ó–∞–≥—Ä—É–∑–∫–∞ .env –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
env_path = Path("/home/yan/A101/HR/.env")
if env_path.exists():
    with open(env_path) as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith("#") and "=" in line:
                key, value = line.split("=", 1)
                os.environ[key] = value

sys.path.insert(0, "/home/yan/A101/HR")

from langfuse import Langfuse
from openai import OpenAI


def test_trace_linking():
    """–¢–µ—Å—Ç–∏—Ä—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Å–≤—è–∑–∫—É —Ç—Ä–µ–π—Å–æ–≤ —Å –ø—Ä–æ–º–ø—Ç–∞–º–∏"""

    print("üîó Testing Langfuse trace linking...")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤
    langfuse = Langfuse(
        public_key=os.getenv("LANGFUSE_PUBLIC_KEY"),
        secret_key=os.getenv("LANGFUSE_SECRET_KEY"),
        host=os.getenv("LANGFUSE_HOST"),
    )

    openai_client = OpenAI(
        base_url="https://openrouter.ai/api/v1", api_key=os.getenv("OPENROUTER_API_KEY")
    )

    # 1. –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–º–ø—Ç –∏–∑ Langfuse
    print("üìã Getting prompt from Langfuse...")
    try:
        prompt = langfuse.get_prompt(
            "a101-hr-profile-gemini-v3-simple", label="production"
        )
        print(f"‚úÖ Got prompt: {prompt.name}")
    except Exception as e:
        print(f"‚ùå Failed to get prompt: {e}")
        return False

    # 2. –°–æ–∑–¥–∞–µ–º trace ID –≤—Ä—É—á–Ω—É—é —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
    print("üöÄ Creating manual trace...")
    trace_id = langfuse.create_trace_id()
    print(f"üìù Created trace ID: {trace_id}")

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º context –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç—Ä–µ–π—Å–æ–º
    trace_metadata = {
        "prompt_name": prompt.name,
        "prompt_version": prompt.version,
        "department": "–î–ò–¢",
        "position": "Senior ML Engineer",
        "user_id": "test_user",
        "session_id": f"test_session_{int(time.time())}",
    }

    # 3. –°–æ–∑–¥–∞–µ–º generation —Å —è–≤–Ω–æ–π —Å–≤—è–∑–∫–æ–π –∫ –ø—Ä–æ–º–ø—Ç—É
    print("ü§ñ Creating generation linked to prompt...")

    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–µ –¥–ª—è —Ç–µ—Å—Ç–∞)
    variables = {
        "position": "Senior ML Engineer",
        "department": "–î–ò–¢",
        "company_map": "Test company data...",
        "org_structure": '{"–î–ò–¢": {"positions": ["Engineer", "Senior Engineer"]}}',
        "kpi_data": "Test KPI data...",
        "it_systems": "Test IT systems...",
        "json_schema": '{"type": "object", "properties": {}}',
    }

    # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç —Å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏
    compiled_prompt = prompt.compile(**variables)
    print(f"üìù Compiled prompt length: {len(compiled_prompt)} chars")

    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ –ø—Ä–æ–º–ø—Ç–∞
    model = prompt.config.get("model", "google/gemini-2.5-flash")
    temperature = prompt.config.get("temperature", 0.1)
    max_tokens = prompt.config.get("max_tokens", 4000)
    response_format = prompt.config.get("response_format")

    print(f"‚öôÔ∏è Using model: {model}")
    print(f"üå°Ô∏è Temperature: {temperature}")

    start_time = time.time()

    try:
        # –°–æ–∑–¥–∞–µ–º generation –≤ —Ä–∞–º–∫–∞—Ö –Ω–∞—à–µ–≥–æ —Ç—Ä–µ–π—Å–∞
        generation = langfuse.start_generation(
            name="profile-generation",
            model=model,
            input=[{"role": "user", "content": compiled_prompt}],
            prompt=prompt,  # üîó –≠—Ç–æ –∫–ª—é—á–µ–≤–∞—è —Å–≤—è–∑–∫–∞!
            metadata={
                "temperature": temperature,
                "max_tokens": max_tokens,
                "trace_id": trace_id,
                **trace_metadata,
            },
        )

        # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ OpenAI
        print("üîÑ Making OpenAI request...")

        messages = [{"role": "user", "content": compiled_prompt}]

        response = openai_client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            response_format=response_format,
        )

        generation_time = time.time() - start_time

        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        generated_text = response.choices[0].message.content
        usage = response.usage

        # –û–±–Ω–æ–≤–ª—è–µ–º generation —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        generation.end(
            output=generated_text,
            usage={
                "input": usage.prompt_tokens if usage else 0,
                "output": usage.completion_tokens if usage else 0,
                "total": usage.total_tokens if usage else 0,
            },
            metadata={
                "generation_time": generation_time,
                "finish_reason": response.choices[0].finish_reason,
            },
        )

        print(f"‚úÖ Generation completed in {generation_time:.2f}s")
        print(
            f"üìä Tokens: {usage.prompt_tokens} input + {usage.completion_tokens} output = {usage.total_tokens} total"
        )
        print(f"üÜî Trace ID: {trace_id}")
        print(f"üîó Trace URL: https://cloud.langfuse.com/traces/{trace_id}")

        # –ü–∞—Ä—Å–∏–º JSON –æ—Ç–≤–µ—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        try:
            profile_json = json.loads(generated_text.strip())
            print(f"üìã Generated profile with {len(profile_json)} fields")
        except json.JSONDecodeError as e:
            print(f"‚ö†Ô∏è JSON parse warning: {e}")

        return True

    except Exception as e:
        generation_time = time.time() - start_time
        print(f"‚ùå Generation failed after {generation_time:.2f}s: {e}")

        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –æ—à–∏–±–∫—É –≤ generation
        generation.end(
            output={"error": str(e)},
            metadata={"generation_time": generation_time, "error": True},
        )

        return False

    finally:
        # –ó–∞–≤–µ—Ä—à–∞–µ–º —Ç—Ä–µ–π—Å —Å –ø–æ–º–æ—â—å—é update_current_trace
        langfuse.update_current_trace(
            output={
                "success": True,
                "generation_time": generation_time,
                "trace_id": trace_id,
            },
            metadata=trace_metadata,
        )


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∞"""
    print("üöÄ Starting Langfuse trace linking test...")

    success = test_trace_linking()

    if success:
        print(
            "\nüéâ SUCCESS! Check Langfuse dashboard - trace should be linked to prompt!"
        )
        print("üìç Go to: Langfuse ‚Üí Prompts ‚Üí a101-hr-profile-gemini-v3-simple")
        print("üìà The trace should appear in the prompt's analytics")
    else:
        print("\n‚ùå FAILED! Check the errors above")

    return success


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
