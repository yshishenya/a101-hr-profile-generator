"""
–ì–ª–∞–≤–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –ø—Ä–æ—Ñ–∏–ª–µ–π –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π –ê101.

–û—Ä–∫–µ—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã:
- DataLoader –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö
- LLMClient –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ Gemini 2.5 Flash
- –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –ø–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Langfuse –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
"""

import json
import logging
import uuid
from datetime import datetime
from typing import Dict, Any, Optional, List
from pathlib import Path

from .data_loader import DataLoader
from .llm_client import LLMClient
from .prompt_manager import PromptManager
from .config import config
from ..services.profile_markdown_generator import ProfileMarkdownGenerator
from ..services.profile_storage_service import ProfileStorageService

# from langfuse.decorators import observe  # –í—Ä–µ–º–µ–Ω–Ω–æ —É–±—Ä–∞–ª–∏ –∏–∑-–∑–∞ –ø—Ä–æ–±–ª–µ–º —Å –≤–µ—Ä—Å–∏–µ–π

logger = logging.getLogger(__name__)


class ProfileGenerator:
    """
    –ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ—Ñ–∏–ª–µ–π –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π –ê101.

    –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –µ–¥–∏–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
    –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ—Ñ–∏–ª–µ–π —Å –ø–æ–ª–Ω—ã–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º —á–µ—Ä–µ–∑ Langfuse.
    """

    def __init__(
        self,
        openrouter_api_key: Optional[str] = None,
        langfuse_public_key: Optional[str] = None,
        langfuse_secret_key: Optional[str] = None,
        base_data_path: Optional[str] = None,
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –ø—Ä–æ—Ñ–∏–ª–µ–π.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç config –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–∫ –∏–∑ .env

        Args:
            openrouter_api_key: API –∫–ª—é—á –¥–ª—è OpenRouter (–∏–ª–∏ –∏–∑ config)
            langfuse_public_key: –ü—É–±–ª–∏—á–Ω—ã–π –∫–ª—é—á Langfuse (–∏–ª–∏ –∏–∑ config)
            langfuse_secret_key: –°–µ–∫—Ä–µ—Ç–Ω—ã–π –∫–ª—é—á Langfuse (–∏–ª–∏ –∏–∑ config)
            base_data_path: –ë–∞–∑–æ–≤—ã–π –ø—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º –ê101 (–∏–ª–∏ –∏–∑ config)
        """
        # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ config –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω—ã
        self.base_data_path = Path(base_data_path or config.BASE_DATA_PATH)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.data_loader = DataLoader(str(self.base_data_path))
        self.md_generator = ProfileMarkdownGenerator()
        self.storage_service = ProfileStorageService(str(self.base_data_path / "generated_profiles"))

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º LLMClient (–æ–Ω —Å–∞–º –ø–æ–ª—É—á–∏—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ config)
        try:
            self.llm_client = LLMClient(
                openrouter_api_key=openrouter_api_key,
                langfuse_public_key=langfuse_public_key,
                langfuse_secret_key=langfuse_secret_key,
            )
            self.langfuse_enabled = bool(self.llm_client.langfuse)
            logger.info("‚úÖ LLMClient initialized from config")
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize LLMClient: {e}")
            self.llm_client = None
            self.langfuse_enabled = False

        logger.info("‚úÖ ProfileGenerator initialized successfully")

    # @observe(name="generate_profile", capture_input=True, capture_output=True)  # –í—Ä–µ–º–µ–Ω–Ω–æ —É–±—Ä–∞–ª–∏
    async def generate_profile(
        self,
        department: str,
        position: str,
        employee_name: Optional[str] = None,
        temperature: float = 0.1,
        save_result: bool = True,
        profile_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ—Ñ–∏–ª—è –¥–æ–ª–∂–Ω–æ—Å—Ç–∏

        Args:
            department: –ù–∞–∑–≤–∞–Ω–∏–µ –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–∞
            position: –ù–∞–∑–≤–∞–Ω–∏–µ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏
            employee_name: –§–ò–û —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ LLM
            save_result: –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–∞–π–ª

        Returns:
            –ü–æ–ª–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        generation_start = datetime.now()

        # LLMClient —Ç–µ–ø–µ—Ä—å —Å–∞–º —Å–æ–∑–¥–∞–µ—Ç traces –≤ Langfuse

        try:
            logger.info(f"Starting profile generation: {department} - {position}")

            # 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ DataLoader
            logger.info("üìä Preparing data with deterministic logic...")
            variables = self.data_loader.prepare_langfuse_variables(
                department=department, position=position, employee_name=employee_name
            )

            # 2. –î–∞–Ω–Ω—ã–µ –≥–æ—Ç–æ–≤—ã, –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ LLM
            logger.info("ü§ñ Generating profile through Langfuse LLM client...")

            # 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ LLMClient —Å –ø–æ–ª–Ω–æ–π Langfuse –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π
            if not self.llm_client:
                raise ValueError(
                    "LLMClient not initialized - Langfuse credentials required"
                )

            llm_result = self.llm_client.generate_profile_from_langfuse(
                prompt_name="a101-hr-profile-gemini-v3-simple",
                variables=variables,
                user_id=employee_name or f"user_{department}_{position}",
                session_id=f"session_{generation_start.timestamp()}",
            )

            # 4. –í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            logger.info("‚úÖ Validating generated profile...")
            validation_result = self._validate_and_enhance_profile(llm_result)

            # 5. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            final_result = {
                "success": validation_result["success"],
                "profile": validation_result["profile"],
                "metadata": {
                    "generation": {
                        "department": department,
                        "position": position,
                        "employee_name": employee_name,
                        "timestamp": generation_start.isoformat(),
                        "duration": (datetime.now() - generation_start).total_seconds(),
                        "temperature": temperature,
                    },
                    "llm": llm_result["metadata"],
                    "validation": validation_result["validation"],
                    "data_sources": variables.get("estimated_input_tokens", 0),
                },
                "errors": validation_result.get("errors", []),
                "warnings": validation_result.get("warnings", []),
            }

            # 6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            if save_result and final_result["success"]:
                saved_path = self._save_result(final_result, department, position, profile_id)
                final_result["metadata"]["saved_path"] = str(saved_path)
                logger.info(f"üíæ Result saved to: {saved_path}")

            # 7. –¢—Ä–µ–π—Å–∏–Ω–≥ —É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω –≤ LLMClient

            duration = final_result["metadata"]["generation"]["duration"]
            success_emoji = "‚úÖ" if final_result["success"] else "‚ùå"

            logger.info(
                f"{success_emoji} Profile generation completed in {duration:.2f}s"
            )

            return final_result

        except Exception as e:
            error_result = {
                "success": False,
                "profile": None,
                "metadata": {
                    "generation": {
                        "department": department,
                        "position": position,
                        "employee_name": employee_name,
                        "timestamp": generation_start.isoformat(),
                        "duration": (datetime.now() - generation_start).total_seconds(),
                        "error": str(e),
                    }
                },
                "errors": [f"Generation failed: {str(e)}"],
                "warnings": [],
            }

            # trace handling removed as it's not available in this context

            logger.error(f"‚ùå Profile generation failed: {e}")
            return error_result

    def _validate_and_enhance_profile(
        self, llm_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∏ —É–ª—É—á—à–µ–Ω–∏–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ—Ñ–∏–ª—è"""

        if not llm_result["metadata"]["success"]:
            return {
                "success": False,
                "profile": None,
                "validation": {"is_valid": False, "errors": ["LLM generation failed"]},
                "errors": [llm_result["metadata"].get("error", "Unknown LLM error")],
            }

        profile = llm_result["profile"]

        if not profile or "error" in profile:
            return {
                "success": False,
                "profile": profile,
                "validation": {
                    "is_valid": False,
                    "errors": ["Invalid profile structure"],
                },
                "errors": ["Failed to parse valid profile from LLM response"],
            }

        # –í–∞–ª–∏–¥–∞—Ü–∏—è —á–µ—Ä–µ–∑ LLM –∫–ª–∏–µ–Ω—Ç
        validation = self.llm_client.validate_profile_structure(profile)

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ —É–ª—É—á—à–µ–Ω–∏—è
        enhanced_profile = self._enhance_profile_data(profile)

        return {
            "success": validation["completeness_score"] >= 0.7,
            "profile": enhanced_profile,
            "validation": validation,
            "errors": validation.get("errors", []),
            "warnings": validation.get("warnings", []),
        }

    def _enhance_profile_data(self, profile: Dict[str, Any]) -> Dict[str, Any]:
        """–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ —É–ª—É—á—à–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ—Ñ–∏–ª—è"""
        enhanced = profile.copy()

        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        if "metadata" not in enhanced:
            enhanced["metadata"] = {}

        enhanced["metadata"].update(
            {
                "generated_by": "A101 HR Profile Generator v1.0",
                "generation_method": "LLM + Deterministic Logic",
                "data_version": "v1.0",
                "last_updated": datetime.now().isoformat(),
            }
        )

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        if "basic_info" in enhanced and isinstance(enhanced["basic_info"], dict):
            basic_info = enhanced["basic_info"]

            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –µ—Å—Ç—å –≤—Å–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è
            if "employment_type" not in basic_info:
                basic_info["employment_type"] = "–ü–æ–ª–Ω–∞—è –∑–∞–Ω—è—Ç–æ—Å—Ç—å"

            if "salary_range" not in basic_info and "salary_from" in basic_info:
                salary_from = basic_info.get("salary_from", 0)
                salary_to = basic_info.get("salary_to", salary_from * 1.3)
                basic_info["salary_range"] = (
                    f"{salary_from:,.0f} - {salary_to:,.0f} —Ä—É–±."
                )

        return enhanced

    def _save_result(
        self, result: Dict[str, Any], department: str, position: str, profile_id: str
    ) -> Path:
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ –Ω–æ–≤—É—é –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ñ–∞–π–ª–æ–≤.
        
        –°–æ–∑–¥–∞–µ—Ç –ø–æ–ª–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É: –ë–ª–æ–∫/–î–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç/–û—Ç–¥–µ–ª/–ì—Ä—É–ø–ø–∞/–î–æ–ª–∂–Ω–æ—Å—Ç—å/–≠–∫–∑–µ–º–ø–ª—è—Ä/
        """
        generation_timestamp = datetime.now()
        
        try:
            logger.info(f"üíæ Creating hierarchical directory structure for: {department} -> {position}")
            
            # 1. –°–æ–∑–¥–∞–µ–º –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–∞–ø–æ–∫
            profile_dir = self.storage_service.create_profile_directory(
                department=department,
                position=position,
                timestamp=generation_timestamp,
                profile_id=profile_id
            )
            
            # 2. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º MD —Ñ–∞–π–ª
            logger.info("üìù Auto-generating Markdown profile...")
            md_content = self.md_generator.generate_from_json(result)
            
            # 3. –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON –∏ MD —Ñ–∞–π–ª—ã –≤ –æ–¥–Ω—É –ø–∞–ø–∫—É
            json_path, md_path = self.storage_service.save_profile_files(
                directory=profile_dir,
                json_content=result,
                md_content=md_content,
                profile_id=profile_id
            )
            
            logger.info(f"‚úÖ Profile saved to hierarchical structure:")
            logger.info(f"  üìÅ Directory: {profile_dir}")
            logger.info(f"  üìÑ JSON: {json_path.name}")
            logger.info(f"  üìù MD: {md_path.name}")
            
            return json_path  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            
        except Exception as e:
            logger.error(f"‚ùå Error saving profile to hierarchical structure: {e}")
            
            # Fallback –∫ —Å—Ç–∞—Ä–æ–π —Å–∏—Å—Ç–µ–º–µ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
            logger.warning("‚ö†Ô∏è Falling back to legacy file structure...")
            return self._save_result_legacy(result, department, position)
    
    def _save_result_legacy(
        self, result: Dict[str, Any], department: str, position: str
    ) -> Path:
        """Fallback –∫ —Å—Ç–∞—Ä–æ–π —Å–∏—Å—Ç–µ–º–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤"""
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        results_dir = self.base_data_path / "generated_profiles"
        results_dir.mkdir(exist_ok=True)

        # –°–æ–∑–¥–∞–µ–º –ø–æ–¥–ø–∞–ø–∫—É –ø–æ –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–∞–º
        dept_dir = results_dir / self._sanitize_filename(department)
        dept_dir.mkdir(exist_ok=True)

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{self._sanitize_filename(position)}_{timestamp}.json"

        file_path = dept_dir / filename

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        logger.info(f"üíæ Legacy save completed: {file_path}")
        return file_path

    def _sanitize_filename(self, name: str) -> str:
        """–°–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞"""
        # –ó–∞–º–µ–Ω—è–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
        sanitized = name.replace(" ", "_")
        sanitized = sanitized.replace("/", "_")
        sanitized = sanitized.replace("\\", "_")
        sanitized = sanitized.replace(":", "_")
        sanitized = sanitized.replace("*", "_")
        sanitized = sanitized.replace("?", "_")
        sanitized = sanitized.replace("<", "_")
        sanitized = sanitized.replace(">", "_")
        sanitized = sanitized.replace("|", "_")

        return sanitized

    def get_available_departments(self) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–æ–≤"""
        return self.data_loader.get_available_departments()

    def get_positions_for_department(self, department: str) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π –¥–ª—è –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–∞"""
        return self.data_loader.get_positions_for_department(department)

    def get_prompt_analytics(
        self, prompt_name: str = "profile_generation"
    ) -> Dict[str, Any]:
        """
        @doc –ü–æ–ª—É—á–µ–Ω–∏–µ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –ø—Ä–æ–º–ø—Ç–∞

        Examples:
            python>
            generator = ProfileGenerator(api_key)
            analytics = generator.get_prompt_analytics()
            print(f"Cache status: {analytics['cache_status']}")
        """
        return self.prompt_manager.get_prompt_analytics(prompt_name)

    def validate_prompt_template(
        self, template: str, prompt_name: str = "profile_generation"
    ) -> Dict[str, Any]:
        """
        @doc –í–∞–ª–∏–¥–∞—Ü–∏—è —à–∞–±–ª–æ–Ω–∞ –ø—Ä–æ–º–ø—Ç–∞

        Examples:
            python>
            generator = ProfileGenerator(api_key)
            validation = generator.validate_prompt_template(new_template)
            if validation["valid"]:
                print("Template is valid!")
        """
        return self.prompt_manager.validate_prompt_template(template, prompt_name)

    async def validate_system(self) -> Dict[str, Any]:
        """–ü–æ–ª–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã"""
        validation_result = {
            "system_ready": True,
            "components": {},
            "warnings": [],
            "errors": [],
        }

        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö
        data_sources = self.data_loader.validate_data_sources()
        validation_result["components"]["data_sources"] = data_sources

        missing_sources = [name for name, status in data_sources.items() if not status]
        if missing_sources:
            validation_result["errors"].extend(
                [f"Missing data source: {source}" for source in missing_sources]
            )
            validation_result["system_ready"] = False

        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ LLM –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
        if self.llm_client:
            try:
                llm_test = self.llm_client.test_connection()
                validation_result["components"]["llm_connection"] = llm_test

                if not llm_test["success"]:
                    validation_result["errors"].append(
                        f"LLM connection failed: {llm_test['error']}"
                    )
                    validation_result["system_ready"] = False
            except Exception as e:
                validation_result["components"]["llm_connection"] = {
                    "success": False,
                    "error": str(e),
                }
                validation_result["errors"].append(f"LLM connection test failed: {e}")
                validation_result["system_ready"] = False
        else:
            validation_result["errors"].append(
                "LLM client not initialized - requires Langfuse credentials"
            )
            validation_result["system_ready"] = False

        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ Langfuse
        validation_result["components"]["langfuse"] = {"enabled": self.langfuse_enabled}
        if not self.langfuse_enabled:
            validation_result["warnings"].append("Langfuse monitoring not configured")

        # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ Langfuse Prompt Management
        if self.langfuse_enabled and self.llm_client:
            try:
                # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –∏–∑ Langfuse
                validation_result["components"]["langfuse_prompts"] = {
                    "success": True,
                    "message": "Prompts managed via Langfuse",
                }
            except Exception as e:
                validation_result["components"]["langfuse_prompts"] = {
                    "success": False,
                    "error": str(e),
                }
                validation_result["errors"].append(f"Langfuse prompt test failed: {e}")
                validation_result["system_ready"] = False
        else:
            validation_result["warnings"].append(
                "Langfuse prompt management not available"
            )

        return validation_result


if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ ProfileGenerator
    import os
    import asyncio

    logging.basicConfig(level=logging.INFO)

    async def test_profile_generator():
        api_key = os.getenv("OPENROUTER_API_KEY")

        if not api_key:
            print("‚ùå OPENROUTER_API_KEY not set")
            return

        generator = ProfileGenerator(openrouter_api_key=api_key)

        print("=== –í–∞–ª–∏–¥–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã ===")
        validation = await generator.validate_system()

        print(f"–°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞: {'‚úÖ' if validation['system_ready'] else '‚ùå'}")

        for component, status in validation["components"].items():
            if isinstance(status, dict) and "success" in status:
                emoji = "‚úÖ" if status["success"] else "‚ùå"
                print(f"  {component}: {emoji}")
            else:
                print(f"  {component}: {status}")

        if validation["errors"]:
            print("–û—à–∏–±–∫–∏:")
            for error in validation["errors"]:
                print(f"  ‚ùå {error}")

        if validation["warnings"]:
            print("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è:")
            for warning in validation["warnings"]:
                print(f"  ‚ö†Ô∏è {warning}")

        if validation["system_ready"]:
            print("\n=== –¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ—Ñ–∏–ª—è ===")
            result = await generator.generate_profile(
                department="–î–ò–¢",
                position="–°–∏—Å—Ç–µ–º–Ω—ã–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä",
                employee_name="–¢–µ—Å—Ç–æ–≤ –¢–µ—Å—Ç –¢–µ—Å—Ç–æ–≤–∏—á",
            )

            if result["success"]:
                print("‚úÖ –ü—Ä–æ—Ñ–∏–ª—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")
                print(
                    f"‚è±Ô∏è –í—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {result['metadata']['generation']['duration']:.2f}s"
                )
                print(
                    f"üìä –ü–æ–ª–Ω–æ—Ç–∞ –ø—Ä–æ—Ñ–∏–ª—è: {result['metadata']['validation']['completeness_score']:.2%}"
                )
            else:
                print("‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ—Ñ–∏–ª—è")
                for error in result["errors"]:
                    print(f"  {error}")

    # asyncio.run(test_profile_generator())
