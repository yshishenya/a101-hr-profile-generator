"""
@doc Prompt Template Management System –¥–ª—è A101 HR Profile Generator

–°–∏—Å—Ç–µ–º–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ–º–ø—Ç-—à–∞–±–ª–æ–Ω–∞–º–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π:
- Langfuse Prompt Management –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
- Versioning –∏ A/B testing
- Fallback –∫ –ª–æ–∫–∞–ª—å–Ω—ã–º —à–∞–±–ª–æ–Ω–∞–º
- Template variable substitution

Examples:
    python>
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –ø—Ä–æ–º–ø—Ç–æ–≤
    pm = PromptManager(langfuse_client=langfuse)

    # –ü–æ–ª—É—á–µ–Ω–∏–µ –∞–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
    prompt = pm.get_prompt("profile_generation", variables={"department": "IT"})

    # A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    prompt = pm.get_prompt_variant("profile_generation", variant="experimental")
"""

import logging
import json
import os
from pathlib import Path
from typing import Dict, Any, Optional, List
from datetime import datetime
import hashlib

logger = logging.getLogger(__name__)


class PromptManager:
    """
    @doc –ú–µ–Ω–µ–¥–∂–µ—Ä –ø—Ä–æ–º–ø—Ç-—à–∞–±–ª–æ–Ω–æ–≤ —Å Langfuse –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π

    –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞–º–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π
    –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è, A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ fallback –∫ –ª–æ–∫–∞–ª—å–Ω—ã–º —Ñ–∞–π–ª–∞–º.

    Examples:
        python>
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–µ–Ω–µ–¥–∂–µ—Ä–∞ —Å Langfuse
        pm = PromptManager(langfuse_client=langfuse_client)

        # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ —Å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏
        prompt = pm.get_prompt("generation", {
            "department": "IT",
            "position": "Developer"
        })
    """

    def __init__(
        self,
        langfuse_client=None,
        templates_dir: str = None,
        cache_ttl: int = 300,
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Prompt Manager

        Args:
            langfuse_client: –ö–ª–∏–µ–Ω—Ç Langfuse (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            templates_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ —à–∞–±–ª–æ–Ω–∞–º–∏ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è PROJECT_ROOT/templates)
            cache_ttl: –í—Ä–µ–º—è –∂–∏–∑–Ω–∏ –∫–µ—à–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        """
        self.langfuse_client = langfuse_client

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º templates_dir: –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π –ø—É—Ç—å –∏–ª–∏ default
        if templates_dir is None:
            # –í—ã—á–∏—Å–ª—è–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏—è —ç—Ç–æ–≥–æ —Ñ–∞–π–ª–∞
            project_root = Path(__file__).parent.parent.parent
            self.templates_dir = project_root / "templates"
        else:
            self.templates_dir = Path(templates_dir)

        self.cache_ttl = cache_ttl
        self.langfuse_enabled = langfuse_client is not None

        # –ö–µ—à –¥–ª—è –ø—Ä–æ–º–ø—Ç–æ–≤
        self._prompt_cache = {}
        self._cache_timestamps = {}

        # –†–µ–µ—Å—Ç—Ä –ø—Ä–æ–º–ø—Ç–æ–≤
        self.prompt_registry = {
            "profile_generation": {
                "langfuse_name": "a101-hr-profile-gemini-v3-simple",  # v28 with SGR
                "local_file": "generation_prompt.txt",
                "version": "28",  # Schema-Guided Reasoning (3-phase cascade)
                "description": "Master prompt v28 —Å Schema-Guided Reasoning (SGR) - —Ç—Ä–µ—Ö—Ñ–∞–∑–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å —è–≤–Ω—ã–º–∏ –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏",
                "variables": [
                    "company_map",
                    "org_structure",
                    "department",
                    "position",
                    "department_path",
                    "profile_examples",
                    "kpi_data",
                    "it_systems",
                    "json_schema",
                    "employee_name",
                    "generation_timestamp",
                    "data_version",
                ],
                "config": {
                    # –ë–∞–∑–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è - –ø–æ–ª–Ω–∞—è —Å—Ö–µ–º–∞ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –∏–∑ templates/prompts/production/config.json
                    "model": "google/gemini-2.5-flash",
                    "temperature": 0.1,
                    "max_tokens": 4000,
                    "structured_output": True,
                    # response_format –±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ config.json —á–µ—Ä–µ–∑ get_prompt_config()
                },
            }
        }

        logger.info(f"PromptManager initialized (Langfuse: {self.langfuse_enabled})")

    def get_prompt(
        self,
        prompt_name: str,
        variables: Optional[Dict[str, Any]] = None,
        version: Optional[str] = None,
        variant: Optional[str] = None,
    ) -> str:
        """
        @doc –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ —Å –ø–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö

        Examples:
            python>
            # –ë–∞–∑–æ–≤–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞
            prompt = pm.get_prompt("profile_generation")

            # –° –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏
            prompt = pm.get_prompt("profile_generation", {
                "department": "IT",
                "position": "Senior Developer"
            })

            # –ö–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –≤–µ—Ä—Å–∏—è
            prompt = pm.get_prompt("profile_generation", version="1.2.0")
        """
        if prompt_name not in self.prompt_registry:
            raise ValueError(f"Unknown prompt: {prompt_name}")

        # –ü–æ–ª—É—á–∞–µ–º –±–∞–∑–æ–≤—ã–π —à–∞–±–ª–æ–Ω
        template = self._get_prompt_template(prompt_name, version, variant)

        # –ü–æ–¥—Å—Ç–∞–≤–ª—è–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        if variables:
            template = self._substitute_variables(template, variables, prompt_name)

        return template

    def get_prompt_config(
        self, prompt_name: str, environment: str = "production"
    ) -> Dict[str, Any]:
        """
        @doc –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø—Ä–æ–º–ø—Ç–∞

        –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:
        1. Langfuse (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
        2. –õ–æ–∫–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª config.json (–Ω–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞)
        3. –†–µ–µ—Å—Ç—Ä –≤ –∫–æ–¥–µ (fallback)

        Examples:
            python>
            config = pm.get_prompt_config("profile_generation")
            model = config.get("model", "google/gemini-2.5-flash")
            response_format = config.get("response_format")
        """
        if prompt_name not in self.prompt_registry:
            raise ValueError(f"Unknown prompt: {prompt_name}")

        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ –∏–∑ —Ä–µ–µ—Å—Ç—Ä–∞ (–±–∞–∑–æ–≤—ã–π fallback)
        local_config = self.prompt_registry[prompt_name].get("config", {})

        # –ï—Å–ª–∏ Langfuse –≤–∫–ª—é—á–µ–Ω, –ø—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥ –æ—Ç—Ç—É–¥–∞
        if self.langfuse_enabled:
            try:
                registry_entry = self.prompt_registry[prompt_name]
                langfuse_name = registry_entry["langfuse_name"]

                # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–º–ø—Ç –∏–∑ Langfuse —Å label="production"
                prompt_obj = self.langfuse_client.get_prompt(
                    langfuse_name, label=environment
                )

                if prompt_obj and hasattr(prompt_obj, "config") and prompt_obj.config:
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π –∏ Langfuse –∫–æ–Ω—Ñ–∏–≥
                    langfuse_config = prompt_obj.config
                    merged_config = {**local_config, **langfuse_config}
                    logger.info(f"Using Langfuse config for '{prompt_name}'")
                    return merged_config

            except Exception as e:
                logger.warning(f"Failed to get prompt config from Langfuse: {e}")

        # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ config.json
        try:
            env_dir = self.templates_dir / "prompts" / environment
            config_file = env_dir / "config.json"

            if config_file.exists():
                with open(config_file, "r", encoding="utf-8") as f:
                    file_config = json.load(f)

                merged_config = {**local_config, **file_config}
                logger.info(
                    f"Using local file config for '{prompt_name}' from {config_file}"
                )
                return merged_config

        except Exception as e:
            logger.debug(f"Failed to load config from local file: {e}")

        logger.info(f"Using registry config for '{prompt_name}'")
        return local_config

    def _get_prompt_template(
        self,
        prompt_name: str,
        version: Optional[str] = None,
        variant: Optional[str] = None,
    ) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–≥–æ —à–∞–±–ª–æ–Ω–∞ –ø—Ä–æ–º–ø—Ç–∞"""
        cache_key = f"{prompt_name}:{version or 'latest'}:{variant or 'default'}"

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à
        if self._is_cached_valid(cache_key):
            return self._prompt_cache[cache_key]

        template = None

        # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∏–∑ Langfuse
        if self.langfuse_enabled:
            try:
                template = self._get_from_langfuse(prompt_name, version, variant)
                if template:
                    logger.info(f"Retrieved prompt '{prompt_name}' from Langfuse")
            except Exception as e:
                logger.warning(f"Failed to get prompt from Langfuse: {e}")

        # Fallback –∫ –ª–æ–∫–∞–ª—å–Ω–æ–º—É —Ñ–∞–π–ª—É
        if not template:
            template = self._get_from_local_file(prompt_name)
            logger.info(f"Using local template for '{prompt_name}'")

        # –ö–µ—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        self._prompt_cache[cache_key] = template
        self._cache_timestamps[cache_key] = datetime.now()

        return template

    def _get_from_langfuse(
        self,
        prompt_name: str,
        version: Optional[str] = None,
        variant: Optional[str] = None,
    ) -> Optional[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –∏–∑ Langfuse"""
        if not self.langfuse_enabled:
            return None

        try:
            registry_entry = self.prompt_registry[prompt_name]
            langfuse_name = registry_entry["langfuse_name"]

            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–º–ø—Ç –∏–∑ Langfuse
            if version:
                prompt = self.langfuse_client.get_prompt(langfuse_name, version=version)
            else:
                prompt = self.langfuse_client.get_prompt(
                    langfuse_name, label="production"
                )

            if prompt:
                # üî• –ù–û–í–ê–Ø –§–ò–ß–ê: –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–ø—Ç –≤ –ª–æ–∫–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã –ø—Ä–∏ —É—Å–ø–µ—à–Ω–æ–º –ø–æ–ª—É—á–µ–Ω–∏–∏
                try:
                    prompt_text = None
                    if hasattr(prompt, "prompt"):
                        prompt_text = prompt.prompt
                    elif hasattr(prompt, "content"):
                        prompt_text = prompt.content
                    else:
                        prompt_text = str(prompt) if prompt else None

                    if prompt_text:
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–ø—Ç –∏ –∫–æ–Ω—Ñ–∏–≥ –ª–æ–∫–∞–ª—å–Ω–æ
                        self._save_prompt_to_local(prompt_name, prompt, prompt_text)
                        return prompt_text
                except Exception as save_error:
                    logger.warning(
                        f"Failed to save prompt to local files: {save_error}"
                    )
                    # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ä–∞–±–æ—Ç—É –¥–∞–∂–µ –µ—Å–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å
                    if hasattr(prompt, "prompt"):
                        return prompt.prompt
                    elif hasattr(prompt, "content"):
                        return prompt.content
                    else:
                        return str(prompt) if prompt else None

            return None

        except Exception as e:
            logger.error(f"Error fetching prompt from Langfuse: {e}")
            return None

    def _get_from_local_file(self, prompt_name: str) -> str:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞

        –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:
        1. –ù–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞: /templates/prompts/production/prompt.txt
        2. –°—Ç–∞—Ä—ã–π —Ñ–∞–π–ª: /templates/generation_prompt.txt (fallback)
        """
        # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ –Ω–æ–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        try:
            prompt_text = self._load_prompt_from_local(
                prompt_name, environment="production"
            )
            if prompt_text:
                logger.info(f"Loaded prompt '{prompt_name}' from new local structure")
                return prompt_text
        except Exception as e:
            logger.debug(f"Failed to load from new structure: {e}")

        # Fallback –∫ —Å—Ç–∞—Ä–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É
        registry_entry = self.prompt_registry[prompt_name]
        local_file = registry_entry["local_file"]
        file_path = self.templates_dir / local_file

        if not file_path.exists():
            raise FileNotFoundError(f"Local prompt template not found: {file_path}")

        with open(file_path, "r", encoding="utf-8") as f:
            return f.read()

    def _save_prompt_to_local(
        self,
        prompt_name: str,
        prompt_obj: Any,
        prompt_text: str,
        environment: str = "production",
    ):
        """
        @doc –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –≤ –ª–æ–∫–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã –¥–ª—è fallback

        –°—Ç—Ä—É–∫—Ç—É—Ä–∞:
        /templates/prompts/production/
          prompt.txt           # –¢–µ–∫—Å—Ç –ø—Ä–æ–º–ø—Ç–∞
          config.json          # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (model, temperature, JSON schema)
          metadata.json        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (version, timestamp, hash)

        Examples:
            python>
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ —É—Å–ø–µ—à–Ω–æ–º –ø–æ–ª—É—á–µ–Ω–∏–∏ –∏–∑ Langfuse
            pm._save_prompt_to_local("profile_generation", prompt_obj, prompt_text)
        """
        try:
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –æ–∫—Ä—É–∂–µ–Ω–∏—è
            env_dir = self.templates_dir / "prompts" / environment
            env_dir.mkdir(parents=True, exist_ok=True)

            # 1. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—Å—Ç –ø—Ä–æ–º–ø—Ç–∞
            prompt_file = env_dir / "prompt.txt"
            with open(prompt_file, "w", encoding="utf-8") as f:
                f.write(prompt_text)

            # 2. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é (–µ—Å–ª–∏ –µ—Å—Ç—å)
            config_data = {}
            if hasattr(prompt_obj, "config") and prompt_obj.config:
                config_data = prompt_obj.config
            elif hasattr(prompt_obj, "model_config"):
                config_data = prompt_obj.model_config

            config_file = env_dir / "config.json"
            with open(config_file, "w", encoding="utf-8") as f:
                json.dump(config_data, f, ensure_ascii=False, indent=2)

            # 3. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = {
                "prompt_name": prompt_name,
                "environment": environment,
                "saved_at": datetime.now().isoformat(),
                "version": getattr(prompt_obj, "version", None),
                "hash": hashlib.sha256(prompt_text.encode()).hexdigest(),
                "character_count": len(prompt_text),
                "line_count": len(prompt_text.split("\n")),
            }

            # –î–æ–±–∞–≤–ª—è–µ–º labels –µ—Å–ª–∏ –µ—Å—Ç—å
            if hasattr(prompt_obj, "labels"):
                metadata["labels"] = prompt_obj.labels

            metadata_file = env_dir / "metadata.json"
            with open(metadata_file, "w", encoding="utf-8") as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)

            logger.info(
                f"Saved prompt '{prompt_name}' to local files: {env_dir} "
                f"(version: {metadata['version']}, hash: {metadata['hash'][:8]}...)"
            )

        except Exception as e:
            logger.error(f"Failed to save prompt to local files: {e}")
            raise

    def _load_prompt_from_local(
        self, prompt_name: str, environment: str = "production"
    ) -> Optional[str]:
        """
        @doc –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–º–ø—Ç–∞ –∏–∑ –ª–æ–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤

        Examples:
            python>
            # –ó–∞–≥—Ä—É–∑–∫–∞ production –ø—Ä–æ–º–ø—Ç–∞
            prompt_text = pm._load_prompt_from_local("profile_generation", "production")

            # –ó–∞–≥—Ä—É–∑–∫–∞ development –ø—Ä–æ–º–ø—Ç–∞
            prompt_text = pm._load_prompt_from_local("profile_generation", "development")
        """
        try:
            env_dir = self.templates_dir / "prompts" / environment
            prompt_file = env_dir / "prompt.txt"

            if not prompt_file.exists():
                logger.debug(f"Local prompt file not found: {prompt_file}")
                return None

            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—Å—Ç –ø—Ä–æ–º–ø—Ç–∞
            with open(prompt_file, "r", encoding="utf-8") as f:
                prompt_text = f.read()

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏
            metadata_file = env_dir / "metadata.json"
            if metadata_file.exists():
                with open(metadata_file, "r", encoding="utf-8") as f:
                    metadata = json.load(f)

                # –í–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º hash
                actual_hash = hashlib.sha256(prompt_text.encode()).hexdigest()
                stored_hash = metadata.get("hash", "")

                if actual_hash != stored_hash:
                    logger.warning(
                        f"Hash mismatch for local prompt '{prompt_name}': "
                        f"stored={stored_hash[:8]}..., actual={actual_hash[:8]}..."
                    )

                logger.info(
                    f"Loaded local prompt '{prompt_name}' (version: {metadata.get('version')}, "
                    f"saved: {metadata.get('saved_at')})"
                )

            return prompt_text

        except Exception as e:
            logger.error(f"Failed to load prompt from local files: {e}")
            return None

    def _substitute_variables(
        self, template: str, variables: Dict[str, Any], prompt_name: str
    ) -> str:
        """
        @doc –ü–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –≤ —à–∞–±–ª–æ–Ω –ø—Ä–æ–º–ø—Ç–∞

        Examples:
            python>
            # –®–∞–±–ª–æ–Ω —Å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏
            template = "Hello {{name}}, your role is {{role}}"
            variables = {"name": "John", "role": "Developer"}
            result = pm._substitute_variables(template, variables, "test")
            # Result: "Hello John, your role is Developer"
        """
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –æ–∂–∏–¥–∞–µ–º—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
            expected_vars = set(self.prompt_registry[prompt_name]["variables"])
            provided_vars = set(variables.keys())

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
            critical_vars = {"department", "position", "json_schema"}
            missing_critical = critical_vars - provided_vars

            if missing_critical:
                logger.warning(f"Missing critical variables: {missing_critical}")

            # –ü–æ–¥—Å—Ç–∞–≤–ª—è–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
            result = template
            for var_name, var_value in variables.items():
                placeholder = f"{{{{{var_name}}}}}"
                if placeholder in result:
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º None –∏ –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                    if var_value is None:
                        var_value = "[–ù–ï–¢ –î–ê–ù–ù–´–•]"
                    elif isinstance(var_value, (dict, list)):
                        var_value = json.dumps(var_value, ensure_ascii=False, indent=2)

                    result = result.replace(placeholder, str(var_value))

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –Ω–µ–ø—Ä–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
            remaining_placeholders = self._find_placeholders(result)
            if remaining_placeholders:
                logger.warning(f"Unsubstituted placeholders: {remaining_placeholders}")

            return result

        except Exception as e:
            logger.error(f"Error substituting variables: {e}")
            return template

    def _find_placeholders(self, text: str) -> List[str]:
        """–ü–æ–∏—Å–∫ –Ω–µ–ø—Ä–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
        import re

        pattern = r"\{\{([^}]+)\}\}"
        matches = re.findall(pattern, text)
        return matches

    def _is_cached_valid(self, cache_key: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –∫–µ—à–∞"""
        if cache_key not in self._prompt_cache:
            return False

        timestamp = self._cache_timestamps.get(cache_key)
        if not timestamp:
            return False

        age = (datetime.now() - timestamp).total_seconds()
        return age < self.cache_ttl

    def create_prompt_version(
        self, prompt_name: str, content: str, version: str, description: str = ""
    ) -> bool:
        """
        @doc –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –≤–µ—Ä—Å–∏–∏ –ø—Ä–æ–º–ø—Ç–∞ –≤ Langfuse

        Examples:
            python>
            # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –≤–µ—Ä—Å–∏–∏ –ø—Ä–æ–º–ø—Ç–∞
            success = pm.create_prompt_version(
                "profile_generation",
                updated_content,
                "1.1.0",
                "–£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"
            )
        """
        if not self.langfuse_enabled:
            logger.warning("Cannot create prompt version: Langfuse not enabled")
            return False

        try:
            registry_entry = self.prompt_registry[prompt_name]
            langfuse_name = registry_entry["langfuse_name"]

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π API Langfuse –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–æ–º–ø—Ç–∞
            prompt_config = {
                "description": description,
                "created_at": datetime.now().isoformat(),
                "variables": registry_entry["variables"],
                "type": "text",
            }

            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç —á–µ—Ä–µ–∑ Langfuse API (—Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç)
            self.langfuse_client.create_prompt(
                name=langfuse_name,
                prompt=content,
                labels=[version, "production"],
                tags=["a101", "hr", "profile-generation"],
                type="text",
                config=prompt_config,
                commit_message=f"Created version {version}: {description}",
            )

            # –û—á–∏—â–∞–µ–º –∫–µ—à
            self._clear_cache_for_prompt(prompt_name)

            logger.info(f"Created prompt version {version} for {prompt_name}")
            return True

        except Exception as e:
            logger.error(f"Failed to create prompt version: {e}")
            return False

    def get_prompt_analytics(self, prompt_name: str) -> Dict[str, Any]:
        """
        @doc –ü–æ–ª—É—á–µ–Ω–∏–µ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –ø—Ä–æ–º–ø—Ç–∞

        Examples:
            python>
            # –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø—Ä–æ–º–ø—Ç–∞
            analytics = pm.get_prompt_analytics("profile_generation")
            print(f"Total uses: {analytics['total_uses']}")
        """
        if not self.langfuse_enabled:
            return {"error": "Langfuse not enabled"}

        try:
            registry_entry = self.prompt_registry[prompt_name]
            langfuse_name = registry_entry["langfuse_name"]

            # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∑–∞–ø—Ä–æ—Å—ã –∫ Langfuse Analytics API
            # –ü–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é

            return {
                "prompt_name": prompt_name,
                "langfuse_name": langfuse_name,
                "current_version": registry_entry["version"],
                "cache_status": self._get_cache_status(prompt_name),
                "last_updated": datetime.now().isoformat(),
            }

        except Exception as e:
            logger.error(f"Failed to get prompt analytics: {e}")
            return {"error": str(e)}

    def _get_cache_status(self, prompt_name: str) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –∫–µ—à–∞ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞"""
        matching_keys = [
            k for k in self._prompt_cache.keys() if k.startswith(prompt_name)
        ]

        return {
            "cached_variants": len(matching_keys),
            "cache_keys": matching_keys,
            "cache_ttl": self.cache_ttl,
        }

    def _clear_cache_for_prompt(self, prompt_name: str):
        """–û—á–∏—Å—Ç–∫–∞ –∫–µ—à–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞"""
        keys_to_remove = [
            k for k in self._prompt_cache.keys() if k.startswith(prompt_name)
        ]

        for key in keys_to_remove:
            self._prompt_cache.pop(key, None)
            self._cache_timestamps.pop(key, None)

        logger.info(
            f"Cleared cache for prompt '{prompt_name}' ({len(keys_to_remove)} entries)"
        )

    def validate_prompt_template(
        self, template: str, prompt_name: str
    ) -> Dict[str, Any]:
        """
        @doc –í–∞–ª–∏–¥–∞—Ü–∏—è —à–∞–±–ª–æ–Ω–∞ –ø—Ä–æ–º–ø—Ç–∞

        Examples:
            python>
            # –í–∞–ª–∏–¥–∞—Ü–∏—è —à–∞–±–ª–æ–Ω–∞
            validation = pm.validate_prompt_template(template, "profile_generation")
            if not validation["valid"]:
                print("Errors:", validation["errors"])
        """
        errors = []
        warnings = []

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ —Å–µ–∫—Ü–∏–∏
        required_sections = [
            "–°–ò–°–¢–ï–ú–ê –†–û–õ–ò –ò –≠–ö–°–ü–ï–†–¢–ò–ó–´",
            "–ö–û–ù–¢–ï–ö–°–¢ –ö–û–ú–ü–ê–ù–ò–ò –ê101",
            "–¢–ï–•–ù–ò–ß–ï–°–ö–ê–Ø –°–ü–ï–¶–ò–§–ò–ö–ê–¶–ò–Ø –í–´–•–û–î–ê",
            "–ò–ù–°–¢–†–£–ö–¶–ò–ò –ü–û –ì–ï–ù–ï–†–ê–¶–ò–ò",
        ]

        for section in required_sections:
            if section not in template:
                errors.append(f"Missing required section: {section}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        if prompt_name in self.prompt_registry:
            expected_vars = self.prompt_registry[prompt_name]["variables"]
            placeholders = self._find_placeholders(template)

            missing_vars = set(expected_vars) - set(placeholders)
            extra_vars = set(placeholders) - set(expected_vars)

            if missing_vars:
                warnings.append(f"Missing expected variables: {missing_vars}")

            if extra_vars:
                warnings.append(f"Unexpected variables: {extra_vars}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É
        if len(template) < 1000:
            warnings.append("Template seems too short for comprehensive generation")

        if len(template) > 50000:
            warnings.append("Template might be too long and exceed token limits")

        return {
            "valid": len(errors) == 0,
            "errors": errors,
            "warnings": warnings,
            "character_count": len(template),
            "placeholder_count": len(self._find_placeholders(template)),
        }
