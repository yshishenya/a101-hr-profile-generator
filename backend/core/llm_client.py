"""
LLM –∫–ª–∏–µ–Ω—Ç –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å Gemini 2.5 Flash —á–µ—Ä–µ–∑ Langfuse OpenAI pattern.

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Langfuse –¥–ª—è –ø—Ä–æ–º–ø—Ç–æ–≤ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
- OpenRouter API —á–µ—Ä–µ–∑ langfuse.openai
- Structured output —Å JSON schema
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ç—Ä–µ–π—Å–∏–Ω–≥ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
"""

import json
import time
import logging
from typing import Dict, Any, Optional, Union
from datetime import datetime
from langfuse.openai import OpenAI
from langfuse import Langfuse

from .config import config

logger = logging.getLogger(__name__)


class LLMClient:
    """
    –ö–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Gemini 2.5 Flash —á–µ—Ä–µ–∑ Langfuse OpenAI integration
    """

    def __init__(
        self,
        openrouter_api_key: Optional[str] = None,
        langfuse_public_key: Optional[str] = None,
        langfuse_secret_key: Optional[str] = None,
        langfuse_host: Optional[str] = None,
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM –∫–ª–∏–µ–Ω—Ç–∞ —Å Langfuse –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç config –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–∫ –∏–∑ .env

        Args:
            openrouter_api_key: API –∫–ª—é—á OpenRouter (–∏–ª–∏ –∏–∑ config)
            langfuse_public_key: Langfuse public key (–∏–ª–∏ –∏–∑ config)
            langfuse_secret_key: Langfuse secret key (–∏–ª–∏ –∏–∑ config)
            langfuse_host: Langfuse host URL (–∏–ª–∏ –∏–∑ config)
        """
        # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ config –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω—ã
        self.openrouter_api_key = openrouter_api_key or config.OPENROUTER_API_KEY
        self.langfuse_public_key = langfuse_public_key or config.LANGFUSE_PUBLIC_KEY
        self.langfuse_secret_key = langfuse_secret_key or config.LANGFUSE_SECRET_KEY
        self.langfuse_host = langfuse_host or config.LANGFUSE_HOST

        if not self.openrouter_api_key:
            raise ValueError("OPENROUTER_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env –∏–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Langfuse
        if self.langfuse_public_key and self.langfuse_secret_key:
            self.langfuse = Langfuse(
                public_key=self.langfuse_public_key,
                secret_key=self.langfuse_secret_key,
                host=self.langfuse_host,
            )
            logger.info("‚úÖ Langfuse initialized from config")
        else:
            logger.warning(
                "‚ö†Ô∏è Langfuse credentials not found in config - tracing disabled"
            )
            self.langfuse = None

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º OpenAI –∫–ª–∏–µ–Ω—Ç —á–µ—Ä–µ–∑ Langfuse
        self.client = OpenAI(
            api_key=self.openrouter_api_key,
            base_url=config.OPENROUTER_BASE_URL,
            default_headers={
                "HTTP-Referer": "https://a101-hr-profiles.local",
                "X-Title": "A101 HR Profile Generator",
            },
        )
        logger.info(f"‚úÖ LLM client initialized with model: {config.OPENROUTER_MODEL}")

    def _create_generation_with_prompt(
        self,
        prompt,
        messages,
        model,
        temperature,
        max_tokens,
        response_format,
        trace_metadata=None,
    ):
        """
        @doc –í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è generation —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Å–≤—è–∑–∫–æ–π –ø—Ä–æ–º–ø—Ç–∞

        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –∫–∞—á–µ—Å—Ç–≤–µ –¥–µ–∫–æ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è Langfuse tracing

        Examples:
            python>
            result = self._create_generation_with_prompt(prompt, messages, model, temp, max_tokens, format, metadata)
        """
        # –û–±–æ–≥–∞—â–µ–Ω–Ω—ã–µ metadata –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ —Ç—Ä–µ–∫–∏–Ω–≥–∞ (—É–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ prompt content)
        enriched_metadata = {
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
            "model": model,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "response_format_type": (
                type(response_format).__name__ if response_format else None
            ),
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–æ–º–ø—Ç–µ (–±–µ–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ)
            "prompt_length": len(str(messages)),
            "messages_count": len(messages),
            # –ù–ï –¥–æ–±–∞–≤–ª—è–µ–º prompt_first_100_chars - –∑–∞–Ω–∏–º–∞–µ—Ç –º–µ—Å—Ç–æ
            # Trace metadata (–∏—Å–∫–ª—é—á–∞–µ—Ç json_schema, company_map, org_structure)
            **(trace_metadata or {}),
        }

        # –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è —Å–≤—è–∑–∫–∞ –ø—Ä–æ–º–ø—Ç–∞ —Å–æ–≥–ª–∞—Å–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ 2025 –≥–æ–¥–∞
        response = self.client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            response_format=response_format,
            langfuse_prompt=prompt,  # üîó –ö–õ–Æ–ß–ï–í–ê–Ø —Å–≤—è–∑–∫–∞ —Å –ø—Ä–æ–º–ø—Ç–æ–º!
            metadata=enriched_metadata,
        )

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ metadata –ø–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞
        if hasattr(response, "usage") and response.usage:
            usage = response.usage
            post_metadata = {
                **enriched_metadata,
                # –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ response
                "response_id": getattr(response, "id", None),
                "created_timestamp": getattr(response, "created", None),
                "actual_model": getattr(response, "model", model),
                "system_fingerprint": getattr(response, "system_fingerprint", None),
                # –ü—Ä–æ–≤–∞–π–¥–µ—Ä –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ OpenRouter
                "provider": (
                    response.model_extra.get("provider", "openrouter")
                    if hasattr(response, "model_extra") and response.model_extra
                    else "openrouter"
                ),
                # –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–æ–∫–µ–Ω–∞—Ö
                "prompt_tokens": usage.prompt_tokens,
                "completion_tokens": usage.completion_tokens,
                "total_tokens": usage.total_tokens,
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–µ—Ç–∞–ª–∏ —Ç–æ–∫–µ–Ω–æ–≤ (–∏–∑ model_extra –µ—Å–ª–∏ –µ—Å—Ç—å)
                "prompt_tokens_cached": 0,
                "completion_reasoning_tokens": 0,
                "completion_image_tokens": 0,
                # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏
                "finish_reason": (
                    response.choices[0].finish_reason if response.choices else None
                ),
                "choice_index": response.choices[0].index if response.choices else None,
                "response_length": (
                    len(response.choices[0].message.content)
                    if response.choices and response.choices[0].message
                    else 0
                ),
                # –ü–æ–ø—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å cost –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é (–º–æ–∂–µ—Ç –±—ã—Ç—å –≤ headers –∏–ª–∏ –¥—Ä—É–≥–∏—Ö –º–µ—Å—Ç–∞—Ö)
                "cost_usd": getattr(usage, "cost", None),
                "cost_estimated": None,  # –ú–æ–∂–µ–º –¥–æ–±–∞–≤–∏—Ç—å —Ä–∞—Å—á—ë—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ–∑–∂–µ
                # –°—Ç–∞—Ç—É—Å –∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä
                "success": True,
                "api_provider": "openrouter",
                "langfuse_sdk_version": "3.3.4",
                "openai_sdk_version": (
                    response.__class__.__module__
                    if hasattr(response.__class__, "__module__")
                    else "unknown"
                ),
            }

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é token –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
            try:
                if hasattr(usage, "model_extra") and usage.model_extra:
                    if "prompt_tokens_details" in usage.model_extra:
                        post_metadata["prompt_tokens_cached"] = usage.model_extra[
                            "prompt_tokens_details"
                        ].get("cached_tokens", 0)
                    if "completion_tokens_details" in usage.model_extra:
                        completion_details = usage.model_extra[
                            "completion_tokens_details"
                        ]
                        post_metadata["completion_reasoning_tokens"] = (
                            completion_details.get("reasoning_tokens", 0)
                        )
                        post_metadata["completion_image_tokens"] = (
                            completion_details.get("image_tokens", 0)
                        )
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Failed to extract detailed token info: {e}")

            # –û–±–Ω–æ–≤–ª—è–µ–º generation —Å –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ (–µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ)
            try:
                if self.langfuse:
                    self.langfuse.update_current_generation(
                        metadata=post_metadata,
                        usage_details={
                            "prompt_tokens": usage.prompt_tokens,
                            "completion_tokens": usage.completion_tokens,
                            "total_tokens": usage.total_tokens,
                        },
                    )
            except Exception as e:
                # –õ–æ–≥–∏—Ä—É–µ–º –Ω–æ –Ω–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
                logger.warning(f"‚ö†Ô∏è Failed to update generation metadata: {e}")

        return response

    def generate_profile_from_langfuse(
        self,
        prompt_name: str,
        variables: Dict[str, Any],
        user_id: Optional[str] = None,
        session_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        @doc –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ—Ñ–∏–ª—è –¥–æ–ª–∂–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ Langfuse prompt —Å traced execution

        Args:
            prompt_name: –ò–º—è –ø—Ä–æ–º–ø—Ç–∞ –≤ Langfuse
            variables: –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –ø–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–∏ –≤ –ø—Ä–æ–º–ø—Ç
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è —Ç—Ä–µ–π—Å–∏–Ω–≥–∞
            session_id: ID —Å–µ—Å—Å–∏–∏ –¥–ª—è —Ç—Ä–µ–π—Å–∏–Ω–≥–∞

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏

        Examples:
            python>
            client = LLMClient(api_key, langfuse_keys...)
            result = client.generate_profile_from_langfuse(
                prompt_name="a101-hr-profile-gemini-v2",
                variables={"position": "Senior Developer", "department": "IT"}
            )
        """
        start_time = time.time()

        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ Langfuse –¥–æ—Å—Ç—É–ø–µ–Ω
            if not self.langfuse:
                raise ValueError(
                    "Langfuse –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω - –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–æ–º–ø—Ç"
                )

            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–º–ø—Ç –∏–∑ Langfuse —Å label="production"
            prompt = self.langfuse.get_prompt(prompt_name, label="production")

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            model = prompt.config.get("model", "google/gemini-2.5-flash")
            temperature = prompt.config.get("temperature", 0.1)
            max_tokens = prompt.config.get("max_tokens", 4000)
            response_format = prompt.config.get("response_format")

            logger.info(f"Starting Langfuse generation with prompt: {prompt_name}")
            logger.info(f"Model: {model}, Temperature: {temperature}")
            logger.info(f"Response format: {response_format}")

            # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç —Å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏
            compiled_prompt = prompt.compile(**variables)
            logger.info(f"Compiled prompt type: {type(compiled_prompt)}")

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç messages –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if isinstance(compiled_prompt, str):
                # –ï—Å–ª–∏ compiled_prompt - —Å—Ç—Ä–æ–∫–∞, –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ messages —Ñ–æ—Ä–º–∞—Ç
                messages = [{"role": "user", "content": compiled_prompt}]
                logger.info(
                    f"Converted string prompt to messages format, length: {len(compiled_prompt)}"
                )
            elif isinstance(compiled_prompt, list):
                # –ï—Å–ª–∏ —É–∂–µ —Å–ø–∏—Å–æ–∫ messages
                messages = compiled_prompt
                logger.info(f"Using existing messages format, count: {len(messages)}")
            else:
                raise ValueError(f"Unexpected prompt format: {type(compiled_prompt)}")

            logger.info(f"Final messages count: {len(messages)}")

            # –õ–æ–≥–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞
            logger.info(f"Request parameters:")
            logger.info(f"  Model: {model}")
            logger.info(f"  Messages count: {len(messages)}")
            logger.info(f"  Temperature: {temperature}")
            logger.info(f"  Max tokens: {max_tokens}")
            logger.info(f"  Response format type: {type(response_format)}")

            # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–µ–∫–æ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏
            trace_metadata = {
                "prompt_name": prompt_name,
                "prompt_version": prompt.version,
                "department": variables.get("department"),
                "position": variables.get("position"),
                "employee_name": variables.get("employee_name"),
                "user_id": user_id,
                "session_id": session_id,
                "environment": "production",
                "source": "hr_profile_generator",
            }
            # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é —Å –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä–æ–º –¥–ª—è —Å–≤—è–∑–∫–∏ –ø—Ä–æ–º–ø—Ç–æ–≤
            try:
                response = self._create_generation_with_prompt(
                    prompt=prompt,
                    messages=messages,
                    model=model,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    response_format=response_format,
                    trace_metadata=trace_metadata,
                )
                logger.info("‚úÖ Generation with prompt linking completed")
            except Exception as api_error:
                logger.error(f"OpenAI API error: {api_error}")
                logger.error(f"Error type: {type(api_error)}")
                raise api_error

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç–≤–µ—Ç
            generated_text = response.choices[0].message.content

            # –ü–∞—Ä—Å–∏–º JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞
            profile_json = self._extract_and_parse_json(generated_text)

            generation_time = time.time() - start_time

            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ç–æ–∫–µ–Ω—ã –∏–∑ usage
            usage = response.usage
            input_tokens = usage.prompt_tokens if usage else 0
            output_tokens = usage.completion_tokens if usage else 0
            total_tokens = usage.total_tokens if usage else input_tokens + output_tokens

            logger.info(f"Langfuse generation completed in {generation_time:.2f}s")
            logger.info(
                f"Tokens used: {input_tokens} input + {output_tokens} output = {total_tokens} total"
            )

            # –ü–æ–ª—É—á–∞–µ–º trace_id –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä—ã
            trace_id = None
            try:
                from langfuse.decorators import langfuse_context

                trace_id = langfuse_context.get_current_trace_id()
                logger.info("‚úÖ Langfuse tracing with decorators completed")
            except ImportError:
                logger.info("‚úÖ Basic Langfuse tracing completed")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Failed to get trace ID: {e}")

            return {
                "profile": profile_json,
                "metadata": {
                    "model": model,
                    "generation_time": generation_time,
                    "tokens": {
                        "input": input_tokens,
                        "output": output_tokens,
                        "total": total_tokens,
                    },
                    "temperature": temperature,
                    "timestamp": datetime.now().isoformat(),
                    "success": True,
                    "langfuse_trace_id": trace_id,
                    "prompt_name": prompt_name,
                    "prompt_version": prompt.version,
                    "tracing_mode": "decorator_based",
                    "department": variables.get("department"),
                    "position": variables.get("position"),
                },
                "raw_response": generated_text,
            }

        except Exception as e:
            generation_time = time.time() - start_time
            logger.error(
                f"Langfuse generation failed after {generation_time:.2f}s: {e}"
            )

            # –ü–æ–ª—É—á–∞–µ–º trace_id –¥–∞–∂–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
            try:
                from langfuse.decorators import langfuse_context

                trace_id = langfuse_context.get_current_trace_id()
            except (ImportError, NameError):
                trace_id = None

            return {
                "profile": None,
                "metadata": {
                    "model": "unknown",
                    "generation_time": generation_time,
                    "error": str(e),
                    "timestamp": datetime.now().isoformat(),
                    "success": False,
                    "langfuse_trace_id": trace_id,
                },
                "raw_response": None,
            }

    def _extract_and_parse_json(self, generated_text: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ –ø–∞—Ä—Å–∏–Ω–≥ JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM"""
        try:
            # –ò—â–µ–º JSON –≤ —Ç–µ–∫—Å—Ç–µ (–º–µ–∂–¥—É ``` –∏–ª–∏ –≤ —á–∏—Å—Ç–æ–º –≤–∏–¥–µ)
            json_text = generated_text.strip()

            # –£–±–∏—Ä–∞–µ–º markdown code blocks –µ—Å–ª–∏ –µ—Å—Ç—å
            if json_text.startswith("```"):
                json_text = json_text.split("```")[1]
                if json_text.startswith("json"):
                    json_text = json_text[4:]
                json_text = json_text.strip()

            if json_text.endswith("```"):
                json_text = json_text[:-3].strip()

            # –ü–∞—Ä—Å–∏–º JSON
            profile_data = json.loads(json_text)

            if not isinstance(profile_data, dict):
                raise ValueError("Response is not a JSON object")

            logger.info("JSON successfully parsed from LLM response")
            return profile_data

        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse JSON from LLM response: {e}")
            logger.debug(f"Raw response text: {generated_text[:500]}...")

            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º fallback —Å—Ç—Ä—É–∫—Ç—É—Ä—É
            return {
                "error": "Failed to parse JSON from LLM response",
                "raw_response": generated_text,
                "parse_error": str(e),
            }

        except Exception as e:
            logger.error(f"Unexpected error while parsing response: {e}")
            return {
                "error": "Unexpected error during response parsing",
                "raw_response": generated_text,
                "parse_error": str(e),
            }

    def validate_profile_structure(self, profile: Dict[str, Any]) -> Dict[str, Any]:
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ—Ñ–∏–ª—è

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        """
        validation_result = {
            "is_valid": True,
            "errors": [],
            "warnings": [],
            "completeness_score": 0.0,
        }

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è (—Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å–æ —Å—Ö–µ–º–æ–π JSON)
        required_fields = [
            "position_title",
            "department_broad", 
            "department_specific",
            "category",
            "primary_activity",
            "responsibility_areas",
            "professional_skills",
            "personal_qualities"
        ]

        missing_fields = []
        empty_fields = []
        total_fields = len(required_fields)
        filled_fields = 0

        for field in required_fields:
            if field not in profile:
                missing_fields.append(field)
            elif not profile[field] or (
                isinstance(profile[field], (list, dict)) and len(profile[field]) == 0
            ):
                empty_fields.append(field)
            else:
                filled_fields += 1

        if missing_fields:
            validation_result["is_valid"] = False
            validation_result["errors"].extend(
                [f"Missing required field: {field}" for field in missing_fields]
            )

        if empty_fields:
            validation_result["warnings"].extend(
                [f"Empty required field: {field}" for field in empty_fields]
            )

        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–æ–ª–Ω–æ—Ç—É –ø—Ä–æ—Ñ–∏–ª—è
        validation_result["completeness_score"] = filled_fields / total_fields

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–æ–ª—è
        if "basic_info" in profile and isinstance(profile["basic_info"], dict):
            basic_info = profile["basic_info"]
            if not basic_info.get("position_title"):
                validation_result["errors"].append(
                    "Missing position_title in basic_info"
                )

        logger.info(
            f"Profile validation completed: {validation_result['completeness_score']:.2%} complete, {len(validation_result['errors'])} errors"
        )

        return validation_result

    def test_connection(self) -> Dict[str, Any]:
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ Langfuse –∫ OpenRouter API"""
        try:
            response = self.client.chat.completions.create(
                model="google/gemini-2.5-flash",
                messages=[
                    {
                        "role": "user",
                        "content": 'Hello! Please respond with a simple JSON: {"status": "ok", "message": "Connection test successful"}',
                    }
                ],
                max_tokens=100,
                temperature=0.1,
            )

            return {
                "success": True,
                "model": "google/gemini-2.5-flash",
                "response": response.choices[0].message.content,
            }

        except Exception as e:
            return {"success": False, "error": str(e)}


if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–æ–≥–æ LLM –∫–ª–∏–µ–Ω—Ç–∞ —Å Langfuse
    import os

    logging.basicConfig(level=logging.INFO)

    def test_llm_client():
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–ª—é—á–∏
        openrouter_key = os.getenv("OPENROUTER_API_KEY")
        langfuse_public = os.getenv("LANGFUSE_PUBLIC_KEY")
        langfuse_secret = os.getenv("LANGFUSE_SECRET_KEY")

        if not all([openrouter_key, langfuse_public, langfuse_secret]):
            print("‚ùå Missing required API keys")
            print(f"  OpenRouter: {'‚úÖ' if openrouter_key else '‚ùå'}")
            print(f"  Langfuse public: {'‚úÖ' if langfuse_public else '‚ùå'}")
            print(f"  Langfuse secret: {'‚úÖ' if langfuse_secret else '‚ùå'}")
            return

        client = LLMClient(
            openrouter_api_key=openrouter_key,
            langfuse_public_key=langfuse_public,
            langfuse_secret_key=langfuse_secret,
        )

        print("=== –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è ===")
        connection_test = client.test_connection()

        if connection_test["success"]:
            print("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ Langfuse —É—Å–ø–µ—à–Ω–æ")
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {connection_test['error']}")
            return

        print("\n=== –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ Langfuse ===")
        test_variables = {
            "position": "Senior ML Engineer",
            "department": "–î–ò–¢",
            "employee_name": "–¢–µ—Å—Ç–æ–≤–∞—è –ì–µ–Ω–µ—Ä–∞—Ü–∏—è",
            "org_structure": "–¢–µ—Å—Ç–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞",
            "kpi_data": "–¢–µ—Å—Ç–æ–≤—ã–µ KPI",
            "it_systems": "–¢–µ—Å—Ç–æ–≤—ã–µ —Å–∏—Å—Ç–µ–º—ã",
        }

        result = client.generate_profile_from_langfuse(
            prompt_name="a101-hr-profile-gemini-v2",
            variables=test_variables,
            user_id="test-user",
            session_id="test-session",
        )

        if result["metadata"]["success"]:
            print("‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ Langfuse —É—Å–ø–µ—à–Ω–∞")
            print(f"üìä –¢–æ–∫–µ–Ω—ã: {result['metadata']['tokens']}")
            print(f"‚è±Ô∏è –í—Ä–µ–º—è: {result['metadata']['generation_time']:.2f}s")
            print(f"üîó Trace ID: {result['metadata']['langfuse_trace_id']}")

            if result["profile"]:
                validation = client.validate_profile_structure(result["profile"])
                print(f"‚úì –í–∞–ª–∏–¥–∞—Ü–∏—è: {validation['completeness_score']:.2%} –∑–∞–ø–æ–ª–Ω–µ–Ω–æ")
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {result['metadata']['error']}")

    # –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Ç–µ—Å—Ç–∞
    # test_llm_client()
