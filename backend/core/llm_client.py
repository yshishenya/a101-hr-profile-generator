"""
LLM –∫–ª–∏–µ–Ω—Ç –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å Gemini 2.5 Flash —á–µ—Ä–µ–∑ OpenRouter API.

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
- –û—Ç–ø—Ä–∞–≤–∫—É –∑–∞–ø—Ä–æ—Å–æ–≤ —á–µ—Ä–µ–∑ OpenRouter
- –û–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–æ–∫ –∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏
- –í–∞–ª–∏–¥–∞—Ü–∏—è JSON –æ—Ç–≤–µ—Ç–æ–≤
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Langfuse –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
"""

import json
import time
import logging
from typing import Dict, Any, Optional, Union
from datetime import datetime
import httpx

logger = logging.getLogger(__name__)


class LLMClient:
    """
    –ö–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Gemini 2.5 Flash —á–µ—Ä–µ–∑ OpenRouter API
    """
    
    def __init__(self, 
                 api_key: str, 
                 model: str = "google/gemini-2.0-flash-exp:free",
                 base_url: str = "https://openrouter.ai/api/v1",
                 timeout: int = 120,
                 max_retries: int = 3):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM –∫–ª–∏–µ–Ω—Ç–∞
        
        Args:
            api_key: API –∫–ª—é—á OpenRouter
            model: –ú–æ–¥–µ–ª—å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
            base_url: –ë–∞–∑–æ–≤—ã–π URL OpenRouter API
            timeout: –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
            max_retries: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫
        """
        self.api_key = api_key
        self.model = model
        self.base_url = base_url
        self.timeout = timeout
        self.max_retries = max_retries
        
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://a101-hr-profiles.local",
            "X-Title": "A101 HR Profile Generator"
        }
    
    async def generate_profile(self, 
                             prompt: str, 
                             variables: Dict[str, Any],
                             temperature: float = 0.1,
                             max_tokens: int = 8192) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ—Ñ–∏–ª—è –¥–æ–ª–∂–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ LLM
        
        Args:
            prompt: –ü—Ä–æ–º–ø—Ç —Å –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä–∞–º–∏ –¥–ª—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
            variables: –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –ø–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–∏ –≤ –ø—Ä–æ–º–ø—Ç
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (0.0-1.0)
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        """
        start_time = time.time()
        
        try:
            # –ü–æ–¥—Å—Ç–∞–≤–ª—è–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –≤ –ø—Ä–æ–º–ø—Ç
            formatted_prompt = self._format_prompt(prompt, variables)
            
            logger.info(f"Starting LLM generation with model {self.model}")
            logger.info(f"Prompt length: {len(formatted_prompt)} chars")
            logger.info(f"Estimated input tokens: {variables.get('estimated_input_tokens', 'unknown')}")
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –∫ OpenRouter
            request_data = {
                "model": self.model,
                "messages": [
                    {
                        "role": "user",
                        "content": formatted_prompt
                    }
                ],
                "temperature": temperature,
                "max_tokens": max_tokens,
                "stream": False
            }
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏
            response_data = await self._make_request_with_retries(request_data)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏ –ø–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç
            generated_text = response_data["choices"][0]["message"]["content"]
            
            # –ü–∞—Ä—Å–∏–º JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞
            profile_json = self._extract_and_parse_json(generated_text)
            
            generation_time = time.time() - start_time
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ç–æ–∫–µ–Ω—ã
            usage = response_data.get("usage", {})
            input_tokens = usage.get("prompt_tokens", 0)
            output_tokens = usage.get("completion_tokens", 0)
            total_tokens = usage.get("total_tokens", input_tokens + output_tokens)
            
            logger.info(f"LLM generation completed in {generation_time:.2f}s")
            logger.info(f"Tokens used: {input_tokens} input + {output_tokens} output = {total_tokens} total")
            
            return {
                "profile": profile_json,
                "metadata": {
                    "model": self.model,
                    "generation_time": generation_time,
                    "tokens": {
                        "input": input_tokens,
                        "output": output_tokens,
                        "total": total_tokens
                    },
                    "temperature": temperature,
                    "timestamp": datetime.now().isoformat(),
                    "success": True
                },
                "raw_response": generated_text
            }
            
        except Exception as e:
            generation_time = time.time() - start_time
            logger.error(f"LLM generation failed after {generation_time:.2f}s: {e}")
            
            return {
                "profile": None,
                "metadata": {
                    "model": self.model,
                    "generation_time": generation_time,
                    "error": str(e),
                    "timestamp": datetime.now().isoformat(),
                    "success": False
                },
                "raw_response": None
            }
    
    def _format_prompt(self, prompt: str, variables: Dict[str, Any]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ —Å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫
            formatted = prompt
            
            for key, value in variables.items():
                placeholder = "{{" + key + "}}"
                if placeholder in formatted:
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –≤ —Å—Ç—Ä–æ–∫—É
                    if isinstance(value, (dict, list)):
                        str_value = json.dumps(value, ensure_ascii=False, indent=2)
                    else:
                        str_value = str(value)
                    
                    formatted = formatted.replace(placeholder, str_value)
            
            return formatted
            
        except Exception as e:
            logger.error(f"Error formatting prompt: {e}")
            raise ValueError(f"Failed to format prompt with variables: {e}")
    
    async def _make_request_with_retries(self, request_data: Dict[str, Any]) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –∫ OpenRouter —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        last_exception = None
        
        for attempt in range(self.max_retries):
            try:
                async with httpx.AsyncClient(timeout=self.timeout) as client:
                    response = await client.post(
                        f"{self.base_url}/chat/completions",
                        headers=self.headers,
                        json=request_data
                    )
                
                if response.status_code == 200:
                    return response.json()
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∫–æ–¥—ã –æ—à–∏–±–æ–∫
                if response.status_code == 429:  # Rate limit
                    wait_time = 2 ** attempt
                    logger.warning(f"Rate limit hit, waiting {wait_time}s before retry {attempt + 1}/{self.max_retries}")
                    await asyncio.sleep(wait_time)
                    continue
                
                elif response.status_code in [500, 502, 503, 504]:  # Server errors
                    wait_time = 1 + attempt
                    logger.warning(f"Server error {response.status_code}, waiting {wait_time}s before retry {attempt + 1}/{self.max_retries}")
                    await asyncio.sleep(wait_time)
                    continue
                
                else:
                    # –î–ª—è –¥—Ä—É–≥–∏—Ö –æ—à–∏–±–æ–∫ –Ω–µ –ø–æ–≤—Ç–æ—Ä—è–µ–º
                    error_text = response.text
                    raise Exception(f"HTTP {response.status_code}: {error_text}")
                
            except Exception as e:
                last_exception = e
                if attempt < self.max_retries - 1:
                    wait_time = 1 + attempt
                    logger.warning(f"Request attempt {attempt + 1} failed: {e}, retrying in {wait_time}s")
                    await asyncio.sleep(wait_time)
                else:
                    logger.error(f"All {self.max_retries} attempts failed")
        
        raise last_exception or Exception("All request attempts failed")
    
    def _extract_and_parse_json(self, generated_text: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ –ø–∞—Ä—Å–∏–Ω–≥ JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM"""
        try:
            # –ò—â–µ–º JSON –≤ —Ç–µ–∫—Å—Ç–µ (–º–µ–∂–¥—É ``` –∏–ª–∏ –≤ —á–∏—Å—Ç–æ–º –≤–∏–¥–µ)
            json_text = generated_text.strip()
            
            # –£–±–∏—Ä–∞–µ–º markdown code blocks –µ—Å–ª–∏ –µ—Å—Ç—å
            if json_text.startswith("```"):
                json_text = json_text.split("```")[1]
                if json_text.startswith("json"):
                    json_text = json_text[4:]
                json_text = json_text.strip()
            
            if json_text.endswith("```"):
                json_text = json_text[:-3].strip()
            
            # –ü–∞—Ä—Å–∏–º JSON
            profile_data = json.loads(json_text)
            
            if not isinstance(profile_data, dict):
                raise ValueError("Response is not a JSON object")
            
            logger.info("JSON successfully parsed from LLM response")
            return profile_data
            
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse JSON from LLM response: {e}")
            logger.debug(f"Raw response text: {generated_text[:500]}...")
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º fallback —Å—Ç—Ä—É–∫—Ç—É—Ä—É
            return {
                "error": "Failed to parse JSON from LLM response",
                "raw_response": generated_text,
                "parse_error": str(e)
            }
        
        except Exception as e:
            logger.error(f"Unexpected error while parsing response: {e}")
            return {
                "error": "Unexpected error during response parsing",
                "raw_response": generated_text,
                "parse_error": str(e)
            }
    
    def validate_profile_structure(self, profile: Dict[str, Any]) -> Dict[str, Any]:
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ—Ñ–∏–ª—è
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        """
        validation_result = {
            "is_valid": True,
            "errors": [],
            "warnings": [],
            "completeness_score": 0.0
        }
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
        required_fields = [
            "basic_info", "responsibilities", "professional_skills", 
            "corporate_competencies", "personal_qualities", 
            "education_experience", "career_paths"
        ]
        
        missing_fields = []
        empty_fields = []
        total_fields = len(required_fields)
        filled_fields = 0
        
        for field in required_fields:
            if field not in profile:
                missing_fields.append(field)
            elif not profile[field] or (isinstance(profile[field], (list, dict)) and len(profile[field]) == 0):
                empty_fields.append(field)
            else:
                filled_fields += 1
        
        if missing_fields:
            validation_result["is_valid"] = False
            validation_result["errors"].extend([f"Missing required field: {field}" for field in missing_fields])
        
        if empty_fields:
            validation_result["warnings"].extend([f"Empty required field: {field}" for field in empty_fields])
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–æ–ª–Ω–æ—Ç—É –ø—Ä–æ—Ñ–∏–ª—è
        validation_result["completeness_score"] = filled_fields / total_fields
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–æ–ª—è
        if "basic_info" in profile and isinstance(profile["basic_info"], dict):
            basic_info = profile["basic_info"]
            if not basic_info.get("position_title"):
                validation_result["errors"].append("Missing position_title in basic_info")
        
        logger.info(f"Profile validation completed: {validation_result['completeness_score']:.2%} complete, {len(validation_result['errors'])} errors")
        
        return validation_result
    
    async def test_connection(self) -> Dict[str, Any]:
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ OpenRouter API"""
        try:
            test_request = {
                "model": self.model,
                "messages": [
                    {
                        "role": "user", 
                        "content": "Hello! Please respond with a simple JSON: {\"status\": \"ok\", \"message\": \"Connection test successful\"}"
                    }
                ],
                "max_tokens": 100,
                "temperature": 0.1
            }
            
            response_data = await self._make_request_with_retries(test_request)
            
            return {
                "success": True,
                "model": self.model,
                "response": response_data
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }


# –ò–º–ø–æ—Ä—Ç asyncio –¥–ª—è sleep —Ñ—É–Ω–∫—Ü–∏–π
import asyncio


if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ LLM –∫–ª–∏–µ–Ω—Ç–∞
    import os
    import asyncio
    
    logging.basicConfig(level=logging.INFO)
    
    async def test_llm_client():
        # –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω—É–∂–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è OPENROUTER_API_KEY
        api_key = os.getenv("OPENROUTER_API_KEY")
        
        if not api_key:
            print("‚ùå OPENROUTER_API_KEY not set in environment")
            return
        
        client = LLMClient(api_key)
        
        print("=== –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è ===")
        connection_test = await client.test_connection()
        
        if connection_test["success"]:
            print("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ OpenRouter —É—Å–ø–µ—à–Ω–æ")
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {connection_test['error']}")
            return
        
        print("\n=== –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ ===")
        test_prompt = """
        –°–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫–∏–π JSON –ø—Ä–æ—Ñ–∏–ª—å –¥–æ–ª–∂–Ω–æ—Å—Ç–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:
        
        –î–æ–ª–∂–Ω–æ—Å—Ç—å: {{position}}
        –î–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç: {{department}}
        
        –í–µ—Ä–Ω–∏ JSON –≤ —Ñ–æ—Ä–º–∞—Ç–µ:
        {
          "basic_info": {
            "position_title": "...",
            "department": "..."
          },
          "test": true
        }
        """
        
        test_variables = {
            "position": "–¢–µ—Å—Ç–æ–≤–∞—è –¥–æ–ª–∂–Ω–æ—Å—Ç—å",
            "department": "–¢–µ—Å—Ç–æ–≤—ã–π –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç",
            "estimated_input_tokens": 100
        }
        
        result = await client.generate_profile(test_prompt, test_variables)
        
        if result["metadata"]["success"]:
            print("‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞")
            print(f"üìä –¢–æ–∫–µ–Ω—ã: {result['metadata']['tokens']}")
            print(f"‚è±Ô∏è –í—Ä–µ–º—è: {result['metadata']['generation_time']:.2f}s")
            
            if result["profile"]:
                validation = client.validate_profile_structure(result["profile"])
                print(f"‚úì –í–∞–ª–∏–¥–∞—Ü–∏—è: {validation['completeness_score']:.2%} –∑–∞–ø–æ–ª–Ω–µ–Ω–æ")
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {result['metadata']['error']}")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç
    # asyncio.run(test_llm_client())