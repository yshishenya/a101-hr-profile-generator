"""
LLM –∫–ª–∏–µ–Ω—Ç –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å Gemini 2.5 Flash —á–µ—Ä–µ–∑ Langfuse OpenAI pattern.

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Langfuse –¥–ª—è –ø—Ä–æ–º–ø—Ç–æ–≤ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
- OpenRouter API —á–µ—Ä–µ–∑ langfuse.openai
- Structured output —Å JSON schema
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ç—Ä–µ–π—Å–∏–Ω–≥ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
"""

import json
import logging
import time
from datetime import datetime
from typing import Any, Dict, Optional

from langfuse import Langfuse
from langfuse.openai import OpenAI

from .config import config
from .prompt_manager import PromptManager

logger = logging.getLogger(__name__)


class LLMClient:
    """
    –ö–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Gemini 2.5 Flash —á–µ—Ä–µ–∑ Langfuse OpenAI integration
    """

    def __init__(
        self,
        openrouter_api_key: Optional[str] = None,
        langfuse_public_key: Optional[str] = None,
        langfuse_secret_key: Optional[str] = None,
        langfuse_host: Optional[str] = None,
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM –∫–ª–∏–µ–Ω—Ç–∞ —Å Langfuse –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç config –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–∫ –∏–∑ .env

        Args:
            openrouter_api_key: API –∫–ª—é—á OpenRouter (–∏–ª–∏ –∏–∑ config)
            langfuse_public_key: Langfuse public key (–∏–ª–∏ –∏–∑ config)
            langfuse_secret_key: Langfuse secret key (–∏–ª–∏ –∏–∑ config)
            langfuse_host: Langfuse host URL (–∏–ª–∏ –∏–∑ config)
        """
        # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ config –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω—ã
        self.openrouter_api_key = openrouter_api_key or config.OPENROUTER_API_KEY
        self.langfuse_public_key = langfuse_public_key or config.LANGFUSE_PUBLIC_KEY
        self.langfuse_secret_key = langfuse_secret_key or config.LANGFUSE_SECRET_KEY
        self.langfuse_host = langfuse_host or config.LANGFUSE_HOST

        if not self.openrouter_api_key:
            raise ValueError("OPENROUTER_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env –∏–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Langfuse
        if self.langfuse_public_key and self.langfuse_secret_key:
            self.langfuse = Langfuse(
                public_key=self.langfuse_public_key,
                secret_key=self.langfuse_secret_key,
                host=self.langfuse_host,
            )
            logger.info("‚úÖ Langfuse initialized from config")
        else:
            logger.warning(
                "‚ö†Ô∏è Langfuse credentials not found in config - tracing disabled"
            )
            self.langfuse = None

        # üî• –ù–û–í–ê–Ø –§–ò–ß–ê: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º PromptManager –¥–ª—è fallback –∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏
        self.prompt_manager = PromptManager(
            langfuse_client=self.langfuse,
            templates_dir=config.TEMPLATES_DIR,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—É—Ç—å –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            cache_ttl=300,  # 5 –º–∏–Ω—É—Ç –∫–µ—à
        )
        logger.info("‚úÖ PromptManager initialized with Langfuse sync")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º OpenAI –∫–ª–∏–µ–Ω—Ç —á–µ—Ä–µ–∑ Langfuse
        self.client = OpenAI(
            api_key=self.openrouter_api_key,
            base_url=config.OPENROUTER_BASE_URL,
            default_headers={
                "HTTP-Referer": "https://a101-hr-profiles.local",
                "X-Title": "A101 HR Profile Generator",
            },
        )
        logger.info(f"‚úÖ LLM client initialized with model: {config.OPENROUTER_MODEL}")

    def _create_generation_with_prompt(
        self,
        prompt,
        messages,
        model,
        temperature,
        max_tokens,
        response_format,
        trace_metadata=None,
    ):
        """
        @doc –í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è generation —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Å–≤—è–∑–∫–æ–π –ø—Ä–æ–º–ø—Ç–∞

        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –∫–∞—á–µ—Å—Ç–≤–µ –¥–µ–∫–æ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è Langfuse tracing

        Examples:
            python>
            result = self._create_generation_with_prompt(prompt, messages, model, temp, max_tokens, format, metadata)
        """
        # –û–±–æ–≥–∞—â–µ–Ω–Ω—ã–µ metadata –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ —Ç—Ä–µ–∫–∏–Ω–≥–∞ (—É–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ prompt content)
        enriched_metadata = {
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
            "model": model,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "response_format_type": (
                type(response_format).__name__ if response_format else None
            ),
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–æ–º–ø—Ç–µ (–±–µ–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ)
            "prompt_length": len(str(messages)),
            "messages_count": len(messages),
            # –ù–ï –¥–æ–±–∞–≤–ª—è–µ–º prompt_first_100_chars - –∑–∞–Ω–∏–º–∞–µ—Ç –º–µ—Å—Ç–æ
            # Trace metadata (–∏—Å–∫–ª—é—á–∞–µ—Ç json_schema, company_map, org_structure)
            **(trace_metadata or {}),
        }

        # –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è —Å–≤—è–∑–∫–∞ –ø—Ä–æ–º–ø—Ç–∞ —Å–æ–≥–ª–∞—Å–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ 2025 –≥–æ–¥–∞
        response = self.client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            response_format=response_format,
            langfuse_prompt=prompt,  # üîó –ö–õ–Æ–ß–ï–í–ê–Ø —Å–≤—è–∑–∫–∞ —Å –ø—Ä–æ–º–ø—Ç–æ–º!
            metadata=enriched_metadata,
        )

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ metadata –ø–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞
        if hasattr(response, "usage") and response.usage:
            usage = response.usage
            post_metadata = {
                **enriched_metadata,
                # –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ response
                "response_id": getattr(response, "id", None),
                "created_timestamp": getattr(response, "created", None),
                "actual_model": getattr(response, "model", model),
                "system_fingerprint": getattr(response, "system_fingerprint", None),
                # –ü—Ä–æ–≤–∞–π–¥–µ—Ä –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ OpenRouter
                "provider": (
                    response.model_extra.get("provider", "openrouter")
                    if hasattr(response, "model_extra") and response.model_extra
                    else "openrouter"
                ),
                # –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–æ–∫–µ–Ω–∞—Ö
                "prompt_tokens": usage.prompt_tokens,
                "completion_tokens": usage.completion_tokens,
                "total_tokens": usage.total_tokens,
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–µ—Ç–∞–ª–∏ —Ç–æ–∫–µ–Ω–æ–≤ (–∏–∑ model_extra –µ—Å–ª–∏ –µ—Å—Ç—å)
                "prompt_tokens_cached": 0,
                "completion_reasoning_tokens": 0,
                "completion_image_tokens": 0,
                # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏
                "finish_reason": (
                    response.choices[0].finish_reason if response.choices else None
                ),
                "choice_index": response.choices[0].index if response.choices else None,
                "response_length": (
                    len(response.choices[0].message.content)
                    if response.choices and response.choices[0].message
                    else 0
                ),
                # –ü–æ–ø—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å cost –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é (–º–æ–∂–µ—Ç –±—ã—Ç—å –≤ headers –∏–ª–∏ –¥—Ä—É–≥–∏—Ö –º–µ—Å—Ç–∞—Ö)
                "cost_usd": getattr(usage, "cost", None),
                "cost_estimated": None,  # –ú–æ–∂–µ–º –¥–æ–±–∞–≤–∏—Ç—å —Ä–∞—Å—á—ë—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ–∑–∂–µ
                # –°—Ç–∞—Ç—É—Å –∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä
                "success": True,
                "api_provider": "openrouter",
                "langfuse_sdk_version": "3.3.4",
                "openai_sdk_version": (
                    response.__class__.__module__
                    if hasattr(response.__class__, "__module__")
                    else "unknown"
                ),
            }

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é token –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
            try:
                if hasattr(usage, "model_extra") and usage.model_extra:
                    if "prompt_tokens_details" in usage.model_extra:
                        post_metadata["prompt_tokens_cached"] = usage.model_extra[
                            "prompt_tokens_details"
                        ].get("cached_tokens", 0)
                    if "completion_tokens_details" in usage.model_extra:
                        completion_details = usage.model_extra[
                            "completion_tokens_details"
                        ]
                        post_metadata["completion_reasoning_tokens"] = (
                            completion_details.get("reasoning_tokens", 0)
                        )
                        post_metadata["completion_image_tokens"] = (
                            completion_details.get("image_tokens", 0)
                        )
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Failed to extract detailed token info: {e}")

            # –û–±–Ω–æ–≤–ª—è–µ–º generation —Å –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ (–µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ)
            try:
                if self.langfuse:
                    self.langfuse.update_current_generation(
                        metadata=post_metadata,
                        usage_details={
                            "prompt_tokens": usage.prompt_tokens,
                            "completion_tokens": usage.completion_tokens,
                            "total_tokens": usage.total_tokens,
                        },
                    )
            except Exception as e:
                # –õ–æ–≥–∏—Ä—É–µ–º –Ω–æ –Ω–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
                logger.warning(f"‚ö†Ô∏è Failed to update generation metadata: {e}")

        return response

    def _get_prompt_and_config(
        self, prompt_name: str
    ) -> tuple[Optional[Any], Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –∏–∑ Langfuse –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å fallback.

        Args:
            prompt_name: –ò–º—è –ø—Ä–æ–º–ø—Ç–∞ –≤ Langfuse

        Returns:
            Tuple –∏–∑ (prompt_obj, config)
        """
        prompt_obj = None
        if self.langfuse:
            try:
                prompt_obj = self.langfuse.get_prompt(prompt_name, label="production")
                logger.info(f"‚úÖ Retrieved prompt from Langfuse directly: {prompt_name}")
            except Exception as langfuse_error:
                logger.warning(f"Failed to get prompt from Langfuse: {langfuse_error}")

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Å fallback —á–µ—Ä–µ–∑ PromptManager
        config = self.prompt_manager.get_prompt_config(
            "profile_generation", environment="production"
        )

        return prompt_obj, config

    def _compile_prompt_to_messages(
        self, prompt_obj: Optional[Any], variables: Dict[str, Any]
    ) -> List[Dict[str, str]]:
        """
        –ö–æ–º–ø–∏–ª—è—Ü–∏—è –ø—Ä–æ–º–ø—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç messages.

        Args:
            prompt_obj: –û–±—ä–µ–∫—Ç –ø—Ä–æ–º–ø—Ç–∞ –∏–∑ Langfuse –∏–ª–∏ None
            variables: –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –ø–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–∏

        Returns:
            –°–ø–∏—Å–æ–∫ messages –¥–ª—è API –∑–∞–ø—Ä–æ—Å–∞
        """
        if prompt_obj:
            # –ï—Å–ª–∏ –ø—Ä–æ–º–ø—Ç –ø–æ–ª—É—á–µ–Ω –∏–∑ Langfuse - –∏—Å–ø–æ–ª—å–∑—É–µ–º compile()
            compiled_prompt = prompt_obj.compile(**variables)
            logger.info(f"Compiled prompt using Langfuse compile(), type: {type(compiled_prompt)}")
        else:
            # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º PromptManager –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–æ–º–ø—Ç–∞ –∏ –ø–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–∏
            logger.warning("‚ö†Ô∏è Using local fallback prompt from PromptManager")
            compiled_prompt = self.prompt_manager.get_prompt(
                "profile_generation", variables=variables
            )
            logger.info(f"Compiled prompt using PromptManager fallback, length: {len(compiled_prompt)}")

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç messages –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if isinstance(compiled_prompt, str):
            messages = [{"role": "user", "content": compiled_prompt}]
            logger.info(f"Converted string prompt to messages format, length: {len(compiled_prompt)}")
        elif isinstance(compiled_prompt, list):
            messages = compiled_prompt
            logger.info(f"Using existing messages format, count: {len(messages)}")
        else:
            raise ValueError(f"Unexpected prompt format: {type(compiled_prompt)}")

        return messages

    def _build_trace_metadata(
        self,
        prompt_name: str,
        prompt_obj: Optional[Any],
        variables: Dict[str, Any],
        user_id: Optional[str],
        session_id: Optional[str],
    ) -> Dict[str, Any]:
        """
        –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç—Ä–µ–π—Å–∏–Ω–≥–∞.

        Args:
            prompt_name: –ò–º—è –ø—Ä–æ–º–ø—Ç–∞
            prompt_obj: –û–±—ä–µ–∫—Ç –ø—Ä–æ–º–ø—Ç–∞
            variables: –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            session_id: ID —Å–µ—Å—Å–∏–∏

        Returns:
            –°–ª–æ–≤–∞—Ä—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        """
        return {
            "prompt_name": prompt_name,
            "prompt_version": getattr(prompt_obj, "version", "local_fallback"),
            "department": variables.get("department"),
            "position": variables.get("position"),
            "employee_name": variables.get("employee_name"),
            "user_id": user_id,
            "session_id": session_id,
            "environment": "production",
            "source": "hr_profile_generator",
            "prompt_source": "langfuse" if prompt_obj else "local_fallback",
        }

    def _build_success_response(
        self,
        profile_json: Dict[str, Any],
        generated_text: str,
        model: str,
        generation_time: float,
        usage: Any,
        prompt_obj: Optional[Any],
        prompt_name: str,
        variables: Dict[str, Any],
    ) -> Dict[str, Any]:
        """
        –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.

        Args:
            profile_json: –†–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–π JSON –ø—Ä–æ—Ñ–∏–ª—è
            generated_text: –°—ã—Ä–æ–π —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞
            model: –ò–º—è –º–æ–¥–µ–ª–∏
            generation_time: –í—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            usage: –û–±—ä–µ–∫—Ç —Å –¥–∞–Ω–Ω—ã–º–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤
            prompt_obj: –û–±—ä–µ–∫—Ç –ø—Ä–æ–º–ø—Ç–∞
            prompt_name: –ò–º—è –ø—Ä–æ–º–ø—Ç–∞
            variables: –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        """
        input_tokens = usage.prompt_tokens if usage else 0
        output_tokens = usage.completion_tokens if usage else 0
        total_tokens = usage.total_tokens if usage else input_tokens + output_tokens

        # –ü–æ–ª—É—á–∞–µ–º trace_id –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä—ã
        trace_id = None
        try:
            from langfuse.decorators import langfuse_context
            trace_id = langfuse_context.get_current_trace_id()
            logger.info("‚úÖ Langfuse tracing with decorators completed")
        except ImportError:
            logger.info("‚úÖ Basic Langfuse tracing completed")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to get trace ID: {e}")

        return {
            "profile": profile_json,
            "metadata": {
                "model": model,
                "generation_time": generation_time,
                "tokens": {
                    "input": input_tokens,
                    "output": output_tokens,
                    "total": total_tokens,
                },
                "temperature": self.prompt_manager.get_prompt_config("profile_generation").get("temperature", 0.1),
                "timestamp": datetime.now().isoformat(),
                "success": True,
                "langfuse_trace_id": trace_id,
                "prompt_name": prompt_name,
                "prompt_version": getattr(prompt_obj, "version", "local_fallback"),
                "prompt_source": "langfuse" if prompt_obj else "local_fallback",
                "tracing_mode": "decorator_based",
                "department": variables.get("department"),
                "position": variables.get("position"),
            },
            "raw_response": generated_text,
        }

    def generate_profile_from_langfuse(
        self,
        prompt_name: str,
        variables: Dict[str, Any],
        user_id: Optional[str] = None,
        session_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        @doc –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ—Ñ–∏–ª—è –¥–æ–ª–∂–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ Langfuse prompt —Å traced execution

        Args:
            prompt_name: –ò–º—è –ø—Ä–æ–º–ø—Ç–∞ –≤ Langfuse
            variables: –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –ø–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–∏ –≤ –ø—Ä–æ–º–ø—Ç
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è —Ç—Ä–µ–π—Å–∏–Ω–≥–∞
            session_id: ID —Å–µ—Å—Å–∏–∏ –¥–ª—è —Ç—Ä–µ–π—Å–∏–Ω–≥–∞

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏

        Examples:
            python>
            client = LLMClient(api_key, langfuse_keys...)
            result = client.generate_profile_from_langfuse(
                prompt_name="a101-hr-profile-gemini-v2",
                variables={"position": "Senior Developer", "department": "IT"}
            )
        """
        start_time = time.time()

        try:
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–º–ø—Ç –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            prompt_obj, config = self._get_prompt_and_config(prompt_name)

            model = config.get("model", "google/gemini-2.5-flash")
            temperature = config.get("temperature", 0.1)
            max_tokens = config.get("max_tokens", 4000)
            response_format = config.get("response_format")

            logger.info(f"Starting generation with prompt: {prompt_name}")
            logger.info(f"Model: {model}, Temperature: {temperature}")

            # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –≤ —Ñ–æ—Ä–º–∞—Ç messages
            messages = self._compile_prompt_to_messages(prompt_obj, variables)
            logger.info(f"Final messages count: {len(messages)}")

            # –°—Ç—Ä–æ–∏–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç—Ä–µ–π—Å–∏–Ω–≥–∞
            trace_metadata = self._build_trace_metadata(
                prompt_name, prompt_obj, variables, user_id, session_id
            )

            # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é —Å –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä–æ–º –¥–ª—è —Å–≤—è–∑–∫–∏ –ø—Ä–æ–º–ø—Ç–æ–≤
            try:
                response = self._create_generation_with_prompt(
                    prompt=prompt_obj,  # –ú–æ–∂–µ—Ç –±—ã—Ç—å None –ø—Ä–∏ fallback
                    messages=messages,
                    model=model,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    response_format=response_format,
                    trace_metadata=trace_metadata,
                )
                logger.info("‚úÖ Generation with prompt linking completed")
            except Exception as api_error:
                logger.error(f"OpenAI API error: {api_error}")
                logger.error(f"Error type: {type(api_error)}")
                raise api_error

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç–≤–µ—Ç –∏ –ø–∞—Ä—Å–∏–º JSON
            generated_text = response.choices[0].message.content
            profile_json = self._extract_and_parse_json(generated_text)

            generation_time = time.time() - start_time
            logger.info(f"Langfuse generation completed in {generation_time:.2f}s")

            # –°—Ç—Ä–æ–∏–º –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —É—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç
            return self._build_success_response(
                profile_json,
                generated_text,
                model,
                generation_time,
                response.usage,
                prompt_obj,
                prompt_name,
                variables,
            )

        except Exception as e:
            generation_time = time.time() - start_time
            logger.error(
                f"Langfuse generation failed after {generation_time:.2f}s: {e}"
            )

            # –ü–æ–ª—É—á–∞–µ–º trace_id –¥–∞–∂–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
            try:
                from langfuse.decorators import langfuse_context

                trace_id = langfuse_context.get_current_trace_id()
            except (ImportError, NameError):
                trace_id = None

            return {
                "profile": None,
                "metadata": {
                    "model": "unknown",
                    "generation_time": generation_time,
                    "error": str(e),
                    "timestamp": datetime.now().isoformat(),
                    "success": False,
                    "langfuse_trace_id": trace_id,
                },
                "raw_response": None,
            }

    def _extract_and_parse_json(self, generated_text: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ –ø–∞—Ä—Å–∏–Ω–≥ JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM"""
        try:
            # –ò—â–µ–º JSON –≤ —Ç–µ–∫—Å—Ç–µ (–º–µ–∂–¥—É ``` –∏–ª–∏ –≤ —á–∏—Å—Ç–æ–º –≤–∏–¥–µ)
            json_text = generated_text.strip()

            # –£–±–∏—Ä–∞–µ–º markdown code blocks –µ—Å–ª–∏ –µ—Å—Ç—å
            if json_text.startswith("```"):
                json_text = json_text.split("```")[1]
                if json_text.startswith("json"):
                    json_text = json_text[4:]
                json_text = json_text.strip()

            if json_text.endswith("```"):
                json_text = json_text[:-3].strip()

            # –ü–∞—Ä—Å–∏–º JSON
            profile_data = json.loads(json_text)

            if not isinstance(profile_data, dict):
                raise ValueError("Response is not a JSON object")

            logger.info("JSON successfully parsed from LLM response")
            return profile_data

        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse JSON from LLM response: {e}")
            logger.debug(f"Raw response text: {generated_text[:500]}...")

            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º fallback —Å—Ç—Ä—É–∫—Ç—É—Ä—É
            return {
                "error": "Failed to parse JSON from LLM response",
                "raw_response": generated_text,
                "parse_error": str(e),
            }

        except Exception as e:
            logger.error(f"Unexpected error while parsing response: {e}")
            return {
                "error": "Unexpected error during response parsing",
                "raw_response": generated_text,
                "parse_error": str(e),
            }

    def validate_profile_structure(self, profile: Dict[str, Any]) -> Dict[str, Any]:
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ—Ñ–∏–ª—è

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        """
        validation_result = {
            "is_valid": True,
            "errors": [],
            "warnings": [],
            "completeness_score": 0.0,
        }

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è (—Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å –Ω–æ–≤–æ–π —Å—Ö–µ–º–æ–π JSON)
        required_fields = [
            "position_title",
            "department_broad",
            "department_specific",
            "position_category",  # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–æ —Å "category"
            "direct_manager",  # –ù–æ–≤–æ–µ –ø–æ–ª–µ
            "subordinates",  # –ù–æ–≤–æ–µ –ø–æ–ª–µ
            "primary_activity_type",  # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–æ —Å "primary_activity"
            "responsibility_areas",
            "professional_skills",
            "corporate_competencies",  # –ù–æ–≤–æ–µ –ø–æ–ª–µ
            "personal_qualities",
            "experience_and_education",  # –ù–æ–≤–æ–µ –ø–æ–ª–µ
            "careerogram",  # –ù–æ–≤–æ–µ –ø–æ–ª–µ
            "workplace_provisioning",  # –ù–æ–≤–æ–µ –ø–æ–ª–µ
            "performance_metrics",  # –ù–æ–≤–æ–µ –ø–æ–ª–µ
            "additional_information",  # –ù–æ–≤–æ–µ –ø–æ–ª–µ
            "metadata",  # –ù–æ–≤–æ–µ –ø–æ–ª–µ
        ]

        missing_fields = []
        empty_fields = []
        total_fields = len(required_fields)
        filled_fields = 0

        # –í–∞–ª–∏–¥–∞—Ü–∏—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π fallback –¥–ª—è —Å—Ç–∞—Ä—ã—Ö –ø–æ–ª–µ–π
        for field in required_fields:
            # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ fallback –¥–ª—è –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–ª–µ–π
            fallback_map = {
                "position_category": "category",
                "primary_activity_type": "primary_activity",
                "experience_and_education": "education",
                "careerogram": "career_path",
                "workplace_provisioning": "technical_requirements",
            }

            field_value = profile.get(field)
            # –ï—Å–ª–∏ –ø–æ–ª–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –ø—Ä–æ–≤–µ—Ä—è–µ–º fallback
            if field_value is None and field in fallback_map:
                field_value = profile.get(fallback_map[field])

            if field_value is None:
                missing_fields.append(field)
            elif not field_value or (
                isinstance(field_value, (list, dict)) and len(field_value) == 0
            ):
                empty_fields.append(field)
            else:
                filled_fields += 1

        if missing_fields:
            validation_result["is_valid"] = False
            validation_result["errors"].extend(
                [f"Missing required field: {field}" for field in missing_fields]
            )

        if empty_fields:
            validation_result["warnings"].extend(
                [f"Empty required field: {field}" for field in empty_fields]
            )

        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–æ–ª–Ω–æ—Ç—É –ø—Ä–æ—Ñ–∏–ª—è
        validation_result["completeness_score"] = filled_fields / total_fields

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–ª—è –Ω–æ–≤—ã—Ö –ø–æ–ª–µ–π
        self._validate_responsibility_areas(profile, validation_result)
        self._validate_professional_skills(profile, validation_result)
        self._validate_subordinates(profile, validation_result)
        self._validate_careerogram(profile, validation_result)
        self._validate_performance_metrics(profile, validation_result)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–æ–ª—è (legacy)
        if "basic_info" in profile and isinstance(profile["basic_info"], dict):
            basic_info = profile["basic_info"]
            if not basic_info.get("position_title"):
                validation_result["errors"].append(
                    "Missing position_title in basic_info"
                )

        logger.info(
            f"Profile validation completed: {validation_result['completeness_score']:.2%} complete, {len(validation_result['errors'])} errors"
        )

        return validation_result

    def _validate_responsibility_areas(
        self, profile: Dict[str, Any], validation_result: Dict[str, Any]
    ):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –æ–±–ª–∞—Å—Ç–µ–π –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏"""
        if "responsibility_areas" in profile:
            areas = profile["responsibility_areas"]
            if isinstance(areas, list):
                for i, area in enumerate(areas):
                    if not isinstance(area, dict):
                        validation_result["warnings"].append(
                            f"–û–±–ª–∞—Å—Ç—å –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ {i+1} –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –æ–±—ä–µ–∫—Ç–æ–º"
                        )
                    else:
                        # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –∫–∞–∫ —Å—Ç–∞—Ä–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ (title), —Ç–∞–∫ –∏ –Ω–æ–≤–æ–≥–æ (area)
                        if "area" not in area and "title" not in area:
                            validation_result["warnings"].append(
                                f"–û–±–ª–∞—Å—Ç—å –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ {i+1} –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å 'area' –∏–ª–∏ 'title'"
                            )
                        if "tasks" not in area:
                            validation_result["warnings"].append(
                                f"–û–±–ª–∞—Å—Ç—å –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ {i+1} –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å 'tasks'"
                            )

    def _validate_professional_skills(
        self, profile: Dict[str, Any], validation_result: Dict[str, Any]
    ):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –Ω–∞–≤—ã–∫–æ–≤"""
        if "professional_skills" in profile:
            skills = profile["professional_skills"]
            if isinstance(skills, list):
                for i, skill_category in enumerate(skills):
                    if not isinstance(skill_category, dict):
                        validation_result["warnings"].append(
                            f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–∞–≤—ã–∫–æ–≤ {i+1} –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –æ–±—ä–µ–∫—Ç–æ–º"
                        )
                    else:
                        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è skill_category –∏–ª–∏ category (fallback)
                        if (
                            "skill_category" not in skill_category
                            and "category" not in skill_category
                        ):
                            validation_result["warnings"].append(
                                f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–∞–≤—ã–∫–æ–≤ {i+1} –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å 'skill_category'"
                            )

                        # –ü—Ä–æ–≤–µ—Ä–∫–∞ specific_skills –∏–ª–∏ skills (fallback)
                        skills_field = skill_category.get(
                            "specific_skills", skill_category.get("skills")
                        )
                        if not skills_field:
                            validation_result["warnings"].append(
                                f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–∞–≤—ã–∫–æ–≤ {i+1} –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å 'specific_skills'"
                            )
                        elif isinstance(skills_field, list) and len(skills_field) > 0:
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–µ—Ä–≤–æ–≥–æ –Ω–∞–≤—ã–∫–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ñ–æ—Ä–º–∞—Ç–∞
                            first_skill = skills_field[0]
                            if isinstance(first_skill, dict):
                                # –ù–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ –Ω–∞–≤—ã–∫–∞–º–∏
                                for j, skill in enumerate(skills_field):
                                    required_skill_fields = [
                                        "skill_name",
                                        "proficiency_level",
                                        "proficiency_description",
                                    ]
                                    for field in required_skill_fields:
                                        if field not in skill:
                                            validation_result["warnings"].append(
                                                f"–ù–∞–≤—ã–∫ {j+1} –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {i+1} –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å '{field}'"
                                            )

    def _validate_subordinates(
        self, profile: Dict[str, Any], validation_result: Dict[str, Any]
    ):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø–æ–¥—á–∏–Ω–µ–Ω–Ω—ã—Ö"""
        if "subordinates" in profile:
            subordinates = profile["subordinates"]
            if isinstance(subordinates, dict):
                if "departments" not in subordinates:
                    validation_result["warnings"].append(
                        "–ü–æ–ª–µ 'subordinates' –¥–æ–ª–∂–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç—å 'departments'"
                    )
                # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –∫–∞–∫ direct_reports, —Ç–∞–∫ –∏ people (fallback)
                if (
                    "direct_reports" not in subordinates
                    and "people" not in subordinates
                ):
                    validation_result["warnings"].append(
                        "–ü–æ–ª–µ 'subordinates' –¥–æ–ª–∂–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç—å 'direct_reports'"
                    )

    def _validate_careerogram(
        self, profile: Dict[str, Any], validation_result: Dict[str, Any]
    ):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∫–∞—Ä—å–µ—Ä–æ–≥—Ä–∞–º–º—ã"""
        # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –∏ –Ω–æ–≤–æ–π (careerogram) –∏ —Å—Ç–∞—Ä–æ–π (career_path) —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        careerogram = profile.get("careerogram") or profile.get("career_path")
        if careerogram and isinstance(careerogram, dict):
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã career_pathways
            if "career_pathways" in careerogram:
                career_pathways = careerogram["career_pathways"]
                if isinstance(career_pathways, list):
                    for i, pathway in enumerate(career_pathways):
                        if not isinstance(pathway, dict):
                            validation_result["warnings"].append(
                                f"–ö–∞—Ä—å–µ—Ä–Ω—ã–π –ø—É—Ç—å {i+1} –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ–±—ä–µ–∫—Ç–æ–º"
                            )
                        else:
                            required_pathway_fields = [
                                "pathway_type",
                                "entry_positions",
                                "advancement_positions",
                            ]
                            for field in required_pathway_fields:
                                if field not in pathway:
                                    validation_result["warnings"].append(
                                        f"–ö–∞—Ä—å–µ—Ä–Ω—ã–π –ø—É—Ç—å {i+1} –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å '{field}'"
                                    )
            else:
                # –î–ª—è —Å—Ç–∞—Ä–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–≤–µ—Ä—è–µ–º legacy –ø–æ–ª—è
                legacy_fields = ["donor_positions", "target_positions"]
                missing_legacy = [
                    field for field in legacy_fields if field not in careerogram
                ]
                if missing_legacy:
                    validation_result["warnings"].append(
                        f"–ö–∞—Ä—å–µ—Ä–æ–≥—Ä–∞–º–º–∞ –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å {missing_legacy}"
                    )

    def _validate_performance_metrics(
        self, profile: Dict[str, Any], validation_result: Dict[str, Any]
    ):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –º–µ—Ç—Ä–∏–∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏"""
        if "performance_metrics" in profile:
            metrics = profile["performance_metrics"]
            if isinstance(metrics, dict):
                required_fields = [
                    "quantitative_kpis",
                    "qualitative_indicators",
                    "evaluation_frequency",
                ]
                for field in required_fields:
                    if field not in metrics:
                        validation_result["warnings"].append(
                            f"performance_metrics –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å '{field}'"
                        )

    def test_connection(self) -> Dict[str, Any]:
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ Langfuse –∫ OpenRouter API"""
        try:
            response = self.client.chat.completions.create(
                model="google/gemini-2.5-flash",
                messages=[
                    {
                        "role": "user",
                        "content": 'Hello! Please respond with a simple JSON: {"status": "ok", "message": "Connection test successful"}',
                    }
                ],
                max_tokens=100,
                temperature=0.1,
            )

            return {
                "success": True,
                "model": "google/gemini-2.5-flash",
                "response": response.choices[0].message.content,
            }

        except Exception as e:
            return {"success": False, "error": str(e)}


if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–æ–≥–æ LLM –∫–ª–∏–µ–Ω—Ç–∞ —Å Langfuse
    import os

    logging.basicConfig(level=logging.INFO)

    def test_llm_client():
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–ª—é—á–∏
        openrouter_key = os.getenv("OPENROUTER_API_KEY")
        langfuse_public = os.getenv("LANGFUSE_PUBLIC_KEY")
        langfuse_secret = os.getenv("LANGFUSE_SECRET_KEY")

        if not all([openrouter_key, langfuse_public, langfuse_secret]):
            print("‚ùå Missing required API keys")
            print(f"  OpenRouter: {'‚úÖ' if openrouter_key else '‚ùå'}")
            print(f"  Langfuse public: {'‚úÖ' if langfuse_public else '‚ùå'}")
            print(f"  Langfuse secret: {'‚úÖ' if langfuse_secret else '‚ùå'}")
            return

        client = LLMClient(
            openrouter_api_key=openrouter_key,
            langfuse_public_key=langfuse_public,
            langfuse_secret_key=langfuse_secret,
        )

        print("=== –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è ===")
        connection_test = client.test_connection()

        if connection_test["success"]:
            print("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ Langfuse —É—Å–ø–µ—à–Ω–æ")
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {connection_test['error']}")
            return

        print("\n=== –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ Langfuse ===")
        test_variables = {
            "position": "Senior ML Engineer",
            "department": "–î–ò–¢",
            "employee_name": "–¢–µ—Å—Ç–æ–≤–∞—è –ì–µ–Ω–µ—Ä–∞—Ü–∏—è",
            "org_structure": "–¢–µ—Å—Ç–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞",
            "kpi_data": "–¢–µ—Å—Ç–æ–≤—ã–µ KPI",
            "it_systems": "–¢–µ—Å—Ç–æ–≤—ã–µ —Å–∏—Å—Ç–µ–º—ã",
        }

        result = client.generate_profile_from_langfuse(
            prompt_name="a101-hr-profile-gemini-v2",
            variables=test_variables,
            user_id="test-user",
            session_id="test-session",
        )

        if result["metadata"]["success"]:
            print("‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ Langfuse —É—Å–ø–µ—à–Ω–∞")
            print(f"üìä –¢–æ–∫–µ–Ω—ã: {result['metadata']['tokens']}")
            print(f"‚è±Ô∏è –í—Ä–µ–º—è: {result['metadata']['generation_time']:.2f}s")
            print(f"üîó Trace ID: {result['metadata']['langfuse_trace_id']}")

            if result["profile"]:
                validation = client.validate_profile_structure(result["profile"])
                print(f"‚úì –í–∞–ª–∏–¥–∞—Ü–∏—è: {validation['completeness_score']:.2%} –∑–∞–ø–æ–ª–Ω–µ–Ω–æ")
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {result['metadata']['error']}")

    # –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Ç–µ—Å—Ç–∞
    # test_llm_client()
