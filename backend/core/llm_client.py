"""
LLM –∫–ª–∏–µ–Ω—Ç –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å Gemini 2.5 Flash —á–µ—Ä–µ–∑ Langfuse OpenAI pattern.

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Langfuse –¥–ª—è –ø—Ä–æ–º–ø—Ç–æ–≤ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
- OpenRouter API —á–µ—Ä–µ–∑ langfuse.openai
- Structured output —Å JSON schema
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ç—Ä–µ–π—Å–∏–Ω–≥ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
"""

import json
import logging
import time
from datetime import datetime
from typing import Any, Dict, List, Optional

import httpx
from langfuse import Langfuse
from langfuse.openai import AsyncOpenAI

from .config import config
from .prompt_manager import PromptManager

logger = logging.getLogger(__name__)

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
PROMPT_CACHE_TTL_SECONDS = 300  # 5 –º–∏–Ω—É—Ç –∫–µ—à –¥–ª—è –ø—Ä–æ–º–ø—Ç–æ–≤


class LLMClient:
    """
    –ö–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Gemini 2.5 Flash —á–µ—Ä–µ–∑ Langfuse OpenAI integration
    """

    def __init__(
        self,
        openrouter_api_key: Optional[str] = None,
        langfuse_public_key: Optional[str] = None,
        langfuse_secret_key: Optional[str] = None,
        langfuse_host: Optional[str] = None,
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM –∫–ª–∏–µ–Ω—Ç–∞ —Å Langfuse –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç config –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–∫ –∏–∑ .env

        Args:
            openrouter_api_key: API –∫–ª—é—á OpenRouter (–∏–ª–∏ –∏–∑ config)
            langfuse_public_key: Langfuse public key (–∏–ª–∏ –∏–∑ config)
            langfuse_secret_key: Langfuse secret key (–∏–ª–∏ –∏–∑ config)
            langfuse_host: Langfuse host URL (–∏–ª–∏ –∏–∑ config)
        """
        # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ config –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω—ã
        self.openrouter_api_key = openrouter_api_key or config.OPENROUTER_API_KEY
        self.langfuse_public_key = langfuse_public_key or config.LANGFUSE_PUBLIC_KEY
        self.langfuse_secret_key = langfuse_secret_key or config.LANGFUSE_SECRET_KEY
        self.langfuse_host = langfuse_host or config.LANGFUSE_HOST

        if not self.openrouter_api_key:
            raise ValueError("OPENROUTER_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env –∏–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Langfuse
        if self.langfuse_public_key and self.langfuse_secret_key:
            self.langfuse = Langfuse(
                public_key=self.langfuse_public_key,
                secret_key=self.langfuse_secret_key,
                host=self.langfuse_host,
            )
            logger.info("‚úÖ Langfuse initialized from config")
        else:
            logger.warning(
                "‚ö†Ô∏è Langfuse credentials not found in config - tracing disabled"
            )
            self.langfuse = None

        # üî• –ù–û–í–ê–Ø –§–ò–ß–ê: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º PromptManager –¥–ª—è fallback –∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏
        self.prompt_manager = PromptManager(
            langfuse_client=self.langfuse,
            templates_dir=config.TEMPLATES_DIR,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—É—Ç—å –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            cache_ttl=PROMPT_CACHE_TTL_SECONDS,
        )
        logger.info("‚úÖ PromptManager initialized with Langfuse sync")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º AsyncOpenAI –∫–ª–∏–µ–Ω—Ç —á–µ—Ä–µ–∑ Langfuse –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã
        self.client = AsyncOpenAI(
            api_key=self.openrouter_api_key,
            base_url=config.OPENROUTER_BASE_URL,
            default_headers={
                "HTTP-Referer": "https://a101-hr-profiles.local",
                "X-Title": "A101 HR Profile Generator",
            },
        )
        logger.info(f"‚úÖ Async LLM client initialized with model: {config.OPENROUTER_MODEL}")

    async def _create_generation_with_prompt(
        self,
        prompt: Optional[Any],
        messages: List[Dict[str, str]],
        model: str,
        temperature: float,
        max_tokens: int,
        response_format: Optional[Any],
        trace_metadata: Optional[Dict[str, Any]] = None,
    ) -> Any:
        """
        –í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è generation —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Å–≤—è–∑–∫–æ–π –ø—Ä–æ–º–ø—Ç–∞.

        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –∫–∞—á–µ—Å—Ç–≤–µ –¥–µ–∫–æ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è Langfuse tracing.
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤ OpenAI API —á–µ—Ä–µ–∑ AsyncOpenAI –∫–ª–∏–µ–Ω—Ç.

        Args:
            prompt: Langfuse prompt object (–º–æ–∂–µ—Ç –±—ã—Ç—å None –ø—Ä–∏ fallback)
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è API –≤ —Ñ–æ—Ä–º–∞—Ç–µ ChatML
            model: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "google/gemini-2.5-flash")
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (0.0-2.0)
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ
            response_format: –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ (JSON schema –∏–ª–∏ None)
            trace_metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç—Ä–µ–π—Å–∏–Ω–≥–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

        Returns:
            ChatCompletion response –æ–±—ä–µ–∫—Ç –æ—Ç OpenAI SDK

        Raises:
            httpx.HTTPStatusError: –ü—Ä–∏ –æ—à–∏–±–∫–∞—Ö HTTP (4xx, 5xx)
            httpx.TimeoutException: –ü—Ä–∏ —Ç–∞–π–º–∞—É—Ç–µ –∑–∞–ø—Ä–æ—Å–∞

        Examples:
            >>> result = await self._create_generation_with_prompt(
            ...     prompt, messages, "google/gemini-2.5-flash", 0.1, 4000, None, {}
            ... )
        """
        # –û–±–æ–≥–∞—â–µ–Ω–Ω—ã–µ metadata –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ —Ç—Ä–µ–∫–∏–Ω–≥–∞ (—É–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ prompt content)
        enriched_metadata = {
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
            "model": model,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "response_format_type": (
                type(response_format).__name__ if response_format else None
            ),
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–æ–º–ø—Ç–µ (–±–µ–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ)
            "prompt_length": len(str(messages)),
            "messages_count": len(messages),
            # –ù–ï –¥–æ–±–∞–≤–ª—è–µ–º prompt_first_100_chars - –∑–∞–Ω–∏–º–∞–µ—Ç –º–µ—Å—Ç–æ
            # Trace metadata (–∏—Å–∫–ª—é—á–∞–µ—Ç json_schema, company_map, org_structure)
            **(trace_metadata or {}),
        }

        # –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è —Å–≤—è–∑–∫–∞ –ø—Ä–æ–º–ø—Ç–∞ —Å–æ–≥–ª–∞—Å–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ 2025 –≥–æ–¥–∞ (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤)
        response = await self.client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            response_format=response_format,
            langfuse_prompt=prompt,  # üîó –ö–õ–Æ–ß–ï–í–ê–Ø —Å–≤—è–∑–∫–∞ —Å –ø—Ä–æ–º–ø—Ç–æ–º!
            metadata=enriched_metadata,
        )

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ metadata –ø–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞
        if hasattr(response, "usage") and response.usage:
            usage = response.usage
            post_metadata = {
                **enriched_metadata,
                # –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ response
                "response_id": getattr(response, "id", None),
                "created_timestamp": getattr(response, "created", None),
                "actual_model": getattr(response, "model", model),
                "system_fingerprint": getattr(response, "system_fingerprint", None),
                # –ü—Ä–æ–≤–∞–π–¥–µ—Ä –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ OpenRouter
                "provider": (
                    response.model_extra.get("provider", "openrouter")
                    if hasattr(response, "model_extra") and response.model_extra
                    else "openrouter"
                ),
                # –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–æ–∫–µ–Ω–∞—Ö
                "prompt_tokens": usage.prompt_tokens,
                "completion_tokens": usage.completion_tokens,
                "total_tokens": usage.total_tokens,
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–µ—Ç–∞–ª–∏ —Ç–æ–∫–µ–Ω–æ–≤ (–∏–∑ model_extra –µ—Å–ª–∏ –µ—Å—Ç—å)
                "prompt_tokens_cached": 0,
                "completion_reasoning_tokens": 0,
                "completion_image_tokens": 0,
                # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏
                "finish_reason": (
                    response.choices[0].finish_reason if response.choices else None
                ),
                "choice_index": response.choices[0].index if response.choices else None,
                "response_length": (
                    len(response.choices[0].message.content)
                    if response.choices and response.choices[0].message
                    else 0
                ),
                # –ü–æ–ø—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å cost –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é (–º–æ–∂–µ—Ç –±—ã—Ç—å –≤ headers –∏–ª–∏ –¥—Ä—É–≥–∏—Ö –º–µ—Å—Ç–∞—Ö)
                "cost_usd": getattr(usage, "cost", None),
                "cost_estimated": None,  # –ú–æ–∂–µ–º –¥–æ–±–∞–≤–∏—Ç—å —Ä–∞—Å—á—ë—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ–∑–∂–µ
                # –°—Ç–∞—Ç—É—Å –∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä
                "success": True,
                "api_provider": "openrouter",
                "langfuse_sdk_version": "3.3.4",
                "openai_sdk_version": (
                    response.__class__.__module__
                    if hasattr(response.__class__, "__module__")
                    else "unknown"
                ),
            }

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é token –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
            try:
                if hasattr(usage, "model_extra") and usage.model_extra:
                    if "prompt_tokens_details" in usage.model_extra:
                        post_metadata["prompt_tokens_cached"] = usage.model_extra[
                            "prompt_tokens_details"
                        ].get("cached_tokens", 0)
                    if "completion_tokens_details" in usage.model_extra:
                        completion_details = usage.model_extra[
                            "completion_tokens_details"
                        ]
                        post_metadata["completion_reasoning_tokens"] = (
                            completion_details.get("reasoning_tokens", 0)
                        )
                        post_metadata["completion_image_tokens"] = (
                            completion_details.get("image_tokens", 0)
                        )
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Failed to extract detailed token info: {e}")

            # –û–±–Ω–æ–≤–ª—è–µ–º generation —Å –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ (–µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ)
            try:
                if self.langfuse:
                    self.langfuse.update_current_generation(
                        metadata=post_metadata,
                        usage_details={
                            "prompt_tokens": usage.prompt_tokens,
                            "completion_tokens": usage.completion_tokens,
                            "total_tokens": usage.total_tokens,
                        },
                    )
            except Exception as e:
                # –õ–æ–≥–∏—Ä—É–µ–º –Ω–æ –Ω–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
                logger.warning(f"‚ö†Ô∏è Failed to update generation metadata: {e}")

        return response

    def _get_prompt_and_config(
        self, prompt_name: str
    ) -> tuple[Optional[Any], Dict[str, Any]]:
        """Retrieve a prompt and its configuration from Langfuse with fallback."""
        prompt_obj = None
        if self.langfuse:
            try:
                prompt_obj = self.langfuse.get_prompt(prompt_name, label="production")
                logger.info(f"‚úÖ Retrieved prompt from Langfuse directly: {prompt_name}")
            except (httpx.HTTPError, ConnectionError, TimeoutError) as langfuse_error:
                logger.warning(
                    "Failed to get prompt from Langfuse",
                    extra={
                        "error": str(langfuse_error),
                        "prompt_name": prompt_name,
                        "error_type": type(langfuse_error).__name__
                    }
                )
            except Exception as e:
                logger.exception(
                    "Unexpected error getting prompt from Langfuse",
                    extra={"prompt_name": prompt_name}
                )

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Å fallback —á–µ—Ä–µ–∑ PromptManager
        config = self.prompt_manager.get_prompt_config(
            "profile_generation", environment="production"
        )

        return prompt_obj, config

    def _compile_prompt_to_messages(
        self, prompt_obj: Optional[Any], variables: Dict[str, Any]
    ) -> List[Dict[str, str]]:
        """
        –ö–æ–º–ø–∏–ª—è—Ü–∏—è –ø—Ä–æ–º–ø—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç messages.

        Args:
            prompt_obj: –û–±—ä–µ–∫—Ç –ø—Ä–æ–º–ø—Ç–∞ –∏–∑ Langfuse –∏–ª–∏ None
            variables: –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –ø–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–∏

        Returns:
            –°–ø–∏—Å–æ–∫ messages –¥–ª—è API –∑–∞–ø—Ä–æ—Å–∞
        """
        if prompt_obj:
            # –ï—Å–ª–∏ –ø—Ä–æ–º–ø—Ç –ø–æ–ª—É—á–µ–Ω –∏–∑ Langfuse - –∏—Å–ø–æ–ª—å–∑—É–µ–º compile()
            compiled_prompt = prompt_obj.compile(**variables)
            logger.info(f"Compiled prompt using Langfuse compile(), type: {type(compiled_prompt)}")
        else:
            # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º PromptManager –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–æ–º–ø—Ç–∞ –∏ –ø–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–∏
            logger.warning("‚ö†Ô∏è Using local fallback prompt from PromptManager")
            compiled_prompt = self.prompt_manager.get_prompt(
                "profile_generation", variables=variables
            )
            logger.info(f"Compiled prompt using PromptManager fallback, length: {len(compiled_prompt)}")

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç messages –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if isinstance(compiled_prompt, str):
            messages = [{"role": "user", "content": compiled_prompt}]
            logger.info(f"Converted string prompt to messages format, length: {len(compiled_prompt)}")
        elif isinstance(compiled_prompt, list):
            messages = compiled_prompt
            logger.info(f"Using existing messages format, count: {len(messages)}")
        else:
            raise ValueError(f"Unexpected prompt format: {type(compiled_prompt)}")

        return messages

    def _build_trace_metadata(
        self,
        prompt_name: str,
        prompt_obj: Optional[Any],
        variables: Dict[str, Any],
        user_id: Optional[str],
        session_id: Optional[str],
    ) -> Dict[str, Any]:
        """
        –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç—Ä–µ–π—Å–∏–Ω–≥–∞.

        Args:
            prompt_name: –ò–º—è –ø—Ä–æ–º–ø—Ç–∞
            prompt_obj: –û–±—ä–µ–∫—Ç –ø—Ä–æ–º–ø—Ç–∞
            variables: –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            session_id: ID —Å–µ—Å—Å–∏–∏

        Returns:
            –°–ª–æ–≤–∞—Ä—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        """
        return {
            "prompt_name": prompt_name,
            "prompt_version": getattr(prompt_obj, "version", "local_fallback"),
            "department": variables.get("department"),
            "position": variables.get("position"),
            "employee_name": variables.get("employee_name"),
            "user_id": user_id,
            "session_id": session_id,
            "environment": "production",
            "source": "hr_profile_generator",
            "prompt_source": "langfuse" if prompt_obj else "local_fallback",
        }

    def _build_success_response(
        self,
        profile_json: Dict[str, Any],
        generated_text: str,
        model: str,
        generation_time: float,
        usage: Any,
        prompt_obj: Optional[Any],
        prompt_name: str,
        variables: Dict[str, Any],
    ) -> Dict[str, Any]:
        """
        –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.

        Args:
            profile_json: –†–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–π JSON –ø—Ä–æ—Ñ–∏–ª—è
            generated_text: –°—ã—Ä–æ–π —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞
            model: –ò–º—è –º–æ–¥–µ–ª–∏
            generation_time: –í—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            usage: –û–±—ä–µ–∫—Ç —Å –¥–∞–Ω–Ω—ã–º–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤
            prompt_obj: –û–±—ä–µ–∫—Ç –ø—Ä–æ–º–ø—Ç–∞
            prompt_name: –ò–º—è –ø—Ä–æ–º–ø—Ç–∞
            variables: –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        """
        input_tokens = usage.prompt_tokens if usage else 0
        output_tokens = usage.completion_tokens if usage else 0
        total_tokens = usage.total_tokens if usage else input_tokens + output_tokens

        # –ü–æ–ª—É—á–∞–µ–º trace_id –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä—ã
        trace_id = None
        try:
            from langfuse.decorators import langfuse_context
            trace_id = langfuse_context.get_current_trace_id()
            logger.info("‚úÖ Langfuse tracing with decorators completed")
        except ImportError:
            logger.info("‚úÖ Basic Langfuse tracing completed")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to get trace ID: {e}")

        return {
            "profile": profile_json,
            "metadata": {
                "model": model,
                "generation_time": generation_time,
                "tokens": {
                    "input": input_tokens,
                    "output": output_tokens,
                    "total": total_tokens,
                },
                "temperature": self.prompt_manager.get_prompt_config("profile_generation").get("temperature", 0.1),
                "timestamp": datetime.now().isoformat(),
                "success": True,
                "langfuse_trace_id": trace_id,
                "prompt_name": prompt_name,
                "prompt_version": getattr(prompt_obj, "version", "local_fallback"),
                "prompt_source": "langfuse" if prompt_obj else "local_fallback",
                "tracing_mode": "decorator_based",
                "department": variables.get("department"),
                "position": variables.get("position"),
            },
            "raw_response": generated_text,
        }

    async def generate_profile_from_langfuse(
        self,
        prompt_name: str,
        variables: Dict[str, Any],
        user_id: Optional[str] = None,
        session_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """Asynchronously generates a profile from a Langfuse prompt with traced
        execution.
        
        This function retrieves the specified prompt and its configuration, compiles
        the prompt  into a message format, and builds metadata for tracing. It then
        performs an asynchronous  request to generate a profile using the Langfuse API,
        handling various potential errors  during the request. Finally, it extracts the
        generated text, parses it into JSON, and  returns a structured response
        containing the profile and metadata.
        
        Args:
            prompt_name: The name of the prompt in Langfuse.
            variables: Variables to substitute into the prompt.
            user_id: The user ID for tracing (optional).
            session_id: The session ID for tracing (optional).
        """
        start_time = time.time()

        try:
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–º–ø—Ç –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            prompt_obj, config = self._get_prompt_and_config(prompt_name)

            model = config.get("model", "google/gemini-2.5-flash")
            temperature = config.get("temperature", 0.1)
            max_tokens = config.get("max_tokens", 4000)
            response_format = config.get("response_format")

            logger.info(f"Starting generation with prompt: {prompt_name}")
            logger.info(f"Model: {model}, Temperature: {temperature}")

            # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –≤ —Ñ–æ—Ä–º–∞—Ç messages
            messages = self._compile_prompt_to_messages(prompt_obj, variables)
            logger.info(f"Final messages count: {len(messages)}")

            # –°—Ç—Ä–æ–∏–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç—Ä–µ–π—Å–∏–Ω–≥–∞
            trace_metadata = self._build_trace_metadata(
                prompt_name, prompt_obj, variables, user_id, session_id
            )

            # –í—ã–ø–æ–ª–Ω—è–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é —Å –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä–æ–º –¥–ª—è —Å–≤—è–∑–∫–∏ –ø—Ä–æ–º–ø—Ç–æ–≤
            try:
                response = await self._create_generation_with_prompt(
                    prompt=prompt_obj,  # –ú–æ–∂–µ—Ç –±—ã—Ç—å None –ø—Ä–∏ fallback
                    messages=messages,
                    model=model,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    response_format=response_format,
                    trace_metadata=trace_metadata,
                )
                logger.info("‚úÖ Generation with prompt linking completed")
            except httpx.HTTPStatusError as e:
                logger.error(
                    "OpenAI API HTTP error",
                    extra={
                        "status_code": e.response.status_code,
                        "prompt_name": prompt_name,
                        "model": model,
                        "response_text": e.response.text[:500] if e.response else None
                    }
                )
                raise
            except httpx.TimeoutException as e:
                logger.error(
                    "OpenAI API timeout",
                    extra={
                        "prompt_name": prompt_name,
                        "model": model,
                        "timeout": str(e)
                    }
                )
                raise
            except httpx.ConnectError as e:
                logger.error(
                    "OpenAI API connection error",
                    extra={
                        "prompt_name": prompt_name,
                        "model": model,
                        "error": str(e)
                    }
                )
                raise
            except Exception as api_error:
                logger.exception(
                    "Unexpected OpenAI API error",
                    extra={
                        "prompt_name": prompt_name,
                        "model": model,
                        "error_type": type(api_error).__name__
                    }
                )
                raise

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç–≤–µ—Ç –∏ –ø–∞—Ä—Å–∏–º JSON
            generated_text = response.choices[0].message.content
            profile_json = self._extract_and_parse_json(generated_text)

            generation_time = time.time() - start_time
            logger.info(f"Langfuse generation completed in {generation_time:.2f}s")

            # –°—Ç—Ä–æ–∏–º –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —É—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç
            return self._build_success_response(
                profile_json,
                generated_text,
                model,
                generation_time,
                response.usage,
                prompt_obj,
                prompt_name,
                variables,
            )

        except Exception as e:
            generation_time = time.time() - start_time
            logger.error(
                f"Langfuse generation failed after {generation_time:.2f}s: {e}"
            )

            # –ü–æ–ª—É—á–∞–µ–º trace_id –¥–∞–∂–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
            try:
                from langfuse.decorators import langfuse_context

                trace_id = langfuse_context.get_current_trace_id()
            except (ImportError, NameError):
                trace_id = None

            return {
                "profile": None,
                "metadata": {
                    "model": "unknown",
                    "generation_time": generation_time,
                    "error": str(e),
                    "timestamp": datetime.now().isoformat(),
                    "success": False,
                    "langfuse_trace_id": trace_id,
                },
                "raw_response": None,
            }

    def _extract_and_parse_json(self, generated_text: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ –ø–∞—Ä—Å–∏–Ω–≥ JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM"""
        try:
            # –ò—â–µ–º JSON –≤ —Ç–µ–∫—Å—Ç–µ (–º–µ–∂–¥—É ``` –∏–ª–∏ –≤ —á–∏—Å—Ç–æ–º –≤–∏–¥–µ)
            json_text = generated_text.strip()

            # –£–±–∏—Ä–∞–µ–º markdown code blocks –µ—Å–ª–∏ –µ—Å—Ç—å
            if json_text.startswith("```"):
                json_text = json_text.split("```")[1]
                if json_text.startswith("json"):
                    json_text = json_text[4:]
                json_text = json_text.strip()

            if json_text.endswith("```"):
                json_text = json_text[:-3].strip()

            # –ü–∞—Ä—Å–∏–º JSON
            profile_data = json.loads(json_text)

            if not isinstance(profile_data, dict):
                raise ValueError("Response is not a JSON object")

            logger.info("JSON successfully parsed from LLM response")
            return profile_data

        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse JSON from LLM response: {e}")
            logger.debug(f"Raw response text: {generated_text[:500]}...")

            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º fallback —Å—Ç—Ä—É–∫—Ç—É—Ä—É
            return {
                "error": "Failed to parse JSON from LLM response",
                "raw_response": generated_text,
                "parse_error": str(e),
            }

        except Exception as e:
            logger.error(f"Unexpected error while parsing response: {e}")
            return {
                "error": "Unexpected error during response parsing",
                "raw_response": generated_text,
                "parse_error": str(e),
            }

    def validate_profile_structure(self, profile: Dict[str, Any]) -> Dict[str, Any]:
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ—Ñ–∏–ª—è

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        """
        validation_result = {
            "is_valid": True,
            "errors": [],
            "warnings": [],
            "completeness_score": 0.0,
        }

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è (—Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å –Ω–æ–≤–æ–π —Å—Ö–µ–º–æ–π JSON)
        required_fields = [
            "position_title",
            "department_broad",
            "department_specific",
            "position_category",  # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–æ —Å "category"
            "direct_manager",  # –ù–æ–≤–æ–µ –ø–æ–ª–µ
            "subordinates",  # –ù–æ–≤–æ–µ –ø–æ–ª–µ
            "primary_activity_type",  # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–æ —Å "primary_activity"
            "responsibility_areas",
            "professional_skills",
            "corporate_competencies",  # –ù–æ–≤–æ–µ –ø–æ–ª–µ
            "personal_qualities",
            "experience_and_education",  # –ù–æ–≤–æ–µ –ø–æ–ª–µ
            "careerogram",  # –ù–æ–≤–æ–µ –ø–æ–ª–µ
            "workplace_provisioning",  # –ù–æ–≤–æ–µ –ø–æ–ª–µ
            "performance_metrics",  # –ù–æ–≤–æ–µ –ø–æ–ª–µ
            "additional_information",  # –ù–æ–≤–æ–µ –ø–æ–ª–µ
            "metadata",  # –ù–æ–≤–æ–µ –ø–æ–ª–µ
        ]

        missing_fields = []
        empty_fields = []
        total_fields = len(required_fields)
        filled_fields = 0

        # –í–∞–ª–∏–¥–∞—Ü–∏—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π fallback –¥–ª—è —Å—Ç–∞—Ä—ã—Ö –ø–æ–ª–µ–π
        for field in required_fields:
            # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ fallback –¥–ª—è –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–ª–µ–π
            fallback_map = {
                "position_category": "category",
                "primary_activity_type": "primary_activity",
                "experience_and_education": "education",
                "careerogram": "career_path",
                "workplace_provisioning": "technical_requirements",
            }

            field_value = profile.get(field)
            # –ï—Å–ª–∏ –ø–æ–ª–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –ø—Ä–æ–≤–µ—Ä—è–µ–º fallback
            if field_value is None and field in fallback_map:
                field_value = profile.get(fallback_map[field])

            if field_value is None:
                missing_fields.append(field)
            elif not field_value or (
                isinstance(field_value, (list, dict)) and len(field_value) == 0
            ):
                empty_fields.append(field)
            else:
                filled_fields += 1

        if missing_fields:
            validation_result["is_valid"] = False
            validation_result["errors"].extend(
                [f"Missing required field: {field}" for field in missing_fields]
            )

        if empty_fields:
            validation_result["warnings"].extend(
                [f"Empty required field: {field}" for field in empty_fields]
            )

        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–æ–ª–Ω–æ—Ç—É –ø—Ä–æ—Ñ–∏–ª—è
        validation_result["completeness_score"] = filled_fields / total_fields

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–ª—è –Ω–æ–≤—ã—Ö –ø–æ–ª–µ–π
        self._validate_responsibility_areas(profile, validation_result)
        self._validate_professional_skills(profile, validation_result)
        self._validate_subordinates(profile, validation_result)
        self._validate_careerogram(profile, validation_result)
        self._validate_performance_metrics(profile, validation_result)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–æ–ª—è (legacy)
        if "basic_info" in profile and isinstance(profile["basic_info"], dict):
            basic_info = profile["basic_info"]
            if not basic_info.get("position_title"):
                validation_result["errors"].append(
                    "Missing position_title in basic_info"
                )

        logger.info(
            f"Profile validation completed: {validation_result['completeness_score']:.2%} complete, {len(validation_result['errors'])} errors"
        )

        return validation_result

    def _validate_responsibility_areas(
        self, profile: Dict[str, Any], validation_result: Dict[str, Any]
    ):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –æ–±–ª–∞—Å—Ç–µ–π –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏"""
        if "responsibility_areas" in profile:
            areas = profile["responsibility_areas"]
            if isinstance(areas, list):
                for i, area in enumerate(areas):
                    if not isinstance(area, dict):
                        validation_result["warnings"].append(
                            f"–û–±–ª–∞—Å—Ç—å –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ {i+1} –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –æ–±—ä–µ–∫—Ç–æ–º"
                        )
                    else:
                        # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –∫–∞–∫ —Å—Ç–∞—Ä–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ (title), —Ç–∞–∫ –∏ –Ω–æ–≤–æ–≥–æ (area)
                        if "area" not in area and "title" not in area:
                            validation_result["warnings"].append(
                                f"–û–±–ª–∞—Å—Ç—å –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ {i+1} –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å 'area' –∏–ª–∏ 'title'"
                            )
                        if "tasks" not in area:
                            validation_result["warnings"].append(
                                f"–û–±–ª–∞—Å—Ç—å –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ {i+1} –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å 'tasks'"
                            )

    def _validate_professional_skills(
        self, profile: Dict[str, Any], validation_result: Dict[str, Any]
    ):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –Ω–∞–≤—ã–∫–æ–≤"""
        if "professional_skills" in profile:
            skills = profile["professional_skills"]
            if isinstance(skills, list):
                for i, skill_category in enumerate(skills):
                    if not isinstance(skill_category, dict):
                        validation_result["warnings"].append(
                            f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–∞–≤—ã–∫–æ–≤ {i+1} –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –æ–±—ä–µ–∫—Ç–æ–º"
                        )
                    else:
                        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è skill_category –∏–ª–∏ category (fallback)
                        if (
                            "skill_category" not in skill_category
                            and "category" not in skill_category
                        ):
                            validation_result["warnings"].append(
                                f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–∞–≤—ã–∫–æ–≤ {i+1} –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å 'skill_category'"
                            )

                        # –ü—Ä–æ–≤–µ—Ä–∫–∞ specific_skills –∏–ª–∏ skills (fallback)
                        skills_field = skill_category.get(
                            "specific_skills", skill_category.get("skills")
                        )
                        if not skills_field:
                            validation_result["warnings"].append(
                                f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–∞–≤—ã–∫–æ–≤ {i+1} –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å 'specific_skills'"
                            )
                        elif isinstance(skills_field, list) and len(skills_field) > 0:
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–µ—Ä–≤–æ–≥–æ –Ω–∞–≤—ã–∫–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ñ–æ—Ä–º–∞—Ç–∞
                            first_skill = skills_field[0]
                            if isinstance(first_skill, dict):
                                # –ù–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ –Ω–∞–≤—ã–∫–∞–º–∏
                                for j, skill in enumerate(skills_field):
                                    required_skill_fields = [
                                        "skill_name",
                                        "proficiency_level",
                                        "proficiency_description",
                                    ]
                                    for field in required_skill_fields:
                                        if field not in skill:
                                            validation_result["warnings"].append(
                                                f"–ù–∞–≤—ã–∫ {j+1} –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {i+1} –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å '{field}'"
                                            )

    def _validate_subordinates(
        self, profile: Dict[str, Any], validation_result: Dict[str, Any]
    ):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø–æ–¥—á–∏–Ω–µ–Ω–Ω—ã—Ö"""
        if "subordinates" in profile:
            subordinates = profile["subordinates"]
            if isinstance(subordinates, dict):
                if "departments" not in subordinates:
                    validation_result["warnings"].append(
                        "–ü–æ–ª–µ 'subordinates' –¥–æ–ª–∂–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç—å 'departments'"
                    )
                # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –∫–∞–∫ direct_reports, —Ç–∞–∫ –∏ people (fallback)
                if (
                    "direct_reports" not in subordinates
                    and "people" not in subordinates
                ):
                    validation_result["warnings"].append(
                        "–ü–æ–ª–µ 'subordinates' –¥–æ–ª–∂–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç—å 'direct_reports'"
                    )

    def _validate_careerogram(
        self, profile: Dict[str, Any], validation_result: Dict[str, Any]
    ):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∫–∞—Ä—å–µ—Ä–æ–≥—Ä–∞–º–º—ã"""
        # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –∏ –Ω–æ–≤–æ–π (careerogram) –∏ —Å—Ç–∞—Ä–æ–π (career_path) —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        careerogram = profile.get("careerogram") or profile.get("career_path")
        if careerogram and isinstance(careerogram, dict):
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã career_pathways
            if "career_pathways" in careerogram:
                career_pathways = careerogram["career_pathways"]
                if isinstance(career_pathways, list):
                    for i, pathway in enumerate(career_pathways):
                        if not isinstance(pathway, dict):
                            validation_result["warnings"].append(
                                f"–ö–∞—Ä—å–µ—Ä–Ω—ã–π –ø—É—Ç—å {i+1} –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ–±—ä–µ–∫—Ç–æ–º"
                            )
                        else:
                            required_pathway_fields = [
                                "pathway_type",
                                "entry_positions",
                                "advancement_positions",
                            ]
                            for field in required_pathway_fields:
                                if field not in pathway:
                                    validation_result["warnings"].append(
                                        f"–ö–∞—Ä—å–µ—Ä–Ω—ã–π –ø—É—Ç—å {i+1} –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å '{field}'"
                                    )
            else:
                # –î–ª—è —Å—Ç–∞—Ä–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–≤–µ—Ä—è–µ–º legacy –ø–æ–ª—è
                legacy_fields = ["donor_positions", "target_positions"]
                missing_legacy = [
                    field for field in legacy_fields if field not in careerogram
                ]
                if missing_legacy:
                    validation_result["warnings"].append(
                        f"–ö–∞—Ä—å–µ—Ä–æ–≥—Ä–∞–º–º–∞ –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å {missing_legacy}"
                    )

    def _validate_performance_metrics(
        self, profile: Dict[str, Any], validation_result: Dict[str, Any]
    ):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –º–µ—Ç—Ä–∏–∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏"""
        if "performance_metrics" in profile:
            metrics = profile["performance_metrics"]
            if isinstance(metrics, dict):
                required_fields = [
                    "quantitative_kpis",
                    "qualitative_indicators",
                    "evaluation_frequency",
                ]
                for field in required_fields:
                    if field not in metrics:
                        validation_result["warnings"].append(
                            f"performance_metrics –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å '{field}'"
                        )

