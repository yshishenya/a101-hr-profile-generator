"""
DataLoader —Å –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π –¥–ª—è —Å–∏—Å—Ç–µ–º—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ—Ñ–∏–ª–µ–π –ê101.

–û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö —Å –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π –º–∞–ø–ø–∏–Ω–≥–∞
–¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –≤ Langfuse –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–∞.
"""

import json
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, List
import logging

from .data_mapper import OrganizationMapper, KPIMapper

logger = logging.getLogger(__name__)


class DataLoader:
    """
    –ì–ª–∞–≤–Ω—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö —Å –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π –º–∞–ø–ø–∏–Ω–≥–∞.
    –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –≤—Å–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è Langfuse –ø—Ä–æ–º–ø—Ç–æ–≤.
    """
    
    def __init__(self, base_path: str = "."):
        self.base_path = Path(base_path)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–∞–ø–ø–∏–Ω–≥–æ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.org_mapper = OrganizationMapper("data/structure.json")
        self.kpi_mapper = KPIMapper("data/KPI")
        
        # –ö–µ—à –¥–ª—è —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        self._cache = {}
        
        # –ü—É—Ç–∏ –∫ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–º —Ñ–∞–π–ª–∞–º
        self.paths = {
            "company_map": self.base_path / "data" / "anonymized_digitization_map.md",
            "profile_examples": self.base_path / "templates" / "profile_examples.xlsx",
            "json_schema": self.base_path / "templates" / "job_profile_schema.json",
            "it_systems_dir": self.base_path / "data" / "it_systems"
        }
    
    def prepare_langfuse_variables(self, department: str, position: str, employee_name: Optional[str] = None) -> Dict[str, Any]:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö —Å –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π –º–∞–ø–ø–∏–Ω–≥–∞ –¥–ª—è Langfuse.
        
        Args:
            department: –ù–∞–∑–≤–∞–Ω–∏–µ –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–∞
            position: –ù–∞–∑–≤–∞–Ω–∏–µ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏
            employee_name: –§–ò–û —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–ª—è Langfuse –ø—Ä–æ–º–ø—Ç–∞
        """
        logger.info(f"Preparing variables for {department} - {position}")
        
        try:
            # üéØ –î–ï–¢–ï–†–ú–ò–ù–ò–†–û–í–ê–ù–ù–û–ï –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –°–¢–†–£–ö–¢–£–†–´
            org_structure = self.org_mapper.extract_relevant_structure(department)
            department_path = org_structure.get("department_path", department)
            
            # üéØ –î–ï–¢–ï–†–ú–ò–ù–ò–†–û–í–ê–ù–ù–´–ô –í–´–ë–û–† KPI –§–ê–ô–õ–ê  
            kpi_content = self.kpi_mapper.load_kpi_content(department)
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Å–µ—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
            variables = {
                # –û–°–ù–û–í–ù–û–ô –ö–û–ù–¢–ï–ö–°–¢ (–∫–µ—à–∏—Ä—É–µ—Ç—Å—è)
                "company_map": self._load_company_map_cached(),           # ~47K —Ç–æ–∫–µ–Ω–æ–≤
                "profile_examples": self._load_architect_examples_cached(), # ~30K —Ç–æ–∫–µ–Ω–æ–≤
                "json_schema": self._load_profile_schema_cached(),        # ~1K —Ç–æ–∫–µ–Ω–æ–≤
                
                # –†–ï–õ–ï–í–ê–ù–¢–ù–ê–Ø –°–¢–†–£–ö–¢–£–†–ê (–¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω–Ω–∞—è)
                "org_structure": json.dumps(org_structure, ensure_ascii=False, indent=2), # ~5K —Ç–æ–∫–µ–Ω–æ–≤
                "department_path": department_path,
                
                # –ü–û–ó–ò–¶–ò–û–ù–ù–´–ï –î–ê–ù–ù–´–ï
                "position": position,
                "department": department,
                "employee_name": employee_name or "",
                
                # –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ô –ö–û–ù–¢–ï–ö–°–¢ (–¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–π)
                "kpi_data": kpi_content,                                 # 0-15K —Ç–æ–∫–µ–Ω–æ–≤
                "it_systems": self._load_relevant_it_systems(department), # 5-20K —Ç–æ–∫–µ–Ω–æ–≤
                
                # –ú–ï–¢–ê–î–ê–ù–ù–´–ï
                "generation_timestamp": datetime.now().isoformat(),
                "data_version": "v1.0"
            }
            
            # –ü–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
            estimated_tokens = self._estimate_tokens(variables)
            variables["estimated_input_tokens"] = estimated_tokens
            
            logger.info(f"Variables prepared successfully. Estimated tokens: {estimated_tokens}")
            return variables
            
        except Exception as e:
            logger.error(f"Error preparing Langfuse variables: {e}")
            raise
    
    def _load_company_map_cached(self) -> str:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–∞—Ä—Ç—ã –∫–æ–º–ø–∞–Ω–∏–∏ –ê101 —Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        cache_key = "company_map"
        
        if cache_key not in self._cache:
            try:
                with open(self.paths["company_map"], 'r', encoding='utf-8') as f:
                    content = f.read()
                
                self._cache[cache_key] = content
                logger.info(f"Company map loaded: {len(content)} chars")
                
            except Exception as e:
                logger.error(f"Error loading company map: {e}")
                self._cache[cache_key] = "# –ö–∞—Ä—Ç–∞ –∫–æ–º–ø–∞–Ω–∏–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞\n\n–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö."
        
        return self._cache[cache_key]
    
    def _load_architect_examples_cached(self) -> str:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∏–º–µ—Ä–æ–≤ –ø—Ä–æ—Ñ–∏–ª–µ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä–æ–≤ —Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        cache_key = "architect_examples"
        
        if cache_key not in self._cache:
            # –ü–æ—Å–∫–æ–ª—å–∫—É —ç—Ç–æ Excel —Ñ–∞–π–ª, –º—ã –Ω–µ –º–æ–∂–µ–º –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ—á–∏—Ç–∞—Ç—å –µ–≥–æ –∫–∞–∫ —Ç–µ–∫—Å—Ç
            # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–¥–µ—Å—å –±—É–¥–µ—Ç pandas –¥–ª—è —á—Ç–µ–Ω–∏—è Excel
            # –ü–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º placeholder
            self._cache[cache_key] = """# –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–æ—Ñ–∏–ª–µ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä–æ–≤

–î–∞–Ω–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –ø—Ä–æ—Ñ–∏–ª–µ–π —Å–ª—É–∂–∞—Ç —ç—Ç–∞–ª–æ–Ω–æ–º –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–æ–≤—ã—Ö –ø—Ä–æ—Ñ–∏–ª–µ–π.

[PLACEHOLDER: –ó–¥–µ—Å—å –±—É–¥—É—Ç –∑–∞–≥—Ä—É–∂–∞—Ç—å—Å—è –¥–µ—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä–æ–≤ –∏–∑ Excel —Ñ–∞–π–ª–∞]

–ö–ª—é—á–µ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –ø—Ä–æ—Ñ–∏–ª–µ–π:
- –î–µ—Ç–∞–ª—å–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ (3-7 –∑–∞–¥–∞—á –∫–∞–∂–¥–∞—è)
- –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ –Ω–∞–≤—ã–∫–∏ —Å —á–µ—Ç–∫–∏–º–∏ —É—Ä–æ–≤–Ω—è–º–∏
- –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–µ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –ê101
- –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –∫–∞—Ä—å–µ—Ä–Ω—ã–µ –ø—É—Ç–∏
- –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
"""
            logger.warning("Architect examples placeholder loaded (Excel parsing not implemented)")
        
        return self._cache[cache_key]
    
    def _load_profile_schema_cached(self) -> str:
        """–ó–∞–≥—Ä—É–∑–∫–∞ JSON —Å—Ö–µ–º—ã –ø—Ä–æ—Ñ–∏–ª—è —Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        cache_key = "profile_schema"
        
        if cache_key not in self._cache:
            try:
                with open(self.paths["json_schema"], 'r', encoding='utf-8') as f:
                    schema_data = json.load(f)
                
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —á–∏—Ç–∞–±–µ–ª—å–Ω—É—é JSON —Å—Ç—Ä–æ–∫—É
                self._cache[cache_key] = json.dumps(schema_data, ensure_ascii=False, indent=2)
                logger.info("Profile JSON schema loaded")
                
            except Exception as e:
                logger.error(f"Error loading profile schema: {e}")
                self._cache[cache_key] = '{"error": "Schema not available"}'
        
        return self._cache[cache_key]
    
    def _load_relevant_it_systems(self, department: str) -> str:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö IT —Å–∏—Å—Ç–µ–º –¥–ª—è –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–∞"""
        it_systems_dir = self.paths["it_systems_dir"]
        
        if not it_systems_dir.exists():
            return "# IT —Å–∏—Å—Ç–µ–º—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã\n\n–î–∞–Ω–Ω—ã–µ –æ–± IT —Å–∏—Å—Ç–µ–º–∞—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω—ã."
        
        # –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ IT —Å–∏—Å—Ç–µ–º
        relevant_files = []
        
        # –ò—â–µ–º —Ñ–∞–π–ª—ã, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–∞
        for file_path in it_systems_dir.glob("*.md"):
            filename = file_path.name.lower()
            dept_lower = department.lower()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞ –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç—É
            if any(keyword in filename for keyword in [
                dept_lower, 
                dept_lower.replace(' ', '_'),
                dept_lower.replace('–¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç', 'dept'),
                '–æ–±—â–∏–π' if '—Ñ–∏–Ω–∞–Ω—Å' in dept_lower or '–∫–æ–º–º–µ—Ä—á' in dept_lower else ''
            ]):
                relevant_files.append(file_path)
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤, –∏—â–µ–º –æ–±—â–∏–µ
        if not relevant_files:
            for file_path in it_systems_dir.glob("*.md"):
                filename = file_path.name.lower()
                if any(keyword in filename for keyword in ['general', '–æ–±—â–∏–π', 'corporate', '–∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω']):
                    relevant_files.append(file_path)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
        combined_content = []
        
        for file_path in relevant_files[:3]:  # –ú–∞–∫—Å–∏–º—É–º 3 —Ñ–∞–π–ª–∞ –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è —Ç–æ–∫–µ–Ω–æ–≤
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                combined_content.append(f"## {file_path.stem}\n\n{content}")
                
            except Exception as e:
                logger.error(f"Error loading IT systems file {file_path}: {e}")
        
        if not combined_content:
            return f"# IT —Å–∏—Å—Ç–µ–º—ã –¥–ª—è {department}\n\n–°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ IT —Å–∏—Å—Ç–µ–º–∞–º –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã."
        
        result = "\n\n---\n\n".join(combined_content)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É (–º–∞–∫—Å–∏–º—É–º 20K —Ç–æ–∫–µ–Ω–æ–≤ ‚âà 60K —Å–∏–º–≤–æ–ª–æ–≤)
        if len(result) > 60000:
            result = result[:60000] + "\n\n[...–∫–æ–Ω—Ç–µ–Ω—Ç –æ–±—Ä–µ–∑–∞–Ω –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤...]"
            logger.warning("IT systems content truncated due to length")
        
        logger.info(f"IT systems loaded for '{department}': {len(relevant_files)} files, {len(result)} chars")
        return result
    
    def _estimate_tokens(self, variables: Dict[str, Any]) -> int:
        """–ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤"""
        total_chars = 0
        
        for key, value in variables.items():
            if isinstance(value, str):
                total_chars += len(value)
            elif isinstance(value, (dict, list)):
                total_chars += len(json.dumps(value, ensure_ascii=False))
        
        # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞: 1 —Ç–æ–∫–µ–Ω ‚âà 3.5 —Å–∏–º–≤–æ–ª–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        estimated_tokens = int(total_chars / 3.5)
        
        return estimated_tokens
    
    def load_full_organization_structure(self) -> Dict[str, Any]:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–ª–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –∑–∞ –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å —Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º.
        
        Returns:
            Dict —Å –ø–æ–ª–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–æ–≤ –∏ –ø–æ–∑–∏—Ü–∏–π
        """
        cache_key = "full_org_structure"
        
        if cache_key not in self._cache:
            start_time = datetime.now()
            logger.info("Loading full organization structure...")
            
            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã
            if not self.org_mapper._department_index:
                self.org_mapper._load_org_structure()
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥
            full_structure = {
                "departments": {},
                "metadata": {
                    "total_departments": 0,
                    "total_positions": 0,
                    "loaded_at": start_time.isoformat()
                }
            }
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç—ã
            all_departments = list(self.org_mapper._department_index.keys()) if self.org_mapper._department_index else []
            
            for dept_name in all_departments:
                # –ü–æ–ª—É—á–∞–µ–º –ø–æ–∑–∏—Ü–∏–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–∞
                positions = self._get_positions_for_department_internal(dept_name)
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–∞
                dept_path = self.org_mapper.find_department_path(dept_name)
                
                # –°–æ–±–∏—Ä–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–µ
                dept_info = {
                    "name": dept_name,
                    "path": dept_path,
                    "positions": [
                        {
                            "name": pos_name,
                            "level": self._determine_position_level(pos_name),
                            "category": self._determine_position_category(pos_name)
                        }
                        for pos_name in positions
                    ],
                    "positions_count": len(positions)
                }
                
                full_structure["departments"][dept_name] = dept_info
                full_structure["metadata"]["total_positions"] += len(positions)
            
            full_structure["metadata"]["total_departments"] = len(all_departments)
            
            # –ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            self._cache[cache_key] = full_structure
            
            load_time = (datetime.now() - start_time).total_seconds()
            logger.info(f"‚úÖ Full organization structure loaded in {load_time:.3f}s: "
                       f"{full_structure['metadata']['total_departments']} departments, "
                       f"{full_structure['metadata']['total_positions']} positions")
        
        return self._cache[cache_key]
    
    def get_available_departments(self) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–æ–≤"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥
        full_structure = self.load_full_organization_structure()
        return list(full_structure["departments"].keys())
    
    def _get_positions_for_department_internal(self, department: str) -> List[str]:
        """–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–µ—Ç–æ–¥ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–∑–∏—Ü–∏–π –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–∞ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        try:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–∏–ø–∏—á–Ω—ã–µ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è
            base_positions = [
                "–†—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–∞",
                "–ó–∞–º–µ—Å—Ç–∏—Ç–µ–ª—å —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è", 
                "–í–µ–¥—É—â–∏–π —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç",
                "–°—Ç–∞—Ä—à–∏–π —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç",
                "–°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç",
                "–ê–Ω–∞–ª–∏—Ç–∏–∫",
                "–ú–µ–Ω–µ–¥–∂–µ—Ä"
            ]
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–∞
            dept_lower = department.lower()
            specialized_positions = []
            
            if any(keyword in dept_lower for keyword in ['–∏—Ç', '–∏–Ω—Ñ–æ—Ä–º–∞—Ü', '—Ü–∏—Ñ—Ä', '—Ä–∞–∑—Ä–∞–±–æ—Ç']):
                specialized_positions.extend([
                    "–°–∏—Å—Ç–µ–º–Ω—ã–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä", 
                    "–ê—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä —Ä–µ—à–µ–Ω–∏–π",
                    "–†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫",
                    "DevOps –∏–Ω–∂–µ–Ω–µ—Ä",
                    "–°–∏—Å—Ç–µ–º–Ω—ã–π –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä"
                ])
            elif any(keyword in dept_lower for keyword in ['–∫–æ–º–º–µ—Ä—á', '–ø—Ä–æ–¥–∞–∂', '—Ä–µ–∞–ª–∏–∑–∞—Ü']):
                specialized_positions.extend([
                    "–ú–µ–Ω–µ–¥–∂–µ—Ä –ø–æ –ø—Ä–æ–¥–∞–∂–∞–º",
                    "–ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π –¥–∏—Ä–µ–∫—Ç–æ—Ä",
                    "–ú–µ–Ω–µ–¥–∂–µ—Ä –ø–æ —Ä–∞–±–æ—Ç–µ —Å –∫–ª–∏–µ–Ω—Ç–∞–º–∏",
                    "–°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –ø–æ –ø—Ä–æ–¥–∞–∂–∞–º"
                ])
            elif any(keyword in dept_lower for keyword in ['—Ñ–∏–Ω–∞–Ω—Å', '–±—É—Ö–≥–∞–ª—Ç', '—ç–∫–æ–Ω–æ–º–∏—á']):
                specialized_positions.extend([
                    "–§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫",
                    "–ö–æ–Ω—Ç—Ä–æ–ª–µ—Ä", 
                    "–≠–∫–æ–Ω–æ–º–∏—Å—Ç",
                    "–ë—É—Ö–≥–∞–ª—Ç–µ—Ä"
                ])
            elif any(keyword in dept_lower for keyword in ['–±–µ–∑–æ–ø–∞—Å–Ω', '–æ—Ö—Ä–∞–Ω']):
                specialized_positions.extend([
                    "–°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –ø–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏",
                    "–ò–Ω–∂–µ–Ω–µ—Ä –ø–æ –æ—Ö—Ä–∞–Ω–µ —Ç—Ä—É–¥–∞"
                ])
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –±–∞–∑–æ–≤—ã–µ –∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏
            all_positions = base_positions + specialized_positions
            return sorted(list(set(all_positions)))
            
        except Exception as e:
            logger.error(f"Error getting positions for {department}: {e}")
            return ["–°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç", "–ú–µ–Ω–µ–¥–∂–µ—Ä", "–ê–Ω–∞–ª–∏—Ç–∏–∫"]  # Fallback
    
    def _determine_position_level(self, position_name: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Ä–æ–≤–Ω—è –¥–æ–ª–∂–Ω–æ—Å—Ç–∏ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é"""
        position_lower = position_name.lower()
        
        if any(keyword in position_lower for keyword in ['—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å', '–¥–∏—Ä–µ–∫—Ç–æ—Ä', '—É–ø—Ä–∞–≤–ª—è—é—â–∏–π', '–Ω–∞—á–∞–ª—å–Ω–∏–∫']):
            return "senior"
        elif any(keyword in position_lower for keyword in ['–≤–µ–¥—É—â–∏–π', '–≥–ª–∞–≤–Ω—ã–π', '—Å—Ç–∞—Ä—à–∏–π']):
            return "lead"  
        elif any(keyword in position_lower for keyword in ['—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç', '–∞–Ω–∞–ª–∏—Ç–∏–∫', '–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç']):
            return "middle"
        elif any(keyword in position_lower for keyword in ['–º–ª–∞–¥—à–∏–π', '–ø–æ–º–æ—â–Ω–∏–∫', '—Å—Ç–∞–∂–µ—Ä']):
            return "junior"
        else:
            return "middle"  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    def _determine_position_category(self, position_name: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏"""
        position_lower = position_name.lower()
        
        if any(keyword in position_lower for keyword in ['—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å', '–¥–∏—Ä–µ–∫—Ç–æ—Ä', '—É–ø—Ä–∞–≤–ª—è—é—â–∏–π', '–Ω–∞—á–∞–ª—å–Ω–∏–∫']):
            return "management"
        elif any(keyword in position_lower for keyword in ['–∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä', '—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫', '–ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç', '—Ç–µ—Ö–Ω–∏–∫']):
            return "technical"
        elif any(keyword in position_lower for keyword in ['–∞–Ω–∞–ª–∏—Ç–∏–∫', '–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å']):
            return "analytical"
        elif any(keyword in position_lower for keyword in ['–ø—Ä–æ–¥–∞–∂', '–º–µ–Ω–µ–¥–∂–µ—Ä', '–∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π']):
            return "sales"
        elif any(keyword in position_lower for keyword in ['–±—É—Ö–≥–∞–ª—Ç–µ—Ä', '—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π', '—ç–∫–æ–Ω–æ–º–∏—Å—Ç']):
            return "financial"
        elif any(keyword in position_lower for keyword in ['hr', '–∫–∞–¥—Ä', '–ø–µ—Ä—Å–æ–Ω–∞–ª']):
            return "hr"
        else:
            return "operational"  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é

    def get_positions_for_department(self, department: str) -> List[str]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–∞.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—É—é –ø–æ–ª–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞.
        """
        try:
            # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø–æ–ª–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            full_structure = self.load_full_organization_structure()
            
            if department in full_structure["departments"]:
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∏–º–µ–Ω–∞ –ø–æ–∑–∏—Ü–∏–π –∏–∑ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
                positions = [pos["name"] for pos in full_structure["departments"][department]["positions"]]
                return positions
            else:
                logger.warning(f"Department '{department}' not found in organization structure")
                return []
                
        except Exception as e:
            logger.error(f"Error getting positions for department '{department}': {e}")
            # Fallback to internal method
            return self._get_positions_for_department_internal(department)
    
    def clear_cache(self):
        """–û—á–∏—Å—Ç–∫–∞ –∫–µ—à–∞ (–ø–æ–ª–µ–∑–Ω–æ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)"""
        self._cache.clear()
        logger.info("Cache cleared")
    
    def validate_data_sources(self) -> Dict[str, bool]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö"""
        validation = {
            "company_map": self.paths["company_map"].exists(),
            "profile_examples": self.paths["profile_examples"].exists(), 
            "json_schema": self.paths["json_schema"].exists(),
            "it_systems_dir": self.paths["it_systems_dir"].exists(),
            "org_structure": (self.base_path / "data" / "structure.json").exists(),
            "kpi_file": (self.base_path / "data" / "KPI" / "KPI_DIT.md").exists()
        }
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º KPI —Ñ–∞–π–ª—ã
        kpi_validation = self.kpi_mapper.validate_kpi_mappings()
        validation["kpi_files"] = all(kpi_validation.values())
        
        return validation


if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ DataLoader
    logging.basicConfig(level=logging.INFO)
    
    print("=== –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ DataLoader ===")
    loader = DataLoader()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö
    validation = loader.validate_data_sources()
    print("–í–∞–ª–∏–¥–∞—Ü–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö:")
    for source, status in validation.items():
        print(f"  {source}: {'‚úÖ' if status else '‚ùå'}")
    
    print("\n=== –¢–µ—Å—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö ===")
    try:
        variables = loader.prepare_langfuse_variables(
            department="–î–ò–¢",
            position="–ê—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä —Ä–µ—à–µ–Ω–∏–π",
            employee_name="–ò–≤–∞–Ω–æ–≤ –ò–≤–∞–Ω –ò–≤–∞–Ω–æ–≤–∏—á"
        )
        
        print(f"–ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã:")
        print(f"  - –î–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç: {variables['department']}")
        print(f"  - –ü—É—Ç—å –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ: {variables['department_path']}")
        print(f"  - –î–æ–ª–∂–Ω–æ—Å—Ç—å: {variables['position']}")
        print(f"  - –û—Ü–µ–Ω–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤: {variables['estimated_input_tokens']}")
        print(f"  - –†–∞–∑–º–µ—Ä –∫–∞—Ä—Ç—ã –∫–æ–º–ø–∞–Ω–∏–∏: {len(variables['company_map'])} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"  - –†–∞–∑–º–µ—Ä KPI –¥–∞–Ω–Ω—ã—Ö: {len(variables['kpi_data'])} —Å–∏–º–≤–æ–ª–æ–≤")
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")