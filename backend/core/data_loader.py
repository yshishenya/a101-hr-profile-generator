"""
DataLoader —Å –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π –¥–ª—è —Å–∏—Å—Ç–µ–º—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ—Ñ–∏–ª–µ–π –ê101.

–û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö —Å –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π –º–∞–ø–ø–∏–Ω–≥–∞
–¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –≤ Langfuse –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–∞.
"""

import json
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, List
import logging

from .data_mapper import OrganizationMapper, KPIMapper

logger = logging.getLogger(__name__)


class DataLoader:
    """
    –ì–ª–∞–≤–Ω—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö —Å –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π –º–∞–ø–ø–∏–Ω–≥–∞.
    –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –≤—Å–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è Langfuse –ø—Ä–æ–º–ø—Ç–æ–≤.
    """
    
    def __init__(self, base_path: str = "/home/yan/A101/HR"):
        self.base_path = Path(base_path)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–∞–ø–ø–∏–Ω–≥–æ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.org_mapper = OrganizationMapper(
            str(self.base_path / "org_structure" / "structure.json")
        )
        self.kpi_mapper = KPIMapper(
            str(self.base_path / "KPI" / "md_converted")
        )
        
        # –ö–µ—à –¥–ª—è —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        self._cache = {}
        
        # –ü—É—Ç–∏ –∫ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–º —Ñ–∞–π–ª–∞–º
        self.paths = {
            "company_map": self.base_path / "–ö–∞—Ä—Ç–∞ –ö–æ–º–ø–∞–Ω–∏–∏ –ê101.md",
            "profile_examples": self.base_path / "Profiles" / "–ü—Ä–æ—Ñ–∏–ª–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä—ã.xlsx",
            "json_schema": self.base_path / "Profiles" / "job_profile_schema.json",
            "it_systems_dir": self.base_path / "IT systems"
        }
    
    def prepare_langfuse_variables(self, department: str, position: str, employee_name: Optional[str] = None) -> Dict[str, Any]:
        """Prepare variables for Langfuse based on department and position."""
        logger.info(f"Preparing variables for {department} - {position}")
        
        try:
            # üéØ –î–ï–¢–ï–†–ú–ò–ù–ò–†–û–í–ê–ù–ù–û–ï –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –°–¢–†–£–ö–¢–£–†–´
            org_structure = self.org_mapper.extract_relevant_structure(department)
            department_path = org_structure.get("department_path", department)
            
            # üéØ –î–ï–¢–ï–†–ú–ò–ù–ò–†–û–í–ê–ù–ù–´–ô –í–´–ë–û–† KPI –§–ê–ô–õ–ê  
            kpi_content = self.kpi_mapper.load_kpi_content(department)
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Å–µ—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
            variables = {
                # –û–°–ù–û–í–ù–û–ô –ö–û–ù–¢–ï–ö–°–¢ (–∫–µ—à–∏—Ä—É–µ—Ç—Å—è)
                "company_map": self._load_company_map_cached(),           # ~47K —Ç–æ–∫–µ–Ω–æ–≤
                "profile_examples": self._load_architect_examples_cached(), # ~30K —Ç–æ–∫–µ–Ω–æ–≤
                "json_schema": self._load_profile_schema_cached(),        # ~1K —Ç–æ–∫–µ–Ω–æ–≤
                
                # –†–ï–õ–ï–í–ê–ù–¢–ù–ê–Ø –°–¢–†–£–ö–¢–£–†–ê (–¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω–Ω–∞—è)
                "org_structure": json.dumps(org_structure, ensure_ascii=False, indent=2), # ~5K —Ç–æ–∫–µ–Ω–æ–≤
                "department_path": department_path,
                
                # –ü–û–ó–ò–¶–ò–û–ù–ù–´–ï –î–ê–ù–ù–´–ï
                "position": position,
                "department": department,
                "employee_name": employee_name or "",
                
                # –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ô –ö–û–ù–¢–ï–ö–°–¢ (–¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–π)
                "kpi_data": kpi_content,                                 # 0-15K —Ç–æ–∫–µ–Ω–æ–≤
                "it_systems": self._load_relevant_it_systems(department), # 5-20K —Ç–æ–∫–µ–Ω–æ–≤
                
                # –ú–ï–¢–ê–î–ê–ù–ù–´–ï
                "generation_timestamp": datetime.now().isoformat(),
                "data_version": "v1.0"
            }
            
            # –ü–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
            estimated_tokens = self._estimate_tokens(variables)
            variables["estimated_input_tokens"] = estimated_tokens
            
            logger.info(f"Variables prepared successfully. Estimated tokens: {estimated_tokens}")
            return variables
            
        except Exception as e:
            logger.error(f"Error preparing Langfuse variables: {e}")
            raise
    
    def _load_company_map_cached(self) -> str:
        """Load the company map with caching."""
        cache_key = "company_map"
        
        if cache_key not in self._cache:
            try:
                with open(self.paths["company_map"], 'r', encoding='utf-8') as f:
                    content = f.read()
                
                self._cache[cache_key] = content
                logger.info(f"Company map loaded: {len(content)} chars")
                
            except Exception as e:
                logger.error(f"Error loading company map: {e}")
                self._cache[cache_key] = "# –ö–∞—Ä—Ç–∞ –∫–æ–º–ø–∞–Ω–∏–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞\n\n–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö."
        
        return self._cache[cache_key]
    
    def _load_architect_examples_cached(self) -> str:
        """Load architect profile examples with caching."""
        cache_key = "architect_examples"
        
        if cache_key not in self._cache:
            # –ü–æ—Å–∫–æ–ª—å–∫—É —ç—Ç–æ Excel —Ñ–∞–π–ª, –º—ã –Ω–µ –º–æ–∂–µ–º –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ—á–∏—Ç–∞—Ç—å –µ–≥–æ –∫–∞–∫ —Ç–µ–∫—Å—Ç
            # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–¥–µ—Å—å –±—É–¥–µ—Ç pandas –¥–ª—è —á—Ç–µ–Ω–∏—è Excel
            # –ü–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º placeholder
            self._cache[cache_key] = """# –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–æ—Ñ–∏–ª–µ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä–æ–≤

–î–∞–Ω–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –ø—Ä–æ—Ñ–∏–ª–µ–π —Å–ª—É–∂–∞—Ç —ç—Ç–∞–ª–æ–Ω–æ–º –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–æ–≤—ã—Ö –ø—Ä–æ—Ñ–∏–ª–µ–π.

[PLACEHOLDER: –ó–¥–µ—Å—å –±—É–¥—É—Ç –∑–∞–≥—Ä—É–∂–∞—Ç—å—Å—è –¥–µ—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä–æ–≤ –∏–∑ Excel —Ñ–∞–π–ª–∞]

–ö–ª—é—á–µ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –ø—Ä–æ—Ñ–∏–ª–µ–π:
- –î–µ—Ç–∞–ª—å–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ (3-7 –∑–∞–¥–∞—á –∫–∞–∂–¥–∞—è)
- –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ –Ω–∞–≤—ã–∫–∏ —Å —á–µ—Ç–∫–∏–º–∏ —É—Ä–æ–≤–Ω—è–º–∏
- –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–µ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –ê101
- –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –∫–∞—Ä—å–µ—Ä–Ω—ã–µ –ø—É—Ç–∏
- –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
"""
            logger.warning("Architect examples placeholder loaded (Excel parsing not implemented)")
        
        return self._cache[cache_key]
    
    def _load_profile_schema_cached(self) -> str:
        """Load the profile JSON schema with caching."""
        cache_key = "profile_schema"
        
        if cache_key not in self._cache:
            try:
                with open(self.paths["json_schema"], 'r', encoding='utf-8') as f:
                    schema_data = json.load(f)
                
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —á–∏—Ç–∞–±–µ–ª—å–Ω—É—é JSON —Å—Ç—Ä–æ–∫—É
                self._cache[cache_key] = json.dumps(schema_data, ensure_ascii=False, indent=2)
                logger.info("Profile JSON schema loaded")
                
            except Exception as e:
                logger.error(f"Error loading profile schema: {e}")
                self._cache[cache_key] = '{"error": "Schema not available"}'
        
        return self._cache[cache_key]
    
    def _load_relevant_it_systems(self, department: str) -> str:
        """Load relevant IT systems for a given department.
        
        This function searches for Markdown files in the specified IT systems directory
        that match the department name or related keywords. It first checks for
        specific files related to the department and, if none are found, looks for
        general files. The contents of up to three relevant files are then loaded and
        combined, with a character limit enforced to optimize token usage. If no
        relevant content is found, an appropriate message is returned.
        
        Args:
            department (str): The name of the department for which to load IT systems.
        
        Returns:
            str: A formatted string containing the relevant IT systems information or a message
                indicating unavailability.
        """
        it_systems_dir = self.paths["it_systems_dir"]
        
        if not it_systems_dir.exists():
            return "# IT —Å–∏—Å—Ç–µ–º—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã\n\n–î–∞–Ω–Ω—ã–µ –æ–± IT —Å–∏—Å—Ç–µ–º–∞—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω—ã."
        
        # –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ IT —Å–∏—Å—Ç–µ–º
        relevant_files = []
        
        # –ò—â–µ–º —Ñ–∞–π–ª—ã, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–∞
        for file_path in it_systems_dir.glob("*.md"):
            filename = file_path.name.lower()
            dept_lower = department.lower()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞ –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç—É
            if any(keyword in filename for keyword in [
                dept_lower, 
                dept_lower.replace(' ', '_'),
                dept_lower.replace('–¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç', 'dept'),
                '–æ–±—â–∏–π' if '—Ñ–∏–Ω–∞–Ω—Å' in dept_lower or '–∫–æ–º–º–µ—Ä—á' in dept_lower else ''
            ]):
                relevant_files.append(file_path)
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤, –∏—â–µ–º –æ–±—â–∏–µ
        if not relevant_files:
            for file_path in it_systems_dir.glob("*.md"):
                filename = file_path.name.lower()
                if any(keyword in filename for keyword in ['general', '–æ–±—â–∏–π', 'corporate', '–∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω']):
                    relevant_files.append(file_path)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
        combined_content = []
        
        for file_path in relevant_files[:3]:  # –ú–∞–∫—Å–∏–º—É–º 3 —Ñ–∞–π–ª–∞ –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è —Ç–æ–∫–µ–Ω–æ–≤
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                combined_content.append(f"## {file_path.stem}\n\n{content}")
                
            except Exception as e:
                logger.error(f"Error loading IT systems file {file_path}: {e}")
        
        if not combined_content:
            return f"# IT —Å–∏—Å—Ç–µ–º—ã –¥–ª—è {department}\n\n–°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ IT —Å–∏—Å—Ç–µ–º–∞–º –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã."
        
        result = "\n\n---\n\n".join(combined_content)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É (–º–∞–∫—Å–∏–º—É–º 20K —Ç–æ–∫–µ–Ω–æ–≤ ‚âà 60K —Å–∏–º–≤–æ–ª–æ–≤)
        if len(result) > 60000:
            result = result[:60000] + "\n\n[...–∫–æ–Ω—Ç–µ–Ω—Ç –æ–±—Ä–µ–∑–∞–Ω –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤...]"
            logger.warning("IT systems content truncated due to length")
        
        logger.info(f"IT systems loaded for '{department}': {len(relevant_files)} files, {len(result)} chars")
        return result
    
    def _estimate_tokens(self, variables: Dict[str, Any]) -> int:
        """Estimate the number of tokens based on the provided variables."""
        total_chars = 0
        
        for key, value in variables.items():
            if isinstance(value, str):
                total_chars += len(value)
            elif isinstance(value, (dict, list)):
                total_chars += len(json.dumps(value, ensure_ascii=False))
        
        # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞: 1 —Ç–æ–∫–µ–Ω ‚âà 3.5 —Å–∏–º–≤–æ–ª–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        estimated_tokens = int(total_chars / 3.5)
        
        return estimated_tokens
    
    def get_available_departments(self) -> List[str]:
        """Retrieve a list of all available departments."""
        return list(self.org_mapper._department_index.keys()) if self.org_mapper._department_index else []
    
    def get_positions_for_department(self, department: str) -> List[str]:
        # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —ç—Ç–æ –±—É–¥–µ—Ç –∏–∑–≤–ª–µ–∫–∞—Ç—å—Å—è –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        """def get_positions_for_department(self, department: str) -> List[str]:
        Retrieve a list of positions for a specific department.  This function returns
        a list of typical positions associated with a  given department. It starts with
        a predefined set of common positions  and then appends specialized roles based
        on the department's focus,  such as IT, sales, or finance. The final list is
        returned in a sorted  and unique format.
        
        Args:
            department (str): The name of the department for which to retrieve"""
        common_positions = [
            "–†—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–∞",
            "–ó–∞–º–µ—Å—Ç–∏—Ç–µ–ª—å —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è",
            "–í–µ–¥—É—â–∏–π —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç", 
            "–°—Ç–∞—Ä—à–∏–π —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç",
            "–°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç",
            "–ú–ª–∞–¥—à–∏–π —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç",
            "–ê–Ω–∞–ª–∏—Ç–∏–∫",
            "–ú–µ–Ω–µ–¥–∂–µ—Ä",
            "–ö–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä"
        ]
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–∞
        if "–∏—Ç" in department.lower() or "–∏–Ω—Ñ–æ—Ä–º–∞—Ü" in department.lower():
            common_positions.extend([
                "–ê—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä —Ä–µ—à–µ–Ω–∏–π",
                "–°–∏—Å—Ç–µ–º–Ω—ã–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä",
                "–†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫",
                "DevOps –∏–Ω–∂–µ–Ω–µ—Ä",
                "–°–∏—Å—Ç–µ–º–Ω—ã–π –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä",
                "–ê–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö"
            ])
        elif "–∫–æ–º–º–µ—Ä—á" in department.lower() or "–ø—Ä–æ–¥–∞–∂" in department.lower():
            common_positions.extend([
                "–ú–µ–Ω–µ–¥–∂–µ—Ä –ø–æ –ø—Ä–æ–¥–∞–∂–∞–º",
                "–ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π –¥–∏—Ä–µ–∫—Ç–æ—Ä",
                "–ú–µ–Ω–µ–¥–∂–µ—Ä –ø–æ —Ä–∞–±–æ—Ç–µ —Å –∫–ª–∏–µ–Ω—Ç–∞–º–∏"
            ])
        elif "—Ñ–∏–Ω–∞–Ω—Å" in department.lower():
            common_positions.extend([
                "–§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫",
                "–ö–æ–Ω—Ç—Ä–æ–ª–µ—Ä",
                "–ö–∞–∑–Ω–∞—á–µ–π"
            ])
        
        return sorted(list(set(common_positions)))
    
    def clear_cache(self):
        """Clear the cache."""
        self._cache.clear()
        logger.info("Cache cleared")
    
    def validate_data_sources(self) -> Dict[str, bool]:
        """Check the availability of all data sources."""
        validation = {
            "company_map": self.paths["company_map"].exists(),
            "profile_examples": self.paths["profile_examples"].exists(), 
            "json_schema": self.paths["json_schema"].exists(),
            "it_systems_dir": self.paths["it_systems_dir"].exists(),
            "org_structure": (self.base_path / "org_structure" / "structure.json").exists(),
            "kpi_dir": (self.base_path / "KPI" / "md_converted").exists()
        }
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º KPI —Ñ–∞–π–ª—ã
        kpi_validation = self.kpi_mapper.validate_kpi_mappings()
        validation["kpi_files"] = all(kpi_validation.values())
        
        return validation


if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ DataLoader
    logging.basicConfig(level=logging.INFO)
    
    print("=== –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ DataLoader ===")
    loader = DataLoader()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö
    validation = loader.validate_data_sources()
    print("–í–∞–ª–∏–¥–∞—Ü–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö:")
    for source, status in validation.items():
        print(f"  {source}: {'‚úÖ' if status else '‚ùå'}")
    
    print("\n=== –¢–µ—Å—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö ===")
    try:
        variables = loader.prepare_langfuse_variables(
            department="–î–ò–¢",
            position="–ê—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä —Ä–µ—à–µ–Ω–∏–π",
            employee_name="–ò–≤–∞–Ω–æ–≤ –ò–≤–∞–Ω –ò–≤–∞–Ω–æ–≤–∏—á"
        )
        
        print(f"–ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã:")
        print(f"  - –î–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç: {variables['department']}")
        print(f"  - –ü—É—Ç—å –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ: {variables['department_path']}")
        print(f"  - –î–æ–ª–∂–Ω–æ—Å—Ç—å: {variables['position']}")
        print(f"  - –û—Ü–µ–Ω–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤: {variables['estimated_input_tokens']}")
        print(f"  - –†–∞–∑–º–µ—Ä –∫–∞—Ä—Ç—ã –∫–æ–º–ø–∞–Ω–∏–∏: {len(variables['company_map'])} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"  - –†–∞–∑–º–µ—Ä KPI –¥–∞–Ω–Ω—ã—Ö: {len(variables['kpi_data'])} —Å–∏–º–≤–æ–ª–æ–≤")
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")