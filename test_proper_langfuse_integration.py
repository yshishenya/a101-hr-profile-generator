#!/usr/bin/env python3
"""
üéØ –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è Langfuse —Å —Ç—Ä–µ–π—Å–∏–Ω–≥–æ–º –∏ —Å–≤—è–∑–∫–æ–π –ø—Ä–æ–º–ø—Ç–æ–≤
–û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ Langfuse
"""

import os
import sys
import json
import time
from pathlib import Path
from datetime import datetime

# –ó–∞–≥—Ä—É–∑–∫–∞ .env –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
env_path = Path('/home/yan/A101/HR/.env')
if env_path.exists():
    with open(env_path) as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#') and '=' in line:
                key, value = line.split('=', 1)
                os.environ[key] = value

sys.path.insert(0, '/home/yan/A101/HR')

from langfuse import Langfuse
from openai import OpenAI

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã –¥–ª—è –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä–æ–≤
try:
    from langfuse.decorators import observe, langfuse_context
    decorators_available = True
except ImportError:
    try:
        from langfuse import observe, langfuse_context
        decorators_available = True
    except ImportError:
        try:
            from langfuse.opentelemetry import observe
            decorators_available = False
            langfuse_context = None
        except ImportError:
            decorators_available = False
            observe = None
            langfuse_context = None

def test_proper_langfuse_integration():
    """
    –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é Langfuse —Å:
    - –î–µ–∫–æ—Ä–∞—Ç–æ—Ä–∞–º–∏ @observe
    - –ü—Ä–∞–≤–∏–ª—å–Ω–æ–π —Å–≤—è–∑–∫–æ–π —Å –ø—Ä–æ–º–ø—Ç–∞–º–∏  
    - Sessions –∏ user tracking
    - Environments –∏ metadata
    - Tags –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏
    """
    
    print("üéØ Testing proper Langfuse integration with prompts...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä–æ–≤
    if not decorators_available:
        print("‚ùå Langfuse decorators not available, using basic client API")
        return test_basic_langfuse_integration()
    
    print("‚úÖ Langfuse decorators available")
    
    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º environment –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –æ—Ç –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞
    os.environ["LANGFUSE_TRACING_ENVIRONMENT"] = "development"
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤
    langfuse = Langfuse()
    
    openai_client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=os.getenv("OPENROUTER_API_KEY")
    )
    
    # 1. –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–º–ø—Ç –∏–∑ Langfuse
    print("üìã Getting prompt from Langfuse...")
    try:
        prompt = langfuse.get_prompt("a101-hr-profile-gemini-v3-simple", label="production")
        print(f"‚úÖ Got prompt: {prompt.name} v{prompt.version}")
    except Exception as e:
        print(f"‚ùå Failed to get prompt: {e}")
        return False
    
    # 2. –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    test_variables = {
        "position": "Senior ML Engineer", 
        "department": "–î–ò–¢",
        "company_map": "Test company data (simplified)...",
        "org_structure": '{"–î–ò–¢": {"positions": ["Engineer", "Senior Engineer"]}}',
        "kpi_data": "Test KPI data for –î–ò–¢ department...",
        "it_systems": "Test IT systems data...",
        "json_schema": '{"type": "object", "properties": {}}'
    }
    
    # 3. –°–æ–∑–¥–∞–µ–º –æ—Å–Ω–æ–≤–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é —Å –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä–æ–º @observe
    @observe(
        name="a101-hr-profile-generation",
        as_type="generation",
        metadata={
            "environment": "development",
            "department": test_variables["department"],
            "position": test_variables["position"],
            "prompt_name": prompt.name,
            "prompt_version": prompt.version
        }
    )
    def generate_profile_with_langfuse(variables: dict):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ—Ñ–∏–ª—è —Å –ø–æ–ª–Ω—ã–º Langfuse —Ç—Ä–µ–π—Å–∏–Ω–≥–æ–º"""
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ç—Ä–µ–π—Å–∞
        langfuse_context.update_current_trace(
            name="A101-HR-Profile-Generation",
            user_id="test_user_hr_system",
            session_id=f"test_session_{int(time.time())}",
            tags=["hr", "profile-generation", "development", "test"],
            metadata={
                "source": "hr_profile_generator",
                "version": "1.0.0",
                "test_run": True
            }
        )
        
        try:
            # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç —Å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏
            compiled_prompt = prompt.compile(**variables)
            
            # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ –ø—Ä–æ–º–ø—Ç–∞
            model = prompt.config.get("model", "google/gemini-2.5-flash")
            temperature = prompt.config.get("temperature", 0.1)
            max_tokens = prompt.config.get("max_tokens", 4000)
            response_format = prompt.config.get("response_format")
            
            print(f"‚öôÔ∏è Model: {model}")
            print(f"üå°Ô∏è Temperature: {temperature}")
            print(f"üìù Compiled prompt length: {len(compiled_prompt)} chars")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π generation —Å prompt –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
            langfuse_context.update_current_observation(
                input=compiled_prompt,
                model=model,
                metadata={
                    "temperature": temperature,
                    "max_tokens": max_tokens,
                    "response_format_type": type(response_format).__name__ if response_format else None
                },
                prompt=prompt  # üîó –ö–ª—é—á–µ–≤–∞—è —Å–≤—è–∑–∫–∞ —Å –ø—Ä–æ–º–ø—Ç–æ–º!
            )
            
            start_time = time.time()
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ OpenAI
            print("üîÑ Making OpenAI request...")
            
            messages = [{"role": "user", "content": compiled_prompt}]
            
            response = openai_client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                response_format=response_format
            )
            
            generation_time = time.time() - start_time
            
            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            generated_text = response.choices[0].message.content
            usage = response.usage
            
            # –û–±–Ω–æ–≤–ª—è–µ–º generation —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
            langfuse_context.update_current_observation(
                output=generated_text,
                usage={
                    "input": usage.prompt_tokens if usage else 0,
                    "output": usage.completion_tokens if usage else 0,
                    "total": usage.total_tokens if usage else 0
                },
                metadata={
                    "generation_time": generation_time,
                    "finish_reason": response.choices[0].finish_reason,
                    "success": True
                }
            )
            
            print(f"‚úÖ Generation completed in {generation_time:.2f}s")
            print(f"üìä Tokens: {usage.prompt_tokens} input + {usage.completion_tokens} output = {usage.total_tokens} total")
            
            # –ü–∞—Ä—Å–∏–º JSON –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            try:
                profile_json = json.loads(generated_text.strip())
                print(f"üìã Generated profile with {len(profile_json)} fields")
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ–ª–µ–π
                for key in list(profile_json.keys())[:3]:
                    print(f"  - {key}: {type(profile_json[key]).__name__}")
                
                if len(profile_json) > 3:
                    print(f"  ... and {len(profile_json) - 3} more fields")
                
            except json.JSONDecodeError as e:
                print(f"‚ö†Ô∏è JSON parse warning: {e}")
                profile_json = {"error": "Failed to parse JSON"}
            
            return {
                "profile": profile_json,
                "metadata": {
                    "generation_time": generation_time,
                    "tokens": {
                        "input": usage.prompt_tokens if usage else 0,
                        "output": usage.completion_tokens if usage else 0,
                        "total": usage.total_tokens if usage else 0
                    },
                    "model": model,
                    "success": True
                }
            }
            
        except Exception as e:
            print(f"‚ùå Generation failed: {e}")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º observation —Å –æ—à–∏–±–∫–æ–π
            langfuse_context.update_current_observation(
                output={"error": str(e)},
                metadata={
                    "error": True,
                    "error_message": str(e)
                }
            )
            
            return {
                "profile": {"error": str(e)},
                "metadata": {
                    "success": False,
                    "error": str(e)
                }
            }
    
    # 4. –í—ã–ø–æ–ª–Ω—è–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
    print("üöÄ Starting profile generation with full tracing...")
    result = generate_profile_with_langfuse(test_variables)
    
    # 5. –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç—Ä–µ–π—Å–µ
    try:
        trace_id = langfuse_context.get_current_trace_id()
        trace_url = langfuse.get_trace_url(trace_id) if trace_id else None
        
        print(f"\nüîó Trace Information:")
        print(f"   - Trace ID: {trace_id}")
        print(f"   - Trace URL: {trace_url}")
        print(f"   - Environment: development")
        print(f"   - User ID: test_user_hr_system")
        print(f"   - Tags: hr, profile-generation, development, test")
        
    except Exception as e:
        print(f"‚ö†Ô∏è Could not get trace info: {e}")
    
    # 6. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    success = result["metadata"]["success"]
    
    if success:
        print(f"\nüéâ SUCCESS! Profile generated successfully")
        print(f"‚è±Ô∏è Generation time: {result['metadata']['generation_time']:.2f}s")
        print(f"üìä Total tokens: {result['metadata']['tokens']['total']}")
    else:
        print(f"\n‚ùå FAILED! Error: {result['metadata']['error']}")
    
    return success

def test_basic_langfuse_integration():
    """
    –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è Langfuse –±–µ–∑ –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä–æ–≤
    –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º—ã–µ –≤—ã–∑–æ–≤—ã API –∫–ª–∏–µ–Ω—Ç–∞
    """
    
    print("üîß Using basic Langfuse client API...")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    langfuse = Langfuse(
        environment="development"  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º environment
    )
    
    openai_client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=os.getenv("OPENROUTER_API_KEY")
    )
    
    # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–º–ø—Ç
    try:
        prompt = langfuse.get_prompt("a101-hr-profile-gemini-v3-simple", label="production")
        print(f"‚úÖ Got prompt: {prompt.name} v{prompt.version}")
    except Exception as e:
        print(f"‚ùå Failed to get prompt: {e}")
        return False
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    test_variables = {
        "position": "Senior ML Engineer",
        "department": "–î–ò–¢", 
        "company_map": "Test company data (simplified)...",
        "org_structure": '{"–î–ò–¢": {"positions": ["Engineer", "Senior Engineer"]}}',
        "kpi_data": "Test KPI data for –î–ò–¢ department...",
        "it_systems": "Test IT systems data...",
        "json_schema": '{"type": "object", "properties": {}}'
    }
    
    try:
        # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
        compiled_prompt = prompt.compile(**test_variables)
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        model = prompt.config.get("model", "google/gemini-2.5-flash")
        temperature = prompt.config.get("temperature", 0.1)
        max_tokens = prompt.config.get("max_tokens", 4000)
        response_format = prompt.config.get("response_format")
        
        print(f"‚öôÔ∏è Model: {model}")
        print(f"üå°Ô∏è Temperature: {temperature}")
        print(f"üìù Compiled prompt length: {len(compiled_prompt)} chars")
        
        # –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–π—Å –≤—Ä—É—á–Ω—É—é (–∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã API)
        trace_data = {
            "name": "A101-HR-Profile-Generation-Basic",
            "user_id": "test_user_hr_system",
            "session_id": f"basic_session_{int(time.time())}",
            "tags": ["hr", "profile-generation", "development", "basic-api"],
            "metadata": {
                "source": "hr_profile_generator",
                "version": "1.0.0",
                "test_run": True,
                "api_mode": "basic",
                "prompt_name": prompt.name,
                "prompt_version": prompt.version
            }
        }
        
        start_time = time.time()
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å
        print("üîÑ Making OpenAI request...")
        
        messages = [{"role": "user", "content": compiled_prompt}]
        
        response = openai_client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            response_format=response_format
        )
        
        generation_time = time.time() - start_time
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        generated_text = response.choices[0].message.content
        usage = response.usage
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ Langfuse (–∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–º–µ—é—â–∏–µ—Å—è –º–µ—Ç–æ–¥—ã)
        try:
            # –°–æ–∑–¥–∞–µ–º —Å–æ–±—ã—Ç–∏–µ —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
            langfuse.create_event(
                name="hr-profile-generation-completed",
                metadata={
                    **trace_data["metadata"],
                    "generation_time": generation_time,
                    "tokens": {
                        "input": usage.prompt_tokens if usage else 0,
                        "output": usage.completion_tokens if usage else 0,
                        "total": usage.total_tokens if usage else 0
                    },
                    "model": model,
                    "success": True
                }
            )
            print("‚úÖ Event logged to Langfuse")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to log to Langfuse: {e}")
        
        print(f"‚úÖ Generation completed in {generation_time:.2f}s")
        print(f"üìä Tokens: {usage.prompt_tokens} input + {usage.completion_tokens} output = {usage.total_tokens} total")
        
        # –ü–∞—Ä—Å–∏–º JSON
        try:
            profile_json = json.loads(generated_text.strip())
            print(f"üìã Generated profile with {len(profile_json)} fields")
        except json.JSONDecodeError as e:
            print(f"‚ö†Ô∏è JSON parse warning: {e}")
            profile_json = {"error": "Failed to parse JSON"}
        
        return True
        
    except Exception as e:
        print(f"‚ùå Basic integration failed: {e}")
        return False

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    print("üéØ Starting Langfuse proper integration test...")
    print("üìö Based on official Langfuse documentation")
    
    success = test_proper_langfuse_integration()
    
    if success:
        print("\n‚úÖ SUCCESS! Proper Langfuse integration working!")
        print("üîç Check Langfuse dashboard:")
        print("   1. Go to https://cloud.langfuse.com")
        print("   2. Check Traces tab - you should see 'A101-HR-Profile-Generation'")
        print("   3. Check Prompts tab - trace should be linked to prompt")
        print("   4. Environment filter should show 'development'")
        print("   5. Session and user tracking should be visible")
    else:
        print("\n‚ùå FAILED! Check the errors above")
    
    return success

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)