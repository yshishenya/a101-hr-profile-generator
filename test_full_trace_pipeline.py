#!/usr/bin/env python3
"""
üîç Full Trace Pipeline Test - –ø–æ–ª–Ω—ã–π —Ç—Ä–µ–π—Å –ø–∞–π–ø–ª–∞–π–Ω–∞ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
–í–∫–ª—é—á–∞–µ—Ç –≤–µ—Å—å –æ—Ç–≤–µ—Ç LLM –∏ –¥–µ—Ç–∞–ª—å–Ω—É—é —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫—É
"""

import os
import sys
import asyncio
import logging
import json
from pathlib import Path
from datetime import datetime

# –ó–∞–≥—Ä—É–∑–∫–∞ .env –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
env_path = Path('/home/yan/A101/HR/.env')
if env_path.exists():
    with open(env_path) as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#') and '=' in line:
                key, value = line.split('=', 1)
                os.environ[key] = value

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É
sys.path.insert(0, '/home/yan/A101/HR')

# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('full_pipeline_trace.log', mode='w'),
        logging.StreamHandler(sys.stdout)
    ]
)

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º DEBUG –¥–ª—è –≤—Å–µ—Ö –Ω–∞—à–∏—Ö –º–æ–¥—É–ª–µ–π
logging.getLogger('backend.core.llm_client').setLevel(logging.DEBUG)
logging.getLogger('backend.core.profile_generator').setLevel(logging.DEBUG)
logging.getLogger('backend.core.data_loader').setLevel(logging.DEBUG)
logging.getLogger('langfuse').setLevel(logging.DEBUG)

from backend.core.profile_generator import ProfileGenerator

logger = logging.getLogger(__name__)

async def test_full_trace_pipeline():
    """–ü–æ–ª–Ω—ã–π —Ç—Ä–µ–π—Å –ø–∞–π–ø–ª–∞–π–Ω–∞ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    
    logger.info("=" * 80)
    logger.info("üîç A101 HR Profile Generator - FULL TRACE PIPELINE TEST")
    logger.info("=" * 80)
    
    start_time = datetime.now()
    
    try:
        # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        logger.info("üöÄ STEP 1: Initializing ProfileGenerator")
        logger.debug("Environment variables loaded from .env")
        
        generator = ProfileGenerator()
        
        logger.info("‚úÖ ProfileGenerator initialized successfully")
        logger.info(f"   - Langfuse enabled: {generator.langfuse_enabled}")
        logger.info(f"   - Base data path: {generator.base_data_path}")
        logger.info(f"   - LLM client available: {generator.llm_client is not None}")
        
        # 2. –í–∞–ª–∏–¥–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã
        logger.info("\nüîß STEP 2: System Validation")
        validation = await generator.validate_system()
        
        logger.info("üìä System validation results:")
        logger.info(f"   - System ready: {validation['system_ready']}")
        
        for component, status in validation["components"].items():
            if isinstance(status, dict):
                logger.info(f"   - {component}: {status}")
            else:
                logger.info(f"   - {component}: {status}")
        
        if validation["errors"]:
            logger.warning("‚ö†Ô∏è Validation errors:")
            for error in validation["errors"]:
                logger.warning(f"     - {error}")
        
        if not validation["system_ready"]:
            logger.error("‚ùå System not ready, aborting")
            return False
        
        # 3. –î–µ—Ç–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ—Ñ–∏–ª—è —Å –ø–æ–ª–Ω—ã–º —Ç—Ä–µ–π—Å–∏–Ω–≥–æ–º
        logger.info("\nü§ñ STEP 3: Profile Generation with Full Tracing")
        
        test_params = {
            "department": "–î–ò–¢",
            "position": "Senior ML Engineer",
            "employee_name": "Full Trace Test User",
            "temperature": 0.1,
            "save_result": False
        }
        
        logger.info("üìã Generation parameters:")
        for key, value in test_params.items():
            logger.info(f"   - {key}: {value}")
        
        logger.info("\nüîÑ Starting profile generation...")
        
        # –ö–∞—Å—Ç–æ–º–∏–∑–∏—Ä—É–µ–º LLMClient –¥–ª—è –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        original_generate = generator.llm_client.generate_profile_from_langfuse
        
        def enhanced_generate(*args, **kwargs):
            logger.info("üîç DETAILED LLM TRACE:")
            logger.info(f"   - Method: generate_profile_from_langfuse")
            logger.info(f"   - Prompt name: {args[0] if args else 'N/A'}")
            logger.info(f"   - Variables keys: {list(args[1].keys()) if len(args) > 1 else 'N/A'}")
            
            result = original_generate(*args, **kwargs)
            
            logger.info("üì§ LLM RESPONSE RECEIVED:")
            logger.info(f"   - Success: {result['metadata']['success']}")
            logger.info(f"   - Generation time: {result['metadata']['generation_time']:.2f}s")
            
            # –í—ã–≤–æ–¥–∏–º RAW –æ—Ç–≤–µ—Ç LLM
            raw_response = result.get('raw_response', '')
            if raw_response:
                logger.info("\nü§ñ RAW LLM RESPONSE:")
                logger.info("=" * 80)
                logger.info(raw_response)
                logger.info("=" * 80)
            
            # –í—ã–≤–æ–¥–∏–º —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–π JSON –ø—Ä–æ—Ñ–∏–ª—å
            profile = result.get('profile', {})
            if profile and isinstance(profile, dict):
                logger.info("\nüìã PARSED JSON PROFILE:")
                logger.info("=" * 80)
                logger.info(json.dumps(profile, indent=2, ensure_ascii=False))
                logger.info("=" * 80)
            
            return result
        
        # –í—Ä–µ–º–µ–Ω–Ω–æ –∑–∞–º–µ–Ω—è–µ–º –º–µ—Ç–æ–¥ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        generator.llm_client.generate_profile_from_langfuse = enhanced_generate
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
        result = await generator.generate_profile(**test_params)
        
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥
        generator.llm_client.generate_profile_from_langfuse = original_generate
        
        logger.info("\nüìä STEP 4: Generation Results Analysis")
        
        generation_time = (datetime.now() - start_time).total_seconds()
        
        logger.info("üéØ FINAL RESULTS:")
        logger.info(f"   - Success: {'‚úÖ' if result['success'] else '‚ùå'}")
        logger.info(f"   - Total execution time: {generation_time:.2f}s")
        
        if result["success"]:
            profile = result.get("profile", {})
            metadata = result.get("metadata", {})
            
            logger.info("üìã GENERATED PROFILE STRUCTURE:")
            if isinstance(profile, dict):
                for key, value in profile.items():
                    if isinstance(value, list):
                        logger.info(f"   - {key}: array with {len(value)} items")
                    elif isinstance(value, dict):
                        logger.info(f"   - {key}: object with {len(value)} fields")
                    else:
                        logger.info(f"   - {key}: {type(value).__name__}")
            
            logger.info("\n‚úÖ Profile generated successfully (see RAW LLM RESPONSE above)")
            
            # LLM –º–µ—Ç—Ä–∏–∫–∏
            llm_meta = metadata.get("llm", {})
            if llm_meta:
                logger.info("\nüìà LLM PERFORMANCE METRICS:")
                logger.info(f"   - Model: {llm_meta.get('model', 'N/A')}")
                logger.info(f"   - Generation time: {llm_meta.get('generation_time', 0):.2f}s")
                
                tokens = llm_meta.get('tokens', {})
                if tokens:
                    logger.info(f"   - Input tokens: {tokens.get('input', 0)}")
                    logger.info(f"   - Output tokens: {tokens.get('output', 0)}")
                    logger.info(f"   - Total tokens: {tokens.get('total', 0)}")
                
                logger.info(f"   - Temperature: {llm_meta.get('temperature', 'N/A')}")
                logger.info(f"   - Success: {llm_meta.get('success', False)}")
                
                # Langfuse —Ç—Ä–µ–π—Å–∏–Ω–≥
                trace_id = llm_meta.get('langfuse_trace_id')
                if trace_id:
                    logger.info(f"   - üîó Langfuse trace ID: {trace_id}")
                    logger.info(f"   - üåê View trace: https://cloud.langfuse.com")
                
                # Raw response already shown above during LLM call
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–æ—Ñ–∏–ª—è
            generation_meta = metadata.get("generation", {})
            if generation_meta:
                logger.info("\n‚úÖ GENERATION METADATA:")
                logger.info(f"   - Department: {generation_meta.get('department', 'N/A')}")
                logger.info(f"   - Position: {generation_meta.get('position', 'N/A')}")
                logger.info(f"   - Duration: {generation_meta.get('duration', 0):.2f}s")
                logger.info(f"   - Timestamp: {generation_meta.get('timestamp', 'N/A')}")
        
        else:
            logger.error("‚ùå GENERATION FAILED:")
            errors = result.get("errors", [])
            for error in errors:
                logger.error(f"     - {error}")
            
            # –í—ã–≤–æ–¥–∏–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–∞–∂–µ –ø—Ä–∏ –Ω–µ—É–¥–∞—á–µ
            metadata = result.get("metadata", {})
            if metadata:
                logger.info("üìä Error metadata:")
                logger.info(json.dumps(metadata, indent=2, ensure_ascii=False))
        
        logger.info(f"\nüèÅ PIPELINE TRACE COMPLETED!")
        logger.info(f"üìÑ Full log saved to: full_pipeline_trace.log")
        logger.info("=" * 80)
        
        return result["success"]
        
    except Exception as e:
        logger.error(f"\nüí• PIPELINE EXCEPTION:")
        logger.error(f"   - Error: {e}")
        logger.error(f"   - Type: {type(e).__name__}")
        
        import traceback
        logger.error("üìç Full traceback:")
        logger.error(traceback.format_exc())
        
        return False


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∞"""
    logger.info("Starting Full Trace Pipeline Test")
    success = await test_full_trace_pipeline()
    
    if success:
        logger.info("üéâ FULL TRACE PIPELINE TEST: SUCCESS!")
    else:
        logger.error("üí• FULL TRACE PIPELINE TEST: FAILED!")
    
    return success


if __name__ == "__main__":
    success = asyncio.run(main())
    exit(0 if success else 1)