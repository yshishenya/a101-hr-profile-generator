#!/usr/bin/env python3
"""
üîó –ü—Ä—è–º–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–≤—è–∑–∫–∏ –ø—Ä–æ–º–ø—Ç–æ–≤ —Å generations –≤ Langfuse
"""

import os
import sys
import json
from pathlib import Path

# –ó–∞–≥—Ä—É–∑–∫–∞ .env –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
env_path = Path('/home/yan/A101/HR/.env')
if env_path.exists():
    with open(env_path) as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#') and '=' in line:
                key, value = line.split('=', 1)
                os.environ[key] = value

from langfuse import Langfuse
from langfuse.openai import OpenAI

def test_direct_prompt_linking():
    """–¢–µ—Å—Ç–∏—Ä—É–µ–º –ø—Ä—è–º—É—é —Å–≤—è–∑–∫—É –ø—Ä–æ–º–ø—Ç–æ–≤ —Å generations"""
    
    print("üîó Testing direct prompt linking...")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Langfuse
    langfuse = Langfuse()
    
    # –°–æ–∑–¥–∞–µ–º Langfuse OpenAI –∫–ª–∏–µ–Ω—Ç–∞
    client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=os.getenv("OPENROUTER_API_KEY"),
        langfuse_public_key=os.getenv("LANGFUSE_PUBLIC_KEY"),
        langfuse_secret_key=os.getenv("LANGFUSE_SECRET_KEY"),
        langfuse_host=os.getenv("LANGFUSE_HOST")
    )
    
    # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–º–ø—Ç
    try:
        prompt = langfuse.get_prompt("a101-hr-profile-gemini-v3-simple", label="production")
        print(f"‚úÖ Got prompt: {prompt.name} v{prompt.version}")
    except Exception as e:
        print(f"‚ùå Failed to get prompt: {e}")
        return False
    
    # –ü—Ä–æ—Å—Ç—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è —Ç–µ—Å—Ç–∞
    variables = {
        "position": "Test Position",
        "department": "Test Department", 
        "company_map": "Test company data",
        "org_structure": '{"test": "data"}',
        "kpi_data": "Test KPI",
        "it_systems": "Test IT",
        "json_schema": '{"type": "object"}'
    }
    
    try:
        # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
        compiled_prompt = prompt.compile(**variables)
        messages = [{"role": "user", "content": compiled_prompt}]
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        config = prompt.config or {}
        model = config.get("model", "google/gemini-2.5-flash")
        temperature = config.get("temperature", 0.1)
        max_tokens = config.get("max_tokens", 1000)
        response_format = config.get("response_format")
        
        print(f"ü§ñ Testing with model: {model}")
        print(f"üìù Prompt length: {len(compiled_prompt)}")
        
        # –ú–µ—Ç–æ–¥ 1: langfuse_prompt –ø–∞—Ä–∞–º–µ—Ç—Ä
        print("\nüîó Method 1: Using langfuse_prompt parameter")
        try:
            response = client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                response_format=response_format,
                langfuse_prompt=prompt  # –ü—Ä—è–º–∞—è —Å–≤—è–∑–∫–∞
            )
            
            print("‚úÖ Method 1 successful!")
            print(f"üìä Response tokens: {response.usage.total_tokens if response.usage else 'N/A'}")
            
        except Exception as e:
            print(f"‚ùå Method 1 failed: {e}")
        
        # –ú–µ—Ç–æ–¥ 2: metadata
        print("\nüîó Method 2: Using metadata")
        try:
            response = client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                response_format=response_format,
                metadata={"langfuse_prompt": prompt}  # –°–≤—è–∑–∫–∞ —á–µ—Ä–µ–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            )
            
            print("‚úÖ Method 2 successful!")
            print(f"üìä Response tokens: {response.usage.total_tokens if response.usage else 'N/A'}")
            
        except Exception as e:
            print(f"‚ùå Method 2 failed: {e}")
        
        # –ú–µ—Ç–æ–¥ 3: –¢–æ–ª—å–∫–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø—Ä–æ–º–ø—Ç–µ
        print("\nüîó Method 3: Using prompt info in metadata")
        try:
            response = client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                response_format=response_format,
                metadata={
                    "prompt_name": prompt.name,
                    "prompt_version": prompt.version,
                    "prompt_id": prompt.id if hasattr(prompt, 'id') else None
                }
            )
            
            print("‚úÖ Method 3 successful!")
            print(f"üìä Response tokens: {response.usage.total_tokens if response.usage else 'N/A'}")
            
        except Exception as e:
            print(f"‚ùå Method 3 failed: {e}")
        
        print(f"\nüéØ Now check Langfuse dashboard:")
        print(f"   - Go to Prompts ‚Üí {prompt.name}")
        print(f"   - Look for 'Linked Generations' section")
        print(f"   - Should see recent test generations")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ Testing direct prompt linking methods...")
    
    success = test_direct_prompt_linking()
    
    if success:
        print("\n‚úÖ SUCCESS! Check Langfuse for linked generations")
    else:
        print("\n‚ùå FAILED! Check errors above")
    
    return success

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)